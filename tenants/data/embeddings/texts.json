[
    "Git repository replication\n\u00b6\nStandard & Business Critical Feature\nDatabase and share replication are available to all accounts.\nReplication of other account objects & failover/failback require Business Critical Edition (or higher).\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nThis topic provides information about Snowflake support for replicating Git repository objects.\nBefore you get started, we recommend that you be familiar with Snowflake support for Git repositories.\nFor more information, see\nUsing a Git repository in Snowflake\n.\nConsiderations for replicating Git repository clones\n\u00b6\nTo replicate any Git repository objects that you\u2019ve integrated with Snowflake,\nyou specify the database or schema that contains the Git repository object\nin a replication group or a failover group. You don\u2019t have to perform any\nseparate step to enable replication for Git repository clones.\nThe secrets from the primary system are replicated to the secondary system.\nOn the secondary system, you can read from the repository. However, you can\u2019t commit, fetch from, or push to\nthe remote\norigin\nserver from the secondary system. After you promote the secondary system to be the primary\nby failing over, you can perform these other operations on the Git repository.\nSnowflake supports replication for Git repository clones up to 5 GB in size. Larger repositories currently aren\u2019t supported.\nOn this page\nConsiderations for replicating Git repository clones",
    "Configure replication for Snowflake-managed Apache Iceberg\u2122 tables\n\u00b6\nPreview Feature\n\u2014 Open\nAvailable to all accounts.\nStandard & Business Critical Feature\nReplication of Snowflake-managed Iceberg tables to a replication group is available to all accounts.\nReplication of Snowflake-managed Iceberg tables to a failover group requires Business Critical Edition (or higher).\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nWith this feature, you can replicate\nSnowflake-managed Apache Iceberg\u2122 tables\nfrom a source account to one or more target accounts in the same organization.\nReplication for Iceberg tables works similarly to replication for regular Snowflake tables. Snowflake replicates an Iceberg table\nwhen you add its parent database to a failover or replication group.\nHowever, Snowflake-managed Iceberg tables rely on external volumes, which are account-level objects that require extra configuration to connect\nto your external cloud storage. Before you can replicate an Iceberg table, you must configure replication for external volumes.\nOpt in to the public preview for replication for Snowflake-managed Iceberg tables\n\u00b6\nTo opt in to this public preview, you must opt in both the source and target account.\nTo opt in your source account, after you\nenable preview features\nfor your account, use the\nALTER ACCOUNT\ncommand to enable the following parameters at the\naccount level:\nENABLE_ICEBERG_MANAGED_TABLE_REPLICATION\nNote\nYou can also enable this parameter at the failover group level.\nENABLE_SELECTIVE_EXTERNAL_VOLUME_REPLICATION_PUPR\nFor example:\nALTER\nACCOUNT\nSET\nENABLE_ICEBERG_MANAGED_TABLE_REPLICATION\n=\nTRUE\nENABLE_SELECTIVE_EXTERNAL_VOLUME_REPLICATION_PUPR\n=\nTRUE\n;\nCopy\nRepeat the previous step for your target account.\nEnable replication\n\u00b6\nA user with the ORGADMIN role must enable replication for each source and target account in the organization:\nUSE\nROLE\nORGADMIN\n;\nSELECT\nSYSTEM$GLOBAL_ACCOUNT_SET_PARAMETER\n(\n'<organization_name>.<account_name>'\n,\n'ENABLE_ACCOUNT_DATABASE_REPLICATION'\n,\n'true'\n);\nCopy\nFor more information, see\nPrerequisite: Enable replication for accounts in the organization\n.\nFor more information about replication, see\nIntroduction to replication and failover across multiple accounts\n.\nReplicate an external volume by using a failover group\n\u00b6\nThese steps provide a sample workflow for replicating an external volume",
    "ADMIN\n;\nSELECT\nSYSTEM$GLOBAL_ACCOUNT_SET_PARAMETER\n(\n'<organization_name>.<account_name>'\n,\n'ENABLE_ACCOUNT_DATABASE_REPLICATION'\n,\n'true'\n);\nCopy\nFor more information, see\nPrerequisite: Enable replication for accounts in the organization\n.\nFor more information about replication, see\nIntroduction to replication and failover across multiple accounts\n.\nReplicate an external volume by using a failover group\n\u00b6\nThese steps provide a sample workflow for replicating an external volume and the Iceberg tables that depend on it\nto a target account by using a failover group.\nNote\nIf you don\u2019t already have an external volume, you can create one with the storage locations that you want, including a\nlocation\nin the same region\nas your target account. After configuring storage access for each location,\nyou can create and replicate an Iceberg table that references the external volume.\nTo create an external volume, see\nConfigure an external volume\n.\nIn the source account, update your external volume to add a storage location\nin the same region\nas your target account.\nFor example:\nALTER\nEXTERNAL VOLUME\nexvol1\nADD\nSTORAGE_LOCATION\n=\n(\nNAME\n=\n'my-s3-us-central-2'\nSTORAGE_PROVIDER\n=\n'S3'\nSTORAGE_BASE_URL\n=\n's3://my_bucket_us_central-2/'\nSTORAGE_AWS_ROLE_ARN\n=\n'arn:aws:iam::123456789012:role/myrole'\nSTORAGE_AWS_EXTERNAL_ID\n=\n'iceberg_table_external_id'\n);\nCopy\nImportant\nIf you don\u2019t specify your own\nSTORAGE_AWS_EXTERNAL_ID\nfor S3 storage, you must call\nDESCRIBE EXTERNAL VOLUME\nafter you\nadd the new storage location to retrieve the Snowflake-generated external ID.\nYou need the external ID to configure access to S3 in the next step.\nSnowflake sets this new location as the\nactive storage location\nfor the secondary\nexternal volume.\nIn the source account, create a Snowflake-managed Iceberg table that uses the external volume that you updated with the additional storage\nlocation.\nFor example:\nCREATE\nICEBERG\nTABLE\nmy_iceberg_table\n(\namount\nint\n)\nCATALOG\n=\n'SNOWFLAKE'\nEXTERNAL_VOLUME\n=\n'exvol1'\nBASE_LOCATION\n=\n'my_iceberg_table'\n;\nCopy\nIn the source account, retrieve",
    " storage location\nfor the secondary\nexternal volume.\nIn the source account, create a Snowflake-managed Iceberg table that uses the external volume that you updated with the additional storage\nlocation.\nFor example:\nCREATE\nICEBERG\nTABLE\nmy_iceberg_table\n(\namount\nint\n)\nCATALOG\n=\n'SNOWFLAKE'\nEXTERNAL_VOLUME\n=\n'exvol1'\nBASE_LOCATION\n=\n'my_iceberg_table'\n;\nCopy\nIn the source account, retrieve information about the\nSnowflake service principal\nfor\nyour\ntarget account\nby following these steps:\nRetrieve the name (\naccount_name\n) of your target account by using the\nSHOW REPLICATION ACCOUNTS\ncommand.\nSHOW\nREPLICATION\nACCOUNTS\nLIKE\n'my_target_account%'\n;\nCopy\nCall the\nSYSTEM$DESC_ICEBERG_ACCESS_IDENTITY\nsystem function.\nSpecify the cloud provider for the target storage location and the name of your target account\nexactly\nas\nit appears in the\naccount_name\ncolumn of the SHOW REPLICATION ACCOUNTS output.\nFor example:\nSELECT\nSYSTEM$DESC_ICEBERG_ACCESS_IDENTITY\n(\n'S3'\n,\n'MY_TARGET_ACCOUNT_1'\n);\nCopy\nConfigure Snowflake access to the storage location associated with your target account.\nFollow the instructions for your cloud provider, using the information you retrieved for the service principal in the target account:\nConfigure an external volume for Amazon S3\n. Use the external ID associated with the storage location for your target account.\nConfigure an external volume for Google Cloud Storage\nConfigure an external volume for Azure\n. In the\nAZURE_CONSENT_URL\nTEMPLATE\nreturned by\nSYSTEM$DESC_ICEBERG_ACCESS_IDENTITY, replace\nyour_tenant_id\nwith the ID for your\ntenant that the storage location belongs to.\nIn the source account, use the\nCREATE FAILOVER GROUP\ncommand to create a failover group.\nSpecify\nEXTERNAL\nVOLUMES\nin the\nOBJECT_TYPES\nlist. In the\nALLOWED_DATABASES\nlist, include the database with the Iceberg tables that you want to replicate. In the\nALLOWED_EXTERNAL_VOLUMES\nlist, include the external volumes that provide access to the Iceberg tables that you want to replicate.\nCREATE\nFAILOVER\nGROUP\nmy_iceberg_fg\nOBJECT_TYPES\n=\nDATABASES\n,\nEXTERNAL\nVOLUMES\nALLOWED_DATABASES",
    "\nVOLUMES\nin the\nOBJECT_TYPES\nlist. In the\nALLOWED_DATABASES\nlist, include the database with the Iceberg tables that you want to replicate. In the\nALLOWED_EXTERNAL_VOLUMES\nlist, include the external volumes that provide access to the Iceberg tables that you want to replicate.\nCREATE\nFAILOVER\nGROUP\nmy_iceberg_fg\nOBJECT_TYPES\n=\nDATABASES\n,\nEXTERNAL\nVOLUMES\nALLOWED_DATABASES\n=\nmy_iceberg_database\nALLOWED_EXTERNAL_VOLUMES\n=\nmy_external_volume\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmy_account_1\n;\nCopy\nNote\nIf you receive a SQL parser error, your list of allowed external volumes might be too long. If you receive this error, shorten this\nlist in your CREATE FAILOVER GROUP statement, and then use the\nALTER FAILOVER GROUP\ncommand to add\nadditional allowed external volumes to the failover group.\nTo update an existing group, use the\nALTER FAILOVER GROUP\ncommand to add\nEXTERNAL\nVOLUMES\nto the\nOBJECT_TYPES\nlist.\nInclude any other existing objects in the\nOBJECT_TYPES\nlist to avoid dropping those objects in the target account.\nFor example, add\nEXTERNAL\nVOLUMES\nto a failover group that already includes\nDATABASES\n:\nALTER\nFAILOVER\nGROUP\nmy_iceberg_rg\nSET\nOBJECT_TYPES\n=\nDATABASES\n,\nEXTERNAL\nVOLUMES\nALLOWED_EXTERNAL_VOLUMES\n=\nmy_external_volume\n;\nCopy\nIn the target account, create a failover group as a replica of the group in the source account (\nmy_source_account\n):\nCREATE\nFAILOVER\nGROUP\nmy_iceberg_fg\nAS\nREPLICA\nOF\nmyorg\n.\nmy_source_account\n.\nmy_iceberg_fg\n;\nCopy\nSkip this step if you already have a secondary group that replicates the group in the source account.\nIn the target account, run a refresh command.\nALTER\nFAILOVER\nGROUP\nmy_iceberg_fg\nREFRESH\n;\nCopy\nAs long as you replicate the database that contains your Snowflake-managed Iceberg table and you\u2019ve\nconfigured access to your cloud storage for the target account, Snowflake replicates the table in the target account.\nNote\nThe refresh operation fails if Snow",
    " step if you already have a secondary group that replicates the group in the source account.\nIn the target account, run a refresh command.\nALTER\nFAILOVER\nGROUP\nmy_iceberg_fg\nREFRESH\n;\nCopy\nAs long as you replicate the database that contains your Snowflake-managed Iceberg table and you\u2019ve\nconfigured access to your cloud storage for the target account, Snowflake replicates the table in the target account.\nNote\nThe refresh operation fails if Snowflake can\u2019t access the storage location configured for the target account.\nIf this happens, double-check your access control settings, or try\nVerifying storage access\n.\nReplicate an external volume by using a replication group\n\u00b6\nThese steps provide a sample workflow for replicating an external volume and the Iceberg tables that depend on it\nto a target account by using a replication group.\nNote\nIf you don\u2019t already have an external volume, you can create one with the storage locations that you want, including a\nlocation\nin the same region\nas your target account. After configuring storage access for each location,\nyou can create and replicate an Iceberg table that references the external volume.\nTo create an external volume, see\nConfigure an external volume\n.\nIn the source account, update your external volume to add a storage location\nin the same region\nas your target account.\nFor example:\nALTER\nEXTERNAL VOLUME\nexvol1\nADD\nSTORAGE_LOCATION\n=\n(\nNAME\n=\n'my-s3-us-central-2'\nSTORAGE_PROVIDER\n=\n'S3'\nSTORAGE_BASE_URL\n=\n's3://my_bucket_us_central-2/'\nSTORAGE_AWS_ROLE_ARN\n=\n'arn:aws:iam::123456789012:role/myrole'\nSTORAGE_AWS_EXTERNAL_ID\n=\n'iceberg_table_external_id'\n);\nCopy\nImportant\nIf you don\u2019t specify your own\nSTORAGE_AWS_EXTERNAL_ID\nfor S3 storage, you must call\nDESCRIBE EXTERNAL VOLUME\nafter you\nadd the new storage location to retrieve the Snowflake-generated external ID.\nYou need the external ID to configure access to S3 in the next step.\nSnowflake sets this new location as the\nactive storage location\nfor the secondary\nexternal volume.\nIn the source account, create a Snowflake-managed Iceberg table that uses the external volume that you updated with the additional storage\nlocation.\nFor example:\nCREATE",
    " storage, you must call\nDESCRIBE EXTERNAL VOLUME\nafter you\nadd the new storage location to retrieve the Snowflake-generated external ID.\nYou need the external ID to configure access to S3 in the next step.\nSnowflake sets this new location as the\nactive storage location\nfor the secondary\nexternal volume.\nIn the source account, create a Snowflake-managed Iceberg table that uses the external volume that you updated with the additional storage\nlocation.\nFor example:\nCREATE\nICEBERG\nTABLE\nmy_iceberg_table\n(\namount\nint\n)\nCATALOG\n=\n'SNOWFLAKE'\nEXTERNAL_VOLUME\n=\n'exvol1'\nBASE_LOCATION\n=\n'my_iceberg_table'\n;\nCopy\nIn the source account, retrieve information about the\nSnowflake service principal\nfor\nyour\ntarget account\nby following these steps:\nRetrieve the name (\naccount_name\n) of your target account by using the\nSHOW REPLICATION ACCOUNTS\ncommand.\nSHOW\nREPLICATION\nACCOUNTS\nLIKE\n'my_target_account%'\n;\nCopy\nCall the\nSYSTEM$DESC_ICEBERG_ACCESS_IDENTITY\nsystem function.\nSpecify the cloud provider for the target storage location and the name of your target account\nexactly\nas\nit appears in the\naccount_name\ncolumn of the SHOW REPLICATION ACCOUNTS output.\nFor example:\nSELECT\nSYSTEM$DESC_ICEBERG_ACCESS_IDENTITY\n(\n'S3'\n,\n'MY_TARGET_ACCOUNT_1'\n);\nCopy\nConfigure Snowflake access to the storage location associated with your target account.\nFollow the instructions for your cloud provider, using the information you retrieved for the service principal in the target account:\nConfigure an external volume for Amazon S3\n. Use the external ID associated with the storage location for your target account.\nConfigure an external volume for Google Cloud Storage\nConfigure an external volume for Azure\n. In the\nAZURE_CONSENT_URL\nTEMPLATE\nreturned by\nSYSTEM$DESC_ICEBERG_ACCESS_IDENTITY, replace\nyour_tenant_id\nwith the ID for your\ntenant that the storage location belongs to.\nIn the source account, use the\nCREATE REPLICATION GROUP\ncommand to create a replication group.\nSpecify\nEXTERNAL\nVOLUMES\nin the\nOBJECT_TYPES\nlist. In the\nALLOWED_DATABASES\nlist, include the database with the Iceberg table(s) you want to replicate. In the\nALLOWED_EXTERNAL_VOLUM",
    "$DESC_ICEBERG_ACCESS_IDENTITY, replace\nyour_tenant_id\nwith the ID for your\ntenant that the storage location belongs to.\nIn the source account, use the\nCREATE REPLICATION GROUP\ncommand to create a replication group.\nSpecify\nEXTERNAL\nVOLUMES\nin the\nOBJECT_TYPES\nlist. In the\nALLOWED_DATABASES\nlist, include the database with the Iceberg table(s) you want to replicate. In the\nALLOWED_EXTERNAL_VOLUMES\nlist, include the external volumes that provide access to the Iceberg table(s) you want to replicate.\nCREATE\nREPLICATION\nGROUP\nmy_iceberg_rg\nOBJECT_TYPES\n=\nDATABASES\n,\nEXTERNAL\nVOLUMES\nALLOWED_DATABASES\n=\nmy_iceberg_database\nALLOWED_EXTERNAL_VOLUMES\n=\nmy_external_volume\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmy_account_1\n;\nCopy\nNote\nIf you receive a SQL parser error, your list of allowed external volumes might be too long. If you receive this error, shorten this\nlist in your CREATE REPLICATION GROUP statement, and then use the\nALTER REPLICATION GROUP\ncommand to add\nadditional allowed external volumes to the replication group.\nTo update an existing group, use the\nALTER REPLICATION GROUP\ncommand to add\nEXTERNAL\nVOLUMES\nto the\nOBJECT_TYPES\nlist.\nInclude any other existing objects in the\nOBJECT_TYPES\nlist to avoid dropping those objects in the target account.\nFor example, add\nEXTERNAL\nVOLUMES\nto a replication group that already includes\nDATABASES\n:\nALTER\nREPLICATION\nGROUP\nmy_iceberg_rg\nSET\nOBJECT_TYPES\n=\nDATABASES\n,\nEXTERNAL\nVOLUMES\nALLOWED_EXTERNAL_VOLUMES\n=\nmy_external_volume\n;\nCopy\nIn the target account, create a replication group as a replica of the group in the source account (\nmy_source_account\n):\nCREATE\nREPLICATION\nGROUP\nmy_iceberg_rg\nAS\nREPLICA\nOF\nmyorg\n.\nmy_source_account\n.\nmy_iceberg_rg\n;\nCopy\nSkip this step if you already have a secondary group that replicates the group in the source account.\nIn the target account, run a refresh command.\nALTER\nREPLICATION\nGROUP\nmy_iceberg_rg\nREFRESH\n;\nCopy\nAs long as you replicate the database that contains your Snowflake",
    "CREATE\nREPLICATION\nGROUP\nmy_iceberg_rg\nAS\nREPLICA\nOF\nmyorg\n.\nmy_source_account\n.\nmy_iceberg_rg\n;\nCopy\nSkip this step if you already have a secondary group that replicates the group in the source account.\nIn the target account, run a refresh command.\nALTER\nREPLICATION\nGROUP\nmy_iceberg_rg\nREFRESH\n;\nCopy\nAs long as you replicate the database that contains your Snowflake-managed Iceberg table and you\u2019ve\nconfigured access to your cloud storage for the target account, Snowflake replicates the table in the target account.\nNote\nThe refresh operation fails if Snowflake can\u2019t access the storage location configured for the target account.\nIf this happens, double-check your access control settings, or try\nVerifying storage access\n.\nConsiderations and limitations\n\u00b6\nConsider the following points when you use replication for Iceberg tables:\nSnowflake currently supports replication of Snowflake-managed tables only.\nReplicating converted Iceberg tables isn\u2019t supported. Snowflake skips converted tables during refresh operations.\nFor replicated tables, you must configure access to a storage location in the\nsame region\nas the target account.\nIf you drop or alter a storage location that is used for replication on the primary external volume, refresh operations might fail.\nSecondary tables in the target account are read-only until you promote the target account to serve as the source account.\nSnowflake maintains the\ndirectory hierarchy\nof the primary Iceberg table for the secondary table.\nReplication costs apply for this feature. For more information, see\nUnderstanding replication cost\n.\nFor considerations about the account objects for replication and failover groups, see\nAccount objects\n.\nOn this page\nOpt in to the public preview for replication for Snowflake-managed Iceberg tables\nEnable replication\nReplicate an external volume by using a failover group\nReplicate an external volume by using a replication group\nConsiderations and limitations\nRelated content\nIntroduction to replication and failover across multiple accounts\nSYSTEM$DESC_ICEBERG_ACCESS_IDENTITY\nCREATE FAILOVER GROUP\nALTER FAILOVER GROUP\nCREATE REPLICATION GROUP\nALTER REPLICATION GROUP",
    " and limitations\nRelated content\nIntroduction to replication and failover across multiple accounts\nSYSTEM$DESC_ICEBERG_ACCESS_IDENTITY\nCREATE FAILOVER GROUP\nALTER FAILOVER GROUP\nCREATE REPLICATION GROUP\nALTER REPLICATION GROUP",
    "Backups for disaster recovery and immutable storage\n\u00b6\nStandard & Business Critical Feature\nBackups are available for all Snowflake editions.\nBackups with retention lock and backups with legal holds are available for Business Critical Edition (or higher).\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nBackups help organizations protect critical data against modification or deletion.\nBackups represent discrete snapshots of Snowflake objects. You choose which objects to back up, how frequently to\nback them up, how long to keep the backups, and whether to add a retention lock so that they can\u2019t\nbe deleted prematurely.\nUse cases for Snowflake backups\n\u00b6\nThe following use cases are typical applications of backups:\nRegulatory compliance\n:\nBackups with retention lock help organizations, financial institutions, and\nrelated industries address regulations that require records to be retained in an immutable format.\nNote\nSnowflake has engaged Cohasset Associates to perform an independent assessment of our Backups\nfeature for compliance with key regulatory recordkeeping requirements, including SEC 17a-4(f),\nSEC 18a-6(e), FINRA Rule 4511(c), and CFTC Rule 1.31(c)-(d). This Cohasset assessment provides\nindependent, third-party verification that Snowflake\u2019s immutable storage controls support the\ncreation, protection, and retention of data, and provides customers with confidence that\nSnowflake meets critical industry standards for regulated data retention subject to the\nevaluated regulations.\nFor the full compliance report that applies to Snowflake backups with retention lock,\nsee\nthe Snowflake Compliance Center\n.\nRecovery\n:\nBackups help organizations create discrete snapshots to protect and recover business-critical data\nin case of accidental modifications or deletions.\nCyber resilience\n:\nBackups with retention lock are part of an overall cyber-resilience strategy. They help organizations\nprotect business-critical data during cyber attacks, especially ransomware attacks. The retention lock ensures that this data\ncan\u2019t be deleted by the attacker, even if they gain access to the account by using the ACCOUNTADMIN or ORGADMIN roles.\nKey concepts\n\u00b6\nThis section provides an overview of the key concepts for backups in Snowflake.\nBackup\n\u00b6\nA\nbackup\nrepresents a point-in-time snapshot of an object.\nThe object can be a single table, a schema, or an entire database.\nA specific backup can be identified by a unique ID generated by Snowflake.\nA backup can\u2019t be modified",
    " deleted by the attacker, even if they gain access to the account by using the ACCOUNTADMIN or ORGADMIN roles.\nKey concepts\n\u00b6\nThis section provides an overview of the key concepts for backups in Snowflake.\nBackup\n\u00b6\nA\nbackup\nrepresents a point-in-time snapshot of an object.\nThe object can be a single table, a schema, or an entire database.\nA specific backup can be identified by a unique ID generated by Snowflake.\nA backup can\u2019t be modified. It can, however, be deleted, and the backup expiration period can be modified\n(unless a\nretention lock\nis applied).\nDuring day-to-day operations, you rarely interact with individual backups. Instead, you manage the\nbackup sets\nthat contain\nthem. For example, you get a list of backups by running the SHOW BACKUPS IN BACKUP SET command. You create a new backup by\nrunning an ALTER BACKUP SET command.\nBackup set\n\u00b6\nA\nbackup set\nis a schema-level object that contains a set of backups for a specific database, schema, or table.\nSnowflake has SQL commands to CREATE, ALTER, DROP, SHOW, and DESCRIBE backup sets.\nYou can have multiple backup sets for the same object.\nThe life cycle of the backups within a set is determined by an optional\nbackup policy\nthat you can attach to the backup set.\nYou can also add or delete backups manually in a backup set. Your ability to delete backups is affected by\nother factors, in particular\nretention lock\nand\nlegal hold\n.\nBackup policy\n\u00b6\nA\nbackup policy\nis a schema-level object that contains the settings that define the life cycle of the backups within a backup\nset. These settings include schedule, expiration, and retention lock.\nThe\nschedule\ndetermines when backups are created. The schedule can be defined as\nan interval in minutes, or as a cron expression.\nFor example, if the schedule is set to one hour, a backup of the object is taken every 60 minutes.\nThe\nexpiration period\nis the length of time the backup is valid. After a backup expires,\nSnowflake deletes it automatically, unless a legal hold is applied to that particular backup.\nTip\nIf the backup set doesn\u2019t have a retention lock and the particular backup doesn\u2019t have a legal\nhold applied, you can delete the backup manually before the end of the expiration period.\nYou can manually delete backups one",
    " objects from backups.\nYes\nBackup storage\nSnowflake-managed cloud object storage to store backup data.\nBilled for bytes retained for backups, similar to bytes retained for clones.\nYou can monitor costs for backup storage in the\nTABLE_STORAGE_METRICS\nview using the\nRETAINED_FOR_CLONE_BYTES\ncolumn, and in the\nBACKUP_STORAGE_USAGE\nview.\nAccess control privileges\n\u00b6\nThe following table lists privileges and the object type on which the privilege is granted for managing and using backups.\nPrivilege\nObject type\nDescription\nCREATE BACKUP POLICY\nSchema\nGrants the ability to create a backup policy in a schema. The role granting this privilege must also have the USAGE\nprivilege on the schema.\nCREATE BACKUP SET\nSchema\nGrants the ability to create a backup set in a schema. The role granting this privilege must also have the USAGE\nprivilege on the schema. To actually create the backup set also requires the appropriate privilege on the object that\u2019s the\nsubject of the backup set: SELECT for a table backup, or USAGE for a schema backup or database backup.\nAPPLY\nBackup policy\nGrants the ability to apply a specific backup policy. Only a user with the ACCOUNTADMIN role can grant this privilege.\nAPPLY BACKUP RETENTION LOCK\nAccount\nGrants the ability to create and apply backup policies with retention lock. This privilege is granted to the ACCOUNTADMIN\nrole and can be delegated.\nThis privilege is required to enable a role to do the following:\nCreate a backup policy with retention lock.\nApply a backup policy with retention lock on a backup set.\nCreate a backup, either manually by a user or automatically on a schedule, in a backup set protected by a policy\nwith retention lock.\nAPPLY LEGAL HOLD\nAccount\nGrants the ability to add or remove a legal hold from a backup. By default, the ACCOUNTADMIN role has this privilege.\nThe following privilege requirements apply when Snowflake automatically creates or expires backups in the background.\nThe owner of the backup set needs to have the following privileges:\nThe appropriate privilege on the object that\u2019s the subject of the backup set: SELECT for a table\nbackup, or USAGE for a schema backup or database backup.\nAny privilege on the parent schema or database for the subject of the backup set.\nAny privilege on the parent schema and database of the backup set.\nIf any of those privileges are missing, the automatic backup creation or expiration fails. You can\nmonitor",
    " expires backups in the background.\nThe owner of the backup set needs to have the following privileges:\nThe appropriate privilege on the object that\u2019s the subject of the backup set: SELECT for a table\nbackup, or USAGE for a schema backup or database backup.\nAny privilege on the parent schema or database for the subject of the backup set.\nAny privilege on the parent schema and database of the backup set.\nIf any of those privileges are missing, the automatic backup creation or expiration fails. You can\nmonitor these background operations using the ACCOUNT_USAGE.BACKUP_OPERATION_HISTORY view.\nGrant privileges required to create backup policies and sets\n\u00b6\nNote\nThe role used to grant these privileges must have the OWNERSHIP privilege on the schema,\nor it must have the CREATE BACKUP SET or CREATE BACKUP POLICY privilege WITH GRANT OPTION.\nYou can grant the following privileges to a custom account role or a database role.\nTo enable the role\nmyrole\nto create a backup policy in schema\nmyschema\n, execute the following statement:\nGRANT\nCREATE\nBACKUP\nPOLICY\nON\nSCHEMA\npolicy_schema\nTO\nROLE\nmyrole\n;\nCopy\nTo enable the role\nmyrole\nto create a backup set in schema\nmyschema\n, execute the following statement:\nGRANT\nCREATE\nBACKUP\nSET\nON\nSCHEMA\npolicy_schema\nTO\nROLE\nmyrole\n;\nCopy\nGrant the APPLY privilege on a backup policy to a role\n\u00b6\nNote\nOnly a user with the ACCOUNTADMIN role can grant this privilege.\nYou can grant this privilege to a custom account role or a database role.\nTo enable the role\nmyrole\nto apply the backup policy\nhourly_backup_policy\nto a backup set, execute the following statement:\nGRANT\nAPPLY\nON\nBACKUP\nPOLICY\nhourly_backup_policy\nTO\nROLE\nmyrole\n;\nCopy\nGrant the APPLY BACKUP RETENTION LOCK privilege to a role\n\u00b6\nYou can grant a role the privilege to apply backup policies with retention lock on backup sets.\nOnly a user with the ACCOUNTADMIN role can grant this privilege.\nImportant\nApplying a backup policy with a retention lock to a backup set is\nirreversible\n.\nDue to the strong guarantees needed for regulatory compliance, once you put a retention lock on a backup set,\nyou can\u2019t revoke the lock. Snowflake support also can\u2019t revoke such a retention lock.\nBack",
    " to a role\n\u00b6\nYou can grant a role the privilege to apply backup policies with retention lock on backup sets.\nOnly a user with the ACCOUNTADMIN role can grant this privilege.\nImportant\nApplying a backup policy with a retention lock to a backup set is\nirreversible\n.\nDue to the strong guarantees needed for regulatory compliance, once you put a retention lock on a backup set,\nyou can\u2019t revoke the lock. Snowflake support also can\u2019t revoke such a retention lock.\nBackups created with a retention lock can\u2019t be deleted until the expiration period ends.\nIf a Snowflake organization is deleted, the organization is no longer a Snowflake customer. In this case,\nSnowflake deletes all backups, including those with retention locks.\nTo enable the role\nretention_lock_admin_role\nto apply a backup policy with retention lock on a backup set, execute\nthe following statement:\nGRANT\nAPPLY\nBACKUP\nRETENTION LOCK\nON\nACCOUNT\nTO\nROLE\nretention_lock_admin_role\n;\nCopy\nCreate and configure backups\n\u00b6\nThis section provides example workflows for creating and restoring backups.\nCreate a backup policy named\nhourly_backup_policy\n. Backups taken with this policy are created hourly\nand each backup expires after 90 days.\nCREATE\nBACKUP\nPOLICY\nhourly_backup_policy\nSCHEDULE\n=\n'60 MINUTE'\nEXPIRE_AFTER_DAYS\n=\n90\nCOMMENT\n=\n'Hourly backups expire after 90 days'\n;\nCopy\nCreate a backup set for table\nt1\nwith the backup policy\nhourly_backup_policy\n:\nCREATE\nBACKUP\nSET\nt1_backups\nFOR\nTABLE\nt1\nWITH\nBACKUP\nPOLICY\nhourly_backup_policy\n;\nCopy\nCreate a backup set for schema\ns1\nwith the backup policy\nhourly_backup_policy\n:\nCREATE\nBACKUP\nSET\ns1_backups\nFOR\nSCHEMA\ns1\nWITH\nBACKUP\nPOLICY\nhourly_backup_policy\n;\nCopy\nCreate a backup set for database\nd1\nwith the backup policy\nhourly_backup_policy\n:\nCREATE\nBACKUP\nSET\nd1_backups\nFOR\nDATABASE\nd1\nWITH\nBACKUP\nPOLICY\nhourly_backup_policy\n;\nCopy\nCreate scheduled backups with retention lock\n\u00b6\nCreate a backup set that automatically creates backups with a retention lock on a schedule.\n",
    "s1\nWITH\nBACKUP\nPOLICY\nhourly_backup_policy\n;\nCopy\nCreate a backup set for database\nd1\nwith the backup policy\nhourly_backup_policy\n:\nCREATE\nBACKUP\nSET\nd1_backups\nFOR\nDATABASE\nd1\nWITH\nBACKUP\nPOLICY\nhourly_backup_policy\n;\nCopy\nCreate scheduled backups with retention lock\n\u00b6\nCreate a backup set that automatically creates backups with a retention lock on a schedule.\nThe retention lock prevents anyone, even privileged users, from deleting or modifying backups\nin any backup set that the policy is attached to.\nOnly a role that has the APPLY BACKUP RETENTION LOCK privilege on the account can create a backup policy\nwith a retention lock.\nImportant\nApplying a backup policy with a retention lock to a backup set is\nirreversible\n.\nDue to the strong guarantees needed for regulatory compliance, once you put a retention lock on a backup set,\nyou can\u2019t revoke the lock. Snowflake support also can\u2019t revoke such a retention lock.\nBackups created with a retention lock can\u2019t be deleted until the expiration period ends.\nIf a Snowflake organization is deleted, the organization is no longer a Snowflake customer. In this case,\nSnowflake deletes all backups, including those with retention locks.\nCreate a policy with a retention lock that creates a daily backup with an expiration period of 90 days:\nCREATE\nBACKUP\nPOLICY\ndaily_backup_policy_with_lock\nWITH RETENTION LOCK\nSCHEDULE\n=\n'1440 MINUTE'\nEXPIRE_AFTER_DAYS\n=\n90\nCOMMENT\n=\n'regulatory backups: they have a retention lock and expire after 90 days'\n;\nCopy\nCreate a backup set for table\nt2\nwith the backup policy\ndaily_backup_policy_with_lock\n:\nCREATE\nBACKUP\nSET\nt2_backups\nFOR\nTABLE\nt2\nWITH\nBACKUP\nPOLICY\ndaily_backup_policy_with_lock\n;\nCopy\nCreate a backup set for schema\ns2\nwith the backup policy\ndaily_backup_policy_with_lock\n:\nCREATE\nBACKUP\nSET\ns2_backups\nFOR\nSCHEMA\ns2\nWITH\nBACKUP\nPOLICY\ndaily_backup_policy_with_lock\n;\nCopy\nCreate a backup set for database\nd2\nwith the backup policy\ndaily_backup_policy_with_lock\n:\nCREATE\nBACKUP\nSET\nd2_backups\n",
    "_policy_with_lock\n;\nCopy\nCreate a backup set for schema\ns2\nwith the backup policy\ndaily_backup_policy_with_lock\n:\nCREATE\nBACKUP\nSET\ns2_backups\nFOR\nSCHEMA\ns2\nWITH\nBACKUP\nPOLICY\ndaily_backup_policy_with_lock\n;\nCopy\nCreate a backup set for database\nd2\nwith the backup policy\ndaily_backup_policy_with_lock\n:\nCREATE\nBACKUP\nSET\nd2_backups\nFOR\nDATABASE\nd2\nWITH\nBACKUP\nPOLICY\ndaily_backup_policy_with_lock\n;\nCopy\nCreate backups manually\n\u00b6\nYou can manually add a backup to a backup set at any time. Doing so makes a backup of the database, schema, or table that\u2019s\nassociated with the backup set. You can create backups manually whether or not the backup set also has backups that are\nscheduled by a backup policy. If there\u2019s a backup policy associated with the backup set,\nand the policy defines an expiration period, that expiration period also applies to the manual backup.\nThe following example creates a table backup set\nt1_backups\nand then adds the first backup to it:\nCREATE\nBACKUP\nSET\nt1_backups\nFOR\nTABLE\nt1\n;\nALTER\nBACKUP\nSET\nt1_backups\nADD\nBACKUP\n;\nCopy\nThe following example creates a backup policy with hourly backups, a table backup set\nt2_backups\nthat uses the policy, and\nthen adds a manual backup to the backup set:\nCREATE\nBACKUP\nPOLICY\nhourly_backup_policy\nSCHEDULE\n=\n'60 MINUTE'\nEXPIRE_AFTER_DAYS\n=\n7\n;\nCREATE\nBACKUP\nSET\nt2_backups\nFOR\nTABLE\nt2\nWITH\nBACKUP\nPOLICY\nhourly_backup_policy\n;\n-- Wait several hours. Then the backup set already contains several scheduled backups.\n-- You can manually add a backup at any time, in addition to the scheduled backups.\nALTER\nBACKUP\nSET\nt2_backups\nADD\nBACKUP\n;\nCopy\nYou can run similar commands to add a backup to a schema or database backup set.\nSubstitute the name of the schema or database backup set in the ALTER BACKUP SET command.\nSuspend a backup policy on a backup set\n\u00b6\nWhen you suspend a backup policy on a backup set, you prevent the",
    "-- You can manually add a backup at any time, in addition to the scheduled backups.\nALTER\nBACKUP\nSET\nt2_backups\nADD\nBACKUP\n;\nCopy\nYou can run similar commands to add a backup to a schema or database backup set.\nSubstitute the name of the schema or database backup set in the ALTER BACKUP SET command.\nSuspend a backup policy on a backup set\n\u00b6\nWhen you suspend a backup policy on a backup set, you prevent the backup policy from being used to create\nnew scheduled backups in that backup set. You also suspend the expiration of existing backups in that\nbackup set that use the backup policy. Other backup sets that use the same policy aren\u2019t affected.\nThe following example suspends a backup policy on the backup set\nt2_backups\n:\nALTER\nBACKUP\nSET\nt2_backups\nSUSPEND\nBACKUP\nPOLICY\n;\nCopy\nYou can also selectively suspend just the creation or just the expiration processes of the backup set.\nThe following example suspends the creation of new backups in the backup set\nt3_backups\n, and\nsuspends expiration of old backups from the backup set\nt4_backups\n:\nALTER\nBACKUP\nSET\nt3_backups\nSUSPEND\nBACKUP\nCREATION\nPOLICY\n;\nALTER\nBACKUP\nSET\nt4_backups\nSUSPEND\nBACKUP\nEXPIRATION\nPOLICY\n;\nCopy\nFor more information about the ALTER BACKUP SET command, see\nALTER BACKUP SET\n.\nResume a backup policy on a backup set\n\u00b6\nYou can resume suspended backup policies. Doing so resumes the creation and expiration of backups according to the backup\npolicy. If any backups reached their expiration time while the policy was suspended, Snowflake deletes those backups as soon as\nthe policy is resumed.\nThe following example resumes a backup policy on the backup set\nt1_backup\n:\nALTER\nBACKUP\nSET\nt1_backups\nRESUME\nBACKUP\nPOLICY\n;\nCopy\nYou can also selectively resume just the creation or just the expiration processes of the backup set.\nThe following example resumes the creation of new backups in the backup set\nt3_backups\n, and\nresumes expiration of old backups from the backup set\nt4_backups\n:\nALTER\nBACKUP\nSET\nt3_backups\nSUSPEND\nBACK",
    "\n:\nALTER\nBACKUP\nSET\nt1_backups\nRESUME\nBACKUP\nPOLICY\n;\nCopy\nYou can also selectively resume just the creation or just the expiration processes of the backup set.\nThe following example resumes the creation of new backups in the backup set\nt3_backups\n, and\nresumes expiration of old backups from the backup set\nt4_backups\n:\nALTER\nBACKUP\nSET\nt3_backups\nSUSPEND\nBACKUP\nCREATION\nPOLICY\n;\nALTER\nBACKUP\nSET\nt4_backups\nSUSPEND\nBACKUP\nEXPIRATION\nPOLICY\n;\nCopy\nFor more information about the ALTER BACKUP SET command, see\nALTER BACKUP SET\n.\nRestore a backup\n\u00b6\nYou can restore an object from a backup set by using the ID of the specific backup.\nFor example, to restore table\nt1\nfrom backup set\nt1_backups\nin the current schema,\nexecute the following statements:\nFind the ID of the table backup to restore in the\nbackup_id\ncolumn:\nSHOW\nBACKUPS\nIN\nBACKUP\nSET\nt1_backups\n->>\nSELECT\n\"created_on\"\n,\n\"backup_id\"\n,\n\"expire_on\"\nFROM\n$\n1\n;\nCopy\n+-------------------------------+--------------------------------------+-------------------------------+\n| created_on                    | backup_id                            | expire_on                     |\n|-------------------------------+------------------------------------------+---------------------------|\n| 2024-08-19 17:12:28.991 -0700 | 983e0b66-91eb-41cb-8a0b-037abfec1914 | 2024-08-20 17:12:28.991 -0700 |\n| 2024-08-19 18:12:33.824 -0700 | b5624ef0-1f35-452f-b132-09d8f0592e52 | 2024-08-20 18:12:33.824 -0700 |\n| 2024-08-19 19:12:43.830 -0700 | eca1a94a-fd40-46db-a2bc-4afba6a38c0a | 2024-08-20 19:12:43.830 -0700 |\n| 2024-08-",
    "09d8f0592e52 | 2024-08-20 18:12:33.824 -0700 |\n| 2024-08-19 19:12:43.830 -0700 | eca1a94a-fd40-46db-a2bc-4afba6a38c0a | 2024-08-20 19:12:43.830 -0700 |\n| 2024-08-19 20:12:45.446 -0700 | 8ee2fd7e-1afe-42e1-acd7-79582765a910 | 2024-08-20 20:12:45.446 -0700 |\n| 2024-08-19 21:12:55.305 -0700 | d38caf14-f8a5-4ba8-a248-8287e0cdcf40 | 2024-08-20 21:12:55.305 -0700 |\n+-------------------------------+--------------------------------------+-----------+-------------------+\nFind the ID of the schema backup to restore in the\nbackup_id\ncolumn:\nSHOW\nBACKUPS\nIN\nBACKUP\nSET\ns1_backups\n;\nCopy\n+-------------------------------+--------------------------------------+-------------------------------+\n| created_on                    | backup_id                            | expire_on                     |\n|-------------------------------+--------------------------------------+-------------------------------|\n| 2024-08-19 17:12:28.991 -0700 | 0a0382e1-d265-46e9-b152-4c3b2b859e65 | 2024-08-20 17:12:28.991 -0700 |\n| 2024-08-19 18:12:33.824 -0700 | 8dbcf919-3393-4590-928f-5481d7f2502f | 2024-08-20 18:12:33.824 -0700 |\n| 2024-08-19 19:12:43.830 -0700 | 8ee2fd7e-1afe-42e1-acd7-79582765a910 | 2024-08-20 19:12:43.830 -0700 |\n| 2024-08-19 20:12",
    "7f2502f | 2024-08-20 18:12:33.824 -0700 |\n| 2024-08-19 19:12:43.830 -0700 | 8ee2fd7e-1afe-42e1-acd7-79582765a910 | 2024-08-20 19:12:43.830 -0700 |\n| 2024-08-19 20:12:45.446 -0700 | bd729a79-01bc-444d-a550-adaaa31ab62f | 2024-08-20 20:12:45.446 -0700 |\n| 2024-08-19 21:12:55.305 -0700 | 9a8802c5-5fbd-4200-a09d-43e046103939 | 2024-08-20 21:12:55.305 -0700 |\n+-------------------------------+--------------------------------------+-------------------------------+\nFind the ID of the database backup to restore in the\nbackup_id\ncolumn:\nSHOW\nBACKUPS\nIN\nBACKUP\nSET\nd1_backups\n;\nCopy\n+-------------------------------+--------------------------------------+-------------------------------+\n| created_on                    | backup_id                            | expire_on                     |\n|-------------------------------+--------------------------------------+-------------------------------|\n| 2024-08-19 17:12:28.991 -0700 | 42435925-4e77-4b01-ba89-8163ac03e12f | 2024-08-20 17:12:28.991 -0700 |\n| 2024-08-19 18:12:33.824 -0700 | 29c2c1b9-6599-4f0b-87b8-d43377fd7c77 | 2024-08-20 18:12:33.824 -0700 |\n| 2024-08-19 19:12:43.830 -0700 | a4283984-a063-4415-acc4-0e3c19259fad | 2024-08-20 19:12:43.830 -0700 |\n| 2024-08-19 20:12:45.446 -0700 | ffe253",
    "4-08-20 18:12:33.824 -0700 |\n| 2024-08-19 19:12:43.830 -0700 | a4283984-a063-4415-acc4-0e3c19259fad | 2024-08-20 19:12:43.830 -0700 |\n| 2024-08-19 20:12:45.446 -0700 | ffe25397-64b9-4c5f-b061-23a1885dc2dc | 2024-08-20 20:12:45.446 -0700 |\n| 2024-08-19 21:12:55.305 -0700 | 28e12b8a-aab8-40a8-ae39-9a5a5f654d66 | 2024-08-20 21:12:55.305 -0700 |\n+-------------------------------+--------------------------------------+-------------------------------+\nRestore the backup for table\nt1\ntaken on 2024-08-19 18:12:33:\nCREATE\nTABLE\nrestored_t1\nFROM\nBACKUP\nSET\nt1_backups\nIDENTIFIER\n'b5624ef0-1f35-452f-b132-09d8f0592e52'\n;\nCopy\nRestore the backup for schema\ns1\ntaken on 2024-08-19 18:12:33:\nCREATE\nSCHEMA\nrestored_s1\nFROM\nBACKUP\nSET\ns1_backups\nIDENTIFIER\n'8dbcf919-3393-4590-928f-5481d7f2502f'\n;\nCopy\nRestore the backup for database\nd1\ntaken on 2024-08-19 18:12:33:\nCREATE\nDATABASE\nrestored_d1\nFROM\nBACKUP\nSET\nd1_backups\nIDENTIFIER\n'29c2c1b9-6599-4f0b-87b8-d43377fd7c77'\n;\nCopy\nDelete a backup from a backup set\n\u00b6\nFor any backup set, you can only delete the oldest backup that doesn\u2019t have a legal hold. You do so by specifying the backup\nID. You can find the backups that don\u2019t have a legal",
    " is set to one hour, a backup of the object is taken every 60 minutes.\nThe\nexpiration period\nis the length of time the backup is valid. After a backup expires,\nSnowflake deletes it automatically, unless a legal hold is applied to that particular backup.\nTip\nIf the backup set doesn\u2019t have a retention lock and the particular backup doesn\u2019t have a legal\nhold applied, you can delete the backup manually before the end of the expiration period.\nYou can manually delete backups one at a time, always starting with the oldest backup that\ndoesn\u2019t have a legal hold.\nEach backup policy must have one or both of the schedule and expiration period properties. For example, you can\ncreate a policy with a schedule and an expiration period, and let Snowflake handle all creation and removal\nof the backups in all backup sets where that policy is applied. Alternatively, you might\ncreate a policy with a schedule and no expiration period if you want to manage removing older backups yourself.\nOr, you can create a policy with an expiration period but without a schedule, and then manage\nbackup creation yourself. You can\u2019t create a policy with no schedule and no expiration period.\nIf you associate a backup policy with a backup set, you can do so when you create the backup set, or you can apply the policy\nlater. Or, you can have a backup set that doesn\u2019t have an associated backup policy. In that case, you manually control when to take\nnew backups and expire old ones.\nYou can apply a backup policy to multiple backup sets. If you modify a backup policy, Snowflake applies the changes to all\nbackup sets that the policy is attached to.\nRetention lock\n\u00b6\nA\nretention lock\nprotects a backup from deletion for the defined expiration period.\nYou can use a backup with a retention lock for backups for regulatory compliance and cyber resilience.\nThe following restrictions apply for a backup set with retention lock:\nBackups can\u2019t be deleted by any role, including the ACCOUNTADMIN role.\nYou can\u2019t decrease the backup expiration period, although you can increase the expiration period.\nYou can\u2019t drop a backup set if there are any unexpired backups in the set.\nYou can\u2019t drop a schema that contains a backup set with any unexpired backups.\nYou can\u2019t drop a database that contains a backup set with any unexpired backups.\nYou can\u2019t drop an account that contains a database with a backup set that has any unexpired backups.\nImportant\n",
    "_d1\nFROM\nBACKUP\nSET\nd1_backups\nIDENTIFIER\n'29c2c1b9-6599-4f0b-87b8-d43377fd7c77'\n;\nCopy\nDelete a backup from a backup set\n\u00b6\nFor any backup set, you can only delete the oldest backup that doesn\u2019t have a legal hold. You do so by specifying the backup\nID. You can find the backups that don\u2019t have a legal hold by examining the\nis_under_legal_hold\nproperty. You can find the\noldest backup by examining the\ncreated_on\nproperty.\nNote\nYou can\u2019t delete any backup from a backup set if a backup policy with retention lock is attached to that backup set,\nor if that particular backup has a legal hold applied.\nThe backup that you delete from the backup set must be the earliest backup in the set.\nFind the ID of the table backup to delete in the\nbackup_id\ncolumn in the following output.\nSorting in ascending order by the\ncreated_on\ncolumn puts the oldest backup first.\nYou could add\nLIMIT\n1\nto the SELECT command to return only the row with the details of the oldest backup.\nSHOW\nBACKUPS\nIN\nBACKUP\nSET\nt1_backups\n->>\nSELECT\n\"created_on\"\n,\n\"backup_id\"\n,\n\"expire_on\"\nFROM\n$\n1\nWHERE\n\"is_under_legal_hold\"\n=\n'N'\nORDER\nBY\n\"created_on\"\n;\nCopy\n+-------------------------------+--------------------------------------+-------------------------------+\n| created_on                    | backup_id                            | expire_on                     |\n|-------------------------------+--------------------------------------+-------------------------------|\n| 2024-08-19 17:12:28.991 -0700 | 983e0b66-91eb-41cb-8a0b-037abfec1914 | 2024-08-20 17:12:28.991 -0700 |\n| 2024-08-19 18:12:33.824 -0700 | b5624ef0-1f35-452f-b132-09d8f0592e52 | 2024-08-20 18:12:33.824 -0700 |\n| 2024-08-19 19:12:43.830 -0700 | eca1a94a-fd40-46db-a2",
    "0700 |\n| 2024-08-19 18:12:33.824 -0700 | b5624ef0-1f35-452f-b132-09d8f0592e52 | 2024-08-20 18:12:33.824 -0700 |\n| 2024-08-19 19:12:43.830 -0700 | eca1a94a-fd40-46db-a2bc-4afba6a38c0a | 2024-08-20 19:12:43.830 -0700 |\n| 2024-08-19 20:12:45.446 -0700 | 8ee2fd7e-1afe-42e1-acd7-79582765a910 | 2024-08-20 20:12:45.446 -0700 |\n| 2024-08-19 21:12:55.305 -0700 | d38caf14-f8a5-4ba8-a248-8287e0cdcf40 | 2024-08-20 21:12:55.305 -0700 |\n+-------------------------------+--------------------------------------+-------------------------------+\nDelete the\nt1_backups\nbackup created on 2024-08-19 17:12:28 using the\nbackup_id\n:\nALTER\nBACKUP\nSET\nt1_backups\nDELETE\nBACKUP\nIDENTIFIER\n'983e0b66-91eb-41cb-8a0b-037abfec1914'\n;\nCopy\nFind the ID of the schema backup to delete in the\nbackup_id\ncolumn in the following output:\nSHOW\nBACKUPS\nIN\nBACKUP\nSET\ns1_backups\n->>\nSELECT\n\"created_on\"\n,\n\"backup_id\"\n,\n\"expire_on\"\nFROM\n$\n1\nORDER\nBY\n\"created_on\"\n;\nCopy\n+-------------------------------+--------------------------------------+-------------------------------+\n| created_on                    | backup_id                            | expire_on                     |\n|-------------------------------+--------------------------------------+-------------------------------|\n| 2024-08-19 17:12:28.991 -0700 | 28e12b8a-aab8-40a8-ae39-9a5a5f654d66 | 2024-08-",
    "\n$\n1\nORDER\nBY\n\"created_on\"\n;\nCopy\n+-------------------------------+--------------------------------------+-------------------------------+\n| created_on                    | backup_id                            | expire_on                     |\n|-------------------------------+--------------------------------------+-------------------------------|\n| 2024-08-19 17:12:28.991 -0700 | 28e12b8a-aab8-40a8-ae39-9a5a5f654d66 | 2024-08-20 17:12:28.991 -0700 |\n| 2024-08-19 18:12:33.824 -0700 | 46a1e22a-8557-432f-a14c-1261a4ca2b34 | 2024-08-20 18:12:33.824 -0700 |\n| 2024-08-19 19:12:43.830 -0700 | 3e42fef6-b895-4055-a59f-179744d015d3 | 2024-08-20 19:12:43.830 -0700 |\n| 2024-08-19 20:12:45.446 -0700 | 7807d24e-285e-4741-b332-87c32bad5cb6 | 2024-08-20 20:12:45.446 -0700 |\n| 2024-08-19 21:12:55.305 -0700 | e022e619-ee83-45a0-b2b7-9007e284bdb3 | 2024-08-20 21:12:55.305 -0700 |\n+-------------------------------+--------------------------------------+-------------------------------+\nDelete the\ns1_backups\nbackup created on 2024-08-19 17:12:28 using the\nbackup_id\n:\nALTER\nBACKUP\nSET\ns1_backups\nDELETE\nBACKUP\nIDENTIFIER\n'28e12b8a-aab8-40a8-ae39-9a5a5f654d66'\n;\nCopy\nFind the ID of the database backup to delete in the\nbackup_id\ncolumn in the following output:\nSHOW\nBACKUPS\nIN\nBACKUP\nSET\nd1_backups\n->>\nSELECT\n\"created_on\"\n",
    "_id\n:\nALTER\nBACKUP\nSET\ns1_backups\nDELETE\nBACKUP\nIDENTIFIER\n'28e12b8a-aab8-40a8-ae39-9a5a5f654d66'\n;\nCopy\nFind the ID of the database backup to delete in the\nbackup_id\ncolumn in the following output:\nSHOW\nBACKUPS\nIN\nBACKUP\nSET\nd1_backups\n->>\nSELECT\n\"created_on\"\n,\n\"backup_id\"\n,\n\"expire_on\"\nFROM\n$\n1\nORDER\nBY\n\"created_on\"\n;\nCopy\n+-------------------------------+--------------------------------------+-------------------------------+\n| created_on                    | backup_id                            | expire_on                     |\n|-------------------------------+--------------------------------------+-------------------------------|\n| 2024-08-19 17:12:28.991 -0700 | d3a77432-c98d-4969-91a9-fffae5dd655c | 2024-08-20 17:12:28.991 -0700 |\n| 2024-08-19 18:12:33.824 -0700 | 0a0382e1-d265-46e9-b152-4c3b2b859e65 | 2024-08-20 18:12:33.824 -0700 |\n| 2024-08-19 19:12:43.830 -0700 | 25e01ee0-ea9d-4bb7-af7f-f3fe87f9409e | 2024-08-20 19:12:43.830 -0700 |\n| 2024-08-19 20:12:45.446 -0700 | a12294f5-fc63-49cf-84f1-c7b72f7664af | 2024-08-20 20:12:45.446 -0700 |\n| 2024-08-19 21:12:55.305 -0700 | 28e12b8a-aab8-40a8-ae39-9a5a5f654d66 | 2024-08-20 21:12:55.305 -0700 |\n+-------------------------------+--------------------------------------+-------------------------------+\nDelete the\nd1_backups\nbackup created",
    "-20 20:12:45.446 -0700 |\n| 2024-08-19 21:12:55.305 -0700 | 28e12b8a-aab8-40a8-ae39-9a5a5f654d66 | 2024-08-20 21:12:55.305 -0700 |\n+-------------------------------+--------------------------------------+-------------------------------+\nDelete the\nd1_backups\nbackup created on 2024-08-19 17:12:28 using the\nbackup_id\n:\nALTER\nBACKUP\nSET\nd1_backups\nDELETE\nBACKUP\nIDENTIFIER\n'd3a77432-c98d-4969-91a9-fffae5dd655c'\n;\nCopy\nAttempt to delete a more recent\nd1_backups\nbackup created on 2024-08-19 21:12:55. Notice how Snowflake\nprevents you from deleting a backup other than the oldest one in the backup set.\nALTER\nBACKUP\nSET\nd1_backups\nDELETE\nBACKUP\nIDENTIFIER\n'28e12b8a-aab8-40a8-ae39-9a5a5f654d66'\n;\nCopy\nBackup '28e12b8a-aab8-40a8-ae39-9a5a5f654d66' cannot be deleted as it is not the oldest active backup in the backup set D1_BACKUPS.\nDelete a backup set\n\u00b6\nYou can delete a backup set using the\nDROP BACKUP SET\ncommand.\nNote\nYou can\u2019t delete a backup set that has a retention lock and contains unexpired backups.\nYou also can\u2019t delete a backup set if any of its backups has a legal hold.\nDelete the\nt1_backups\nbackup set:\nDROP\nBACKUP\nSET\nt1_backups\n;\nCopy\nDelete the\ns1_backups\nbackup set:\nDROP\nBACKUP\nSET\ns1_backups\n;\nCopy\nDelete the\nd1_backups\nbackup set:\nDROP\nBACKUP\nSET\nd1_backups\n;\nCopy\nFind all the backup sets that contain backups of a specific table\n\u00b6\nThe following example shows how to find all the backup sets that contain a specific table inside a specific schema and database.\nThe SHOW TABLES",
    "_backups\n;\nCopy\nDelete the\ns1_backups\nbackup set:\nDROP\nBACKUP\nSET\ns1_backups\n;\nCopy\nDelete the\nd1_backups\nbackup set:\nDROP\nBACKUP\nSET\nd1_backups\n;\nCopy\nFind all the backup sets that contain backups of a specific table\n\u00b6\nThe following example shows how to find all the backup sets that contain a specific table inside a specific schema and database.\nThe SHOW TABLES command uses a pipe operator to retrieve the names of the database, schema, and table and store them in variables.\nThe SHOW BACKUP SETS output is filtered to show the backup sets that back up the database containing the table, or the schema\ncontaining the table, or that contain that single table.\nThe filtered output from SHOW BACKUP SETS shows that there are two database backup sets for the\ndatabase\nmy_big_important_database\n, one schema backup set for the schema\nmy_big_important_database.public\n, and one table backup set for the table\nmy_big_important_database.public.my_small_secondary_table\n.\nSHOW\nTABLES\nIN\nSCHEMA\npublic\n->>\nSET\n(\ndname\n,\nsname\n,\ntname\n)\n=\n(\nSELECT\n\"database_name\"\n,\n\"schema_name\"\n,\n\"name\"\nFROM\n$\n1\nWHERE\n\"name\"\n=\n'MY_SMALL_SECONDARY_TABLE'\nAND\n\"kind\"\n=\n'TABLE'\n);\nSHOW\nBACKUP\nSETS\n->>\nSELECT\n\"object_kind\"\n,\n\"name\"\n,\n\"database_name\"\n,\n\"schema_name\"\n,\n\"object_name\"\nFROM\n$\n1\nWHERE\n(\n\"object_kind\"\n=\n'TABLE'\nAND\n\"database_name\"\n=\n$\ndname\nAND\n\"schema_name\"\n=\n$\nsname\nAND\n\"object_name\"\n=\n$\ntname\n)\nOR\n(\n\"object_kind\"\n=\n'SCHEMA'\nAND\n\"database_name\"\n=\n$\ndname\nAND\n\"object_name\"\n=\n$\nsname\n)\nOR\n(\n\"object_kind\"\n=\n'DATABASE'\nAND\n\"object_name\"\n=\n$\ndname\n);\nCopy\n+-------------+------------------+---------------------------+-------------+---------------------------+\n| object_kind | name             | database_name             | schema_name | object_name               |\n|-------------+------------------+---------------------------+-------------+---------------------------|\n| DATABASE    | DATABASE_BACKUP  | MY_BIG_IMPORTANT",
    "\"\n=\n$\ndname\nAND\n\"object_name\"\n=\n$\nsname\n)\nOR\n(\n\"object_kind\"\n=\n'DATABASE'\nAND\n\"object_name\"\n=\n$\ndname\n);\nCopy\n+-------------+------------------+---------------------------+-------------+---------------------------+\n| object_kind | name             | database_name             | schema_name | object_name               |\n|-------------+------------------+---------------------------+-------------+---------------------------|\n| DATABASE    | DATABASE_BACKUP  | MY_BIG_IMPORTANT_DATABASE | PUBLIC      | MY_BIG_IMPORTANT_DATABASE |\n| DATABASE    | DATABASE_BACKUP2 | MY_BIG_IMPORTANT_DATABASE | PUBLIC      | MY_BIG_IMPORTANT_DATABASE |\n| SCHEMA      | SCHEMA_BACKUP3   | MY_BIG_IMPORTANT_DATABASE | PUBLIC      | PUBLIC                    |\n| TABLE       | TABLE_BACKUP2    | MY_BIG_IMPORTANT_DATABASE | PUBLIC      | MY_SMALL_SECONDARY_TABLE  |\n+-------------+------------------+---------------------------+-------------+---------------------------+\nCreate a backup for a table with dependencies\n\u00b6\nThe following examples show how you might create a table backup for a table\nthat refers to a sequence and a foreign key in a different schema. To prepare,\nwe create the schema\nother_schema\ncontaining a sequence and a table. Then we create the\nmain table in the\npublic\nschema, referring to the sequence and the other table.\nUSE\nDATABASE\nmy_big_important_database\n;\nCREATE\nSCHEMA\nother_schema\n;\nUSE\nSCHEMA\nother_schema\n;\nCREATE\nSEQUENCE\nmy_sequence\n;\nCREATE\nTABLE\nmy_dimension_table\n(\nid\nINT\nAUTOINCREMENT\nPRIMARY KEY\n);\nUSE\nSCHEMA\npublic\n;\nCREATE\nTABLE\ndependent_table\n(\nid\nINT\nDEFAULT\nmy_big_important_database\n.\nother_schema\n.\nmy_sequence\n.\nNEXTVAL\nPRIMARY KEY\n,\nforeign_id\nINT\n,\nFOREIGN\nKEY\n(\nforeign_id\n)\nREFERENCES\nmy_big_important_database\n.\nother_schema\n.\nmy_dimension_table\n(\nid\n)\n);\nSELECT\nGET_DDL\n(\n'TABLE'\n,\n'dependent_table'\n);\nCopy\nThe GET_DDL() output shows the references that point to the other schema:\n+-------------------------------------------+\n| GET_DDL('TABLE','DEPENDENT_TABLE')        |\n|-------------------------------------------|\n| create or replace TABLE DEPENDENT_TABLE ( |\n|     ID NUMBER(38,0) NOT NULL DEFAULT MY",
    "my_big_important_database\n.\nother_schema\n.\nmy_dimension_table\n(\nid\n)\n);\nSELECT\nGET_DDL\n(\n'TABLE'\n,\n'dependent_table'\n);\nCopy\nThe GET_DDL() output shows the references that point to the other schema:\n+-------------------------------------------+\n| GET_DDL('TABLE','DEPENDENT_TABLE')        |\n|-------------------------------------------|\n| create or replace TABLE DEPENDENT_TABLE ( |\n|     ID NUMBER(38,0) NOT NULL DEFAULT MY_BIG_IMPORTANT_DATABASE.OTHER_SCHEMA.MY_SEQUENCE.NEXTVAL,\n|     FOREIGN_ID NUMBER(38,0),                |\n|     primary key (ID),                       |\n|     foreign key (FOREIGN_ID) references MY_BIG_IMPORTANT_DATABASE.OTHER_SCHEMA.MY_DIMENSION_TABLE(ID)\n| );                                        |\n+-------------------------------------------+\nNext, we create the backup set for the table and add a backup to it:\nCREATE\nBACKUP\nSET\ndependency_experiments\nFOR\nTABLE\ndependent_table\n;\nALTER\nBACKUP\nSET\ndependency_experiments\nADD\nBACKUP\n;\nSHOW\nBACKUPS\nIN\nBACKUP\nSET\ndependency_experiments\n;\nCopy\nThe SHOW BACKUPS output contains the\nbackup_id\nvalue to use for the restore operation:\n+-------------------------------+--------------------------------------+------------------------+---------------------------+--------------+-----------+\n| created_on                    | backup_id                            | backup_set_name        | database_name             | schema_name  | expire_on |\n|-------------------------------+--------------------------------------+------------------------+---------------------------+--------------+-----------|\n| 2025-07-01 11:53:27.860 -0700 | 0fd44138-b571-449b-be0a-72779501f80e | DEPENDENCY_EXPERIMENTS | MY_BIG_IMPORTANT_DATABASE | OTHER_SCHEMA | NULL      |\n+-------------------------------+--------------------------------------+------------------------+---------------------------+--------------+-----------+\nWe restore that table under a new name, and confirm that the restored table refers to\nthe objects in the other schema:\nCREATE\nTABLE\nrestored_dependent_table\nFROM\nBACKUP\nSET\ndependency_experiments\nIDENTIFIER\n'0fd44138-b571-449b-be0a-72779501f80e'\n;\nSELECT\nGET_DDL\n(\n'TABLE'\n,\n'restored_dependent_table'\n);\nCopy\n+----------------------------------------------------+\n| GET_DDL('TABLE','RESTORED_DEPENDENT_TABLE",
    " name, and confirm that the restored table refers to\nthe objects in the other schema:\nCREATE\nTABLE\nrestored_dependent_table\nFROM\nBACKUP\nSET\ndependency_experiments\nIDENTIFIER\n'0fd44138-b571-449b-be0a-72779501f80e'\n;\nSELECT\nGET_DDL\n(\n'TABLE'\n,\n'restored_dependent_table'\n);\nCopy\n+----------------------------------------------------+\n| GET_DDL('TABLE','RESTORED_DEPENDENT_TABLE')        |\n|----------------------------------------------------|\n| create or replace TABLE RESTORED_DEPENDENT_TABLE ( |\n|     ID NUMBER(38,0) NOT NULL DEFAULT MY_BIG_IMPORTANT_DATABASE.OTHER_SCHEMA.MY_SEQUENCE.NEXTVAL,\n|     FOREIGN_ID NUMBER(38,0),                         |\n|     foreign key (FOREIGN_ID) references MY_BIG_IMPORTANT_DATABASE.OTHER_SCHEMA.MY_DIMENSION_TABLE(ID),\n|     primary key (ID)                                 |\n| );                                                 |\n+----------------------------------------------------+\nTo illustrate what happens if the referred-to object no longer exists, we drop the sequence\nand then restore the table again from the same backup:\nDROP\nSEQUENCE\nmy_big_important_database\n.\nother_schema\n.\nmy_sequence\n;\nCREATE\nTABLE\nOR\nREPLACE\nrestored_dependent_table\nFROM\nBACKUP\nSET\ndependency_experiments\nIDENTIFIER\n'0fd44138-b571-449b-be0a-72779501f80e'\n;\nSELECT\n*\nFROM\nrestored_dependent_table\n;\nCopy\nQuerying the table still works:\n+----+------------+\n| ID | FOREIGN_ID |\n|----+------------|\n+----+------------+\n0 Row(s) produced. Time Elapsed: 0.129s\nHowever, operations such as GET_DDL(), DESCRIBE, and INSERT all fail because they\ndepend on a sequence that no longer exists:\nSELECT\nGET_DDL\n(\n'TABLE'\n,\n'restored_dependent_table'\n);\nCopy\n002073 (02000): SQL compilation error:\nSequence used as a default value in table 'MY_BIG_IMPORTANT_DATABASE.OTHER_SCHEMA.RESTORED_DEPENDENT_TABLE'\ncolumn 'ID' was not found or could not be accessed.\nDESC\nTABLE\nrestored_dependent_table\n;\nCopy\n+------------+--------------+--------+-------+----------------------------------------+-------------+------------+-------+------------+---------+-------------+----------------+\n| name       | type         |",
    "_dependent_table'\n);\nCopy\n002073 (02000): SQL compilation error:\nSequence used as a default value in table 'MY_BIG_IMPORTANT_DATABASE.OTHER_SCHEMA.RESTORED_DEPENDENT_TABLE'\ncolumn 'ID' was not found or could not be accessed.\nDESC\nTABLE\nrestored_dependent_table\n;\nCopy\n+------------+--------------+--------+-------+----------------------------------------+-------------+------------+-------+------------+---------+-------------+----------------+\n| name       | type         | kind   | null? | default                                | primary key | unique key | check | expression | comment | policy name | privacy domain |\n|------------+--------------+--------+-------+----------------------------------------+-------------+------------+-------+------------+---------+-------------+----------------|\n| ID         | NUMBER(38,0) | COLUMN | N     | [sequence cannot be found or accessed] | Y           | N          | NULL  | NULL       | NULL    | NULL        | NULL           |\n| FOREIGN_ID | NUMBER(38,0) | COLUMN | Y     | NULL                                   | N           | N          | NULL  | NULL       | NULL    | NULL        | NULL           |\n+------------+--------------+--------+-------+----------------------------------------+-------------+------------+-------+------------+---------+-------------+----------------+\nINSERT\nINTO\nrestored_dependent_table\n(\nforeign_id\n)\nVALUES\n(\n2\n);\nCopy\n002073 (02000): SQL compilation error:\nSequence used as a default value in table 'MY_BIG_IMPORTANT_DATABASE.OTHER_SCHEMA.RESTORED_DEPENDENT_TABLE'\ncolumn 'ID' was not found or could not be accessed.\nCreate a backup for a dynamic table\n\u00b6\nA dynamic table always involves a reference to some other table. For that reason, you might prefer\nto use schema backups or database backups for dynamic tables, so that the original table and\nthe dynamic table can be included in the same backup.\nIf you make a table backup for a dynamic table, you include the keyword DYNAMIC in the CREATE BACKUP\nSET command, and in the CREATE TABLE command when you restore from a backup. The following example\nsets up the dynamic table, a table backup set for that table, and creates the first backup:\nCREATE\nDYNAMIC TABLE\nmy_dynamic_table\nTARGET_LAG\n=\n'1 minute'\nWAREHOUSE\n=\nmy_wh\nAS\nSELECT\n*\nFROM\nmy_base_table\nWHERE\ncol1\n",
    " role, including the ACCOUNTADMIN role.\nYou can\u2019t decrease the backup expiration period, although you can increase the expiration period.\nYou can\u2019t drop a backup set if there are any unexpired backups in the set.\nYou can\u2019t drop a schema that contains a backup set with any unexpired backups.\nYou can\u2019t drop a database that contains a backup set with any unexpired backups.\nYou can\u2019t drop an account that contains a database with a backup set that has any unexpired backups.\nImportant\nApplying a backup policy with a retention lock to a backup set is\nirreversible\n.\nDue to the strong guarantees that are needed for regulatory compliance, after you put a retention lock on a backup set,\nyou can\u2019t revoke the lock. Snowflake support also can\u2019t revoke such a retention lock. Plan carefully before\nyou set a retention lock on a backup set with a long expiration period, to avoid unexpected storage charges\nfor undeletable backup sets, and the schemas and databases that contain them.\nIf a Snowflake organization is deleted, the organization is no longer a Snowflake customer. In this case,\nSnowflake deletes all backups, including those with retention locks. Deleting a Snowflake organization\nrequires the involvement of Snowflake support. It isn\u2019t something that an administrator can do by accident.\nLegal hold\n\u00b6\nThe\nlegal hold\nfeature of Snowflake backups prevents backups from being overwritten or deleted.\nThat way, you can preserve Snowflake databases, schemas, or tables based on your own legal requirements.\nSnowflake lets you place a legal hold on specific backups.\nWhen a Snowflake backup is under legal hold, the following conditions apply:\nNobody can modify the backup.\nNobody can delete the backup. That\u2019s true even if the backup has passed its EXPIRE_AFTER_DAYS period.\nAccess to the backup is logged and auditable.\nThe legal hold can be removed by a privileged user, unlike a retention lock.\nImportant\nIf you replicate a backup set, make sure to perform a refresh immediately after placing a legal hold on a backup\nin that backup set. If you perform a failover before you replicate the backup set that contains the legal hold, the original\nbackup set can be overwritten when you fail back to the original primary account, potentially erasing the legal hold.\nOverview of the backup lifecycle\n\u00b6\nThe following diagram shows how the Snowflake objects, backups, backup sets, and backup policies relate to each other.\nThe diagram involves the simplest kind of",
    " backup for a dynamic table, you include the keyword DYNAMIC in the CREATE BACKUP\nSET command, and in the CREATE TABLE command when you restore from a backup. The following example\nsets up the dynamic table, a table backup set for that table, and creates the first backup:\nCREATE\nDYNAMIC TABLE\nmy_dynamic_table\nTARGET_LAG\n=\n'1 minute'\nWAREHOUSE\n=\nmy_wh\nAS\nSELECT\n*\nFROM\nmy_base_table\nWHERE\ncol1\nIS\nNOT\nNULL\n;\nCREATE\nBACKUP\nSET\ndynamic_table_backups\nFOR\nDYNAMIC TABLE\nmy_dynamic_table\n;\nALTER\nBACKUP\nSET\ndynamic_table_backups\nADD\nBACKUP\n;\nCopy\nThe following example shows how to determine the backup IDs for backups created at various times.\nIn this case, the newest backup is the first row in the result set. Then you use the ID of the backup\nin the CREATE DYNAMIC TABLE command.\nSHOW\nBACKUPS\nIN\nBACKUP\nSET\ndynamic_table_backups\n->>\nSELECT\n\"created_on\"\n,\n\"backup_id\"\nFROM\n$\n1\nORDER\nBY\n\"created_on\"\nDESC\n;\nCREATE\nDYNAMIC TABLE\nrestored_dynamic_table\nFROM\nBACKUP\nSET\ndynamic_table_backups\nIDENTIFIER\n'<backup_id_from_SHOW_BACKUPS_output>'\n;\nCopy\nTip\nWhen you restore a dynamic table from a backup, Snowflake\nautomatically initializes\nthe new table during its first refresh.\nAdd and remove legal holds\n\u00b6\nBefore you work with legal holds for Snowflake backups, learn their purpose and requirements. For more information, see\nLegal hold\n.\nSuppose that your organization\u2019s legal or compliance team sends a litigation hold request, specifying what types of data need to be\npreserved. In that case, you might follow a process as follows:\nYou work with the legal team to identify where the relevant data is stored, and which backup sets contain the associated objects.\nYou put a legal hold on a backup from the applicable timeframe within a backup set. Doing so disables any automatic expiration for that backup.\nYou can put a legal hold on a backup that Snowflake created automatically based on a schedule, or that you created manually. The\nlegal hold applies whether or not the backup set has an associated backup policy, or an expiration period, or a retention lock.\nYou perform refresh operations for any secondary accounts where the database",
    ", and which backup sets contain the associated objects.\nYou put a legal hold on a backup from the applicable timeframe within a backup set. Doing so disables any automatic expiration for that backup.\nYou can put a legal hold on a backup that Snowflake created automatically based on a schedule, or that you created manually. The\nlegal hold applies whether or not the backup set has an associated backup policy, or an expiration period, or a retention lock.\nYou perform refresh operations for any secondary accounts where the database containing the backup set is replicated.\nThat way, the legal hold and associated backup are preserved across any failover and failback operations.\nYou use the Snowflake access controls and logs to audit access to the data that\u2019s under the legal hold.\nOnce the legal case concludes and the legal team approves removing the legal hold, a user with the APPLY LEGAL HOLD privilege\nreleases the legal hold. Then the normal automation for expiry resumes.\nThis example shows the sequence of SQL commands you might use during the lifecycle of a legal hold for\na backup within a particular backup set. You find the identifier of the relevant backup by using the\nSHOW BACKUPS IN BACKUP SET command, and checking the\n\"is_under_legal_hold\"\ncolumn to see if a legal hold is\nalready in place. Then you add or remove the legal hold from the specific backup.\nUSE\nROLE\n<\nrole_name\n>;\n-- use a role that has the APPLY LEGAL HOLD privilege\nSHOW\nBACKUPS\nIN\nBACKUP\nSET\n<\nbackup_set_name\n>\n->>\nSELECT\n*\nFROM\n$\n1\nWHERE\n\"is_under_legal_hold\"\n=\n'N'\n;\nALTER\nBACKUP\nSET\n<\nbackup_set_name\n>\nMODIFY\nBACKUP\nIDENTIFIER\n'<backup_identifier>'\nADD\nLEGAL HOLD\n;\nUSE\nROLE\n<\nrole_name\n>;\n-- use a role that has the APPLY LEGAL HOLD privilege\nSHOW\nBACKUPS\nIN\nBACKUP\nSET\n<\nbackup_set_name\n>\n->>\nSELECT\n*\nFROM\n$\n1\nWHERE\n\"is_under_legal_hold\"\n=\n'Y'\n;\nALTER\nBACKUP\nSET\n<\nbackup_set_name\n>\nMODIFY\nBACKUP\nIDENTIFIER\n'<backup_identifier>'\nREMOVE\nLEGAL HOLD\n;\nCopy\nTip\nYou can also check for the existence of legal holds by querying the\n\"is_under_legal_hold\"\ncolumn in the\nINFORMATION_SCHEMA.BACKUPS or ACCOUNT_USAGE.BACKUPS views.\nRep",
    "<\nbackup_set_name\n>\n->>\nSELECT\n*\nFROM\n$\n1\nWHERE\n\"is_under_legal_hold\"\n=\n'Y'\n;\nALTER\nBACKUP\nSET\n<\nbackup_set_name\n>\nMODIFY\nBACKUP\nIDENTIFIER\n'<backup_identifier>'\nREMOVE\nLEGAL HOLD\n;\nCopy\nTip\nYou can also check for the existence of legal holds by querying the\n\"is_under_legal_hold\"\ncolumn in the\nINFORMATION_SCHEMA.BACKUPS or ACCOUNT_USAGE.BACKUPS views.\nReplicate backup-related objects\n\u00b6\nWhen you use replication in combination with database, schema, and table backups, you specify the databases that contain the\nbackup sets and backup policies in your replication groups and failover groups. You can control how your backup-related\nobjects are replicated by organizing your replication groups and failover groups, and choosing which databases and schemas\ncontain your backup sets and backup policies. For more information, see\nBackup replication for database, schema, and table backups\n.\nThe backup sets and backup policies are database objects. Snowflake replicates the backup sets and backup\npolicies along with the databases and schemas that contain them.\nSnowflake minimizes the time and storage usage for backups by using a mechanism similar to cloning, so that each backup doesn\u2019t\nrequire a complete new copy of all the table data. If the backup set is part of a different failover group than the database,\nschema, or table that the backup set applies to, there\u2019s a one-time full transfer of the data for the first refresh of that\nreplication group or failover group.\nWhen a replication group or failover group includes a backup set, the increase in refresh latency is proportional to the number of\nbackups created since the last refresh.\nIf you define an expiry period for older backups, the automatic deletion happens on the primary account.\nThose expired backups are removed from the secondary account when you perform a refresh operation.\nImportant\nIf you replicate a backup set, make sure to perform a refresh immediately after placing a legal hold on a backup\nin that backup set. If you perform a failover before replicating the backup set that contains the legal hold, the original\nbackup set can be overwritten when you fail back to the original primary account, potentially erasing the legal hold.\nTherefore, you can fine-tune the replication for backup-related objects by following these practices:\nTo minimize the chance of refresh failures, put the backup set and the optional\nbackup policy within the same database",
    " a refresh immediately after placing a legal hold on a backup\nin that backup set. If you perform a failover before replicating the backup set that contains the legal hold, the original\nbackup set can be overwritten when you fail back to the original primary account, potentially erasing the legal hold.\nTherefore, you can fine-tune the replication for backup-related objects by following these practices:\nTo minimize the chance of refresh failures, put the backup set and the optional\nbackup policy within the same database and schema.\nIf that\u2019s not practical, put those things in the same replication group or failover group.\nThat way, all these related objects are replicated at the same time.\nTo maximize flexibility about which backup-related objects are replicated and the replication schedule,\nput the backup set and optional backup policy in a different database than the associated database,\nschema, or table.\nThat way, you can specify whether the backup-related objects are replicated, and how often.\nIf you apply a policy with an expiry period for older backups, make the expiry period longer than\nthe interval between replication refresh operations. That way, every new backup is replicated to\nthe secondary account at least once before it expires.\nImmediately after adding a legal hold, perform refresh operations on all secondary accounts where the\ndatabase that contains the backup set is replicated. That way, the legal hold and associated backup\nare preserved across any failover and failback operations.\nSuppose that you put a backup policy and an associated backup set into databases that are part of different\nreplication groups or failover groups. In that case, make sure to do the initial refresh of the group containing\nthe backup policy first. Otherwise, the refresh operation fails because you can\u2019t create a backup set that\u2019s missing\nits associated backup policy in the secondary account.\nNote\nPutting the backup set into a different replication group or failover group than the object of the backup set does require a\nfull transfer of all the data, during the first refresh of the group that contains the backup set.\nIf you restore a schema backup or database backup on a secondary account, references to objects within the restored schema\nor database might not resolve properly unless the referenced objects are part of the same failover group as the backup.\nMonitor backups and backup operations\n\u00b6\nYou can determine which backup-related objects exist, their properties, and how much storage they use\nby querying the following views.\nInformation schema:\nBACKUP_POLICIES view\nBACKUP_SETS view",
    " the group that contains the backup set.\nIf you restore a schema backup or database backup on a secondary account, references to objects within the restored schema\nor database might not resolve properly unless the referenced objects are part of the same failover group as the backup.\nMonitor backups and backup operations\n\u00b6\nYou can determine which backup-related objects exist, their properties, and how much storage they use\nby querying the following views.\nInformation schema:\nBACKUP_POLICIES view\nBACKUP_SETS view\nBACKUPS view\nAccount usage:\nBACKUP_OPERATION_HISTORY view\nBACKUP_POLICIES view\nBACKUP_SETS view\nBACKUP_STORAGE_USAGE view\nBACKUPS view\nOrganization usage:\nBACKUP_OPERATION_HISTORY view\nBACKUP_POLICIES view\nBACKUP_SETS view\nBACKUPS view\nSQL reference topics\n\u00b6\nBackup policy\n\u00b6\nCREATE BACKUP POLICY\nALTER BACKUP POLICY\nDROP BACKUP POLICY\nSHOW BACKUP POLICIES\nBackup set\n\u00b6\nCREATE BACKUP SET\nALTER BACKUP SET\nDROP BACKUP SET\nSHOW BACKUP SETS\nBackups\n\u00b6\nYou don\u2019t run an actual CREATE BACKUP command. To create a new backup, you run ALTER BACKUP SET \u2026 ADD BACKUP.\nOr when you associate the backup set with a backup policy that has a schedule, Snowflake automatically creates\nbackups in the backup set based on the specified schedule. To delete an older backup, you run ALTER BACKUP SET \u2026 DELETE\nBACKUP. Such operations require you to specify the identifier for a specific backup. You can find the backup identifiers,\nalong with other information such as when each backup was created, by using the following command.\nSHOW BACKUPS IN BACKUP SET\nRestoring objects from backups\n\u00b6\nYou use the syntax CREATE\nobject_kind\nFROM BACKUP SET to restore each kind of object\nfrom the appropriate kind of backup set.\nFurther backups in the backup set use the original object, not the restored one. That\u2019s true even\nif you rename the restored object to the same name as the original object. If you want to continue using the\nsame backup set after doing a restore, you restore the object under a new name\nand then transfer data back to the original object.\nCREATE DATABASE FROM BACKUP SET\nCREATE SCHEMA FROM BACKUP SET\nCREATE TABLE FROM BACKUP SET\nViews\n\u00b6\nThe following system views contain metadata related to backups, backup sets",
    " object, not the restored one. That\u2019s true even\nif you rename the restored object to the same name as the original object. If you want to continue using the\nsame backup set after doing a restore, you restore the object under a new name\nand then transfer data back to the original object.\nCREATE DATABASE FROM BACKUP SET\nCREATE SCHEMA FROM BACKUP SET\nCREATE TABLE FROM BACKUP SET\nViews\n\u00b6\nThe following system views contain metadata related to backups, backup sets, and backup policies.\nInformation schema views\n\u00b6\nThese views in the INFORMATION_SCHEMA schema contain information about backup-related objects\nthat currently exist:\nBACKUP_POLICIES view\nBACKUP_SETS view\nBACKUPS view\nAccount usage views\n\u00b6\nThese views in the ACCOUNT_USAGE schema contain information at the account level about backup-related objects\nthat exist, or have been dropped, the operations that were performed on the backups, and the storage that they use:\nBACKUP_OPERATION_HISTORY view\nBACKUP_POLICIES view\nBACKUP_SETS view\nBACKUP_STORAGE_USAGE view\nBACKUPS view\nOrganization usage views\n\u00b6\nThese views in the ORGANIZATION_USAGE schema contain information at the organization level about backup-related objects\nthat exist, or have been dropped, the operations that were performed on the backups, and the storage that they use:\nBACKUP_OPERATION_HISTORY view\nBACKUP_POLICIES view\nBACKUP_SETS view\nBACKUPS view\nTerminology change\n\u00b6\nThe feature is now called\nbackups\ninstead of snapshots. All SQL commands, views, and privileges use\nBACKUP\nterminology:\nCREATE BACKUP POLICY, CREATE BACKUP SET\nALTER BACKUP POLICY, ALTER BACKUP SET\nDROP BACKUP POLICY, DROP BACKUP SET\nSHOW BACKUP POLICIES, SHOW BACKUP SETS, SHOW BACKUPS IN BACKUP SET\nBACKUPS, BACKUP_POLICIES, BACKUP_SETS views in Account Usage, Organization Usage, and Information Schema\nAPPLY BACKUP POLICY, APPLY BACKUP RETENTION LOCK privileges\nThe former SNAPSHOT/SNAPSHOTS names are still present but deprecated in favor of their BACKUP/BACKUPS equivalents.\nFor example:\nCREATE SNAPSHOT POLICY is deprecated; use CREATE BACKUP POLICY instead.\nSNAPSHOTS view is deprecated; use BACKUPS view instead.\nAPPLY SNAPSHOT POLICY privilege is deprecated; use APPLY BACKUP POL",
    " in Account Usage, Organization Usage, and Information Schema\nAPPLY BACKUP POLICY, APPLY BACKUP RETENTION LOCK privileges\nThe former SNAPSHOT/SNAPSHOTS names are still present but deprecated in favor of their BACKUP/BACKUPS equivalents.\nFor example:\nCREATE SNAPSHOT POLICY is deprecated; use CREATE BACKUP POLICY instead.\nSNAPSHOTS view is deprecated; use BACKUPS view instead.\nAPPLY SNAPSHOT POLICY privilege is deprecated; use APPLY BACKUP POLICY privilege instead.\nThe deprecated commands, views, and privileges continue to work, but Snowflake intends to remove them in a future release.\nOn this page\nUse cases for Snowflake backups\nKey concepts\nBackup\nBackup set\nBackup policy\nRetention lock\nLegal hold\nOverview of the backup lifecycle\nHow backups work\nRestrictions for backups\nLimitations of backups\nComparison of backups with other disaster recovery and business continuity features\nBackup objects\nReferences from tables to other objects\nTypes of objects in database and schema backups\nHow Snowflake associates objects with their backup sets\nBackups and encryption\nBackups and data lineage\nCost for backups\nAccess control privileges\nGrant privileges required to create backup policies and sets\nGrant the APPLY privilege on a backup policy to a role\nGrant the APPLY BACKUP RETENTION LOCK privilege to a role\nCreate and configure backups\nCreate scheduled backups with retention lock\nCreate backups manually\nSuspend a backup policy on a backup set\nResume a backup policy on a backup set\nRestore a backup\nDelete a backup from a backup set\nDelete a backup set\nFind all the backup sets that contain backups of a specific table\nCreate a backup for a table with dependencies\nCreate a backup for a dynamic table\nAdd and remove legal holds\nReplicate backup-related objects\nMonitor backups and backup operations\nSQL reference topics\nBackup policy\nBackup set\nBackups\nRestoring objects from backups\nViews\nTerminology change",
    " to perform a refresh immediately after placing a legal hold on a backup\nin that backup set. If you perform a failover before you replicate the backup set that contains the legal hold, the original\nbackup set can be overwritten when you fail back to the original primary account, potentially erasing the legal hold.\nOverview of the backup lifecycle\n\u00b6\nThe following diagram shows how the Snowflake objects, backups, backup sets, and backup policies relate to each other.\nThe diagram involves the simplest kind of backup: one for a single table.\nEach backup operation produces a new backup. All the backups for that particular object are grouped together\nin a backup set. The automatic addition and removal of backups in the backup set is governed by the backup policy.\nTo recover the information from a backup, you use a CREATE command to create a new object from a specific backup.\nHow backups work\n\u00b6\nBackups are\nzero-copy\nduplicates of a Snowflake object similar to\nclones\n. Backups don\u2019t make\ncopies of table data when they are created. The backup mechanism backs up table data without incurring the additional cost\nor time of copying the data.\nSnowflake stores data in files that are immutable, and maintains pointers from backups to the data files that underlie the table. As the\ntable evolves and is modified, Snowflake ensures that each data file is protected from deletion as long as there is an unexpired\nbackup that references that file.\nRestrictions for backups\n\u00b6\nSnowflake enforces the following restrictions for backups:\nYou can\u2019t modify the retention lock for a backup policy.\nWhen a policy has a retention lock, you can increase the expiration period, but you can\u2019t decrease it.\nThe minimum schedule interval for scheduled backups is one hour (60 minutes).\nLimitations of backups\n\u00b6\nCurrently, you can create a maximum of two database backup sets for a specific database. Likewise,\nyou can create a maximum of two schema backup sets for a specific schema, and two table backup\nsets for a specific table. An object might still appear in more than two backup sets. For example,\na table might have one or two associated table backup sets. The same table might also be included\nin one or two schema backup sets, and one or two database backup sets.\nComparison of backups with other disaster recovery and business continuity features\n\u00b6\nBackups provide the following advantages that are different from other Snowflake business continuity and disaster recovery\nfeatures, such",
    " schema, and two table backup\nsets for a specific table. An object might still appear in more than two backup sets. For example,\na table might have one or two associated table backup sets. The same table might also be included\nin one or two schema backup sets, and one or two database backup sets.\nComparison of backups with other disaster recovery and business continuity features\n\u00b6\nBackups provide the following advantages that are different from other Snowflake business continuity and disaster recovery\nfeatures, such as replication and Time Travel:\nYou can enable long-term retention for backups. Long-term retention helps with recovery, regulatory compliance,\nand cyber resilience against threats such as ransomware or insider attacks.\nRetention lock ensures that backups can\u2019t be deleted by any user, including account administrators.\nYou can schedule backups on a different timeframe than you use for other data transfer operations, such as\nreplication refreshes.\nYou can backup and restore individual table objects, or container objects such as entire schemas or databases.\nYou can prevent the retention time for backups from being reduced after the backup is taken, by using a backup\npolicy that includes a retention lock. That\u2019s different from the Time Travel feature, where you can reduce the\nretention interval to zero.\nUnlike Time Travel and Fail-safe, backups preserve data from more types of objects than just tables and table data.\nThe speed and storage efficiency of taking backups is similar to the zero-copy mechanism used for cloning.\nThe way all backups for the same object are grouped into backup sets makes management simpler than if\nyou used clones to implement your own backup mechanism. For example, you don\u2019t have to manage large numbers of\nobjects, devise a naming scheme to keep track of the cloned objects, or implement a scheduling mechanism to delete\nold clones. Also, unlike with cloned objects, backups can\u2019t be modified after you create them.\nEach backup represents a single table, schema, or a database as of the specified point in time.\nbackups don\u2019t include account-level objects such as users or roles.\nSome kinds of tables and other database-level objects aren\u2019t included in schema and database backups.\nFor more information, see\nbackup objects\n.\nBackup-related objects are stored in the same cloud service provider (CSP) region as the associated database, schema, or\ntable. For business continuity and disaster recovery scenarios, you typically combine backups with Snowflake account\nreplication. That way, all the backup sets and backup policies can be replicated to a different region",
    "-level objects such as users or roles.\nSome kinds of tables and other database-level objects aren\u2019t included in schema and database backups.\nFor more information, see\nbackup objects\n.\nBackup-related objects are stored in the same cloud service provider (CSP) region as the associated database, schema, or\ntable. For business continuity and disaster recovery scenarios, you typically combine backups with Snowflake account\nreplication. That way, all the backup sets and backup policies can be replicated to a different region or\na different CSP and recovered even if there\u2019s an outage affecting the original region or CSP.\nBackup sets and backup policies can\u2019t be cloned. If you clone a schema or database that contains such objects,\nthey aren\u2019t included in the cloned schema or database.\nBackup objects\n\u00b6\nYou can create backup sets for tables, schemas, and databases.\nReferences from tables to other objects\n\u00b6\nObjects, such as views or functions, can refer to objects outside the schema or database in the backup. To ensure that\nsuch references continue functioning after you restore from a backup, use one of the following strategies:\nIf the tables and the other objects that they refer to are all in the same schema or the same database, create a\nbackup set for the entire schema or database. That way, Snowflake restores all the interconnected objects at once\nwhen you restore from the backup.\nIf objects in a backup set refer to objects that aren\u2019t included in the backup set, be aware that when a backup\nis restored, the references from the restored objects point to the original objects from the other database or schema.\nIf you dropped those other objects or changed their properties after taking the backup, you might encounter errors\nwhen you access the restored objects.\nFor account-level objects, any references from restored objects\nalways\npoint to the original account-level object.\nThat\u2019s because the account-level objects aren\u2019t part of any backup. For example, a schema backup might contain\na secret that refers to a security integration. The security integration is an account-level object and can\u2019t be\nincluded in any backup.\nTypes of objects in database and schema backups\n\u00b6\nThe following table lists the objects that are included in a database or schema backup:\nObject\nIncluded in backup\nNotes\nPermanent tables\nYes\nTime Travel information for tables isn\u2019t stored as part of a backup.\nTransient tables\nYes\nSuch tables continue to be transient tables after you restore them.\nTransient schemas and transient databases also retain the",
    " security integration. The security integration is an account-level object and can\u2019t be\nincluded in any backup.\nTypes of objects in database and schema backups\n\u00b6\nThe following table lists the objects that are included in a database or schema backup:\nObject\nIncluded in backup\nNotes\nPermanent tables\nYes\nTime Travel information for tables isn\u2019t stored as part of a backup.\nTransient tables\nYes\nSuch tables continue to be transient tables after you restore them.\nTransient schemas and transient databases also retain the transient property\nafter you restore them.\nTemporary tables\nNo\nTemporary tables are session scoped and aren\u2019t included in backups.\nDynamic tables\nYes\nDynamic tables have their own data definition language (DDL) syntax for backups. You can run\nCREATE BACKUP SET FOR DYNAMIC TABLE and CREATE DYNAMIC TABLE FROM BACKUP SET commands.\nWhen you restore a dynamic table from a backup, the table is restored in a suspended state.\nSnowflake\nautomatically initializes\nthe new table\nduring its first refresh.\nExternal tables\nNo\nHybrid tables\nNo\nApache Iceberg\u2122 tables\nNo\nTable constraints\nYes\nEvent tables\nNo\nSequences\nYes\nViews\nYes\nMaterialized views\nNo\nSecure views\nYes\nFile formats\nYes\nInternal stages\nNo\nExternal stages\nNo\nTemporary stages\nNo\nDirectory tables\nNo\nPipes\nNo\nStored procedures\nYes\nSQL, Javascript, Python, Java, and Scala procedures are all supported.\nUser-defined functions (UDFs)\nYes\nSQL, Javascript, Python, Java, and Scala functions are all supported.\nBoth scalar UDFs and user-defined table functions (UDTFs) are included in the backup.\nJava UDFs in backups have the same requirements as in\nLimitations on cloning\n.\nStreams\nNo\nTasks\nYes\nTasks are included in the backup. Tasks restored from a backup are suspended and must be resumed.\nData metric functions (DMFs)\nNo\nPolicies\nYes\nThe following kinds of policies are included in a schema or database backup:\nColumn-level security (masking)\nRow access policies\nTag-based masking policies\nIf any table included in the backup has any other kind of policy applied (for example an\naggregation policy, a projection policy, or a storage lifecycle policy), backup creation\nfails.\nGrants\nYes\nIf you drop a role, associated ownership grants are transferred to the role that\nperforms the DROP",
    "No\nPolicies\nYes\nThe following kinds of policies are included in a schema or database backup:\nColumn-level security (masking)\nRow access policies\nTag-based masking policies\nIf any table included in the backup has any other kind of policy applied (for example an\naggregation policy, a projection policy, or a storage lifecycle policy), backup creation\nfails.\nGrants\nYes\nIf you drop a role, associated ownership grants are transferred to the role that\nperforms the DROP ROLE command. Grants other than ownership are deleted in this case.\nTherefore, the grants on a restored object might differ from the grants that existed\nwhen the backup was created.\nDatabase roles\nNo\nObject tagging\nYes\nAlerts\nYes\nNetwork rules\nYes\nGithub repos\nNo\nModels\nNo\nModel monitors\nNo\nDatasets\nNo\nNotebooks\nNo\nContacts\nNo\nCortex search services\nNo\nDbt projects\nNo\nImage repositories\nNo\nListings\nNo\nOrganization listings\nNo\nPipes\nNo\nPolicy (aggregation)\nNo\nPolicy (authentication)\nNo\nPolicy (feature)\nNo\nPolicy (join)\nNo\nPolicy (packages)\nNo\nPolicy (password)\nNo\nPolicy (privacy)\nNo\nPolicy (projection)\nNo\nPolicy (session)\nNo\nProvisioned throughput\nNo\nSemantic views\nNo\nServices\nNo\nStreamlits\nNo\nHow Snowflake associates objects with their backup sets\n\u00b6\nWhen you create a backup set for a database, schema, or table, Snowflake associates the backup set with the\ninternal ID of that database, schema, or table. If you delete the original object, you can\u2019t add any more backups\nto that backup set. This behavior applies even if you recreate an object with the same name, or replace it with an\nobject that was restored from a backup.\nIf you instead rename the original object, then you can continue making more backups of it by adding more backups to\nthe same backup set. In that case, the output of SHOW BACKUP SETS changes to reflect the OBJECT_NAME value of the\nrenamed object.\nIf you want to make backups of a table but you frequently drop and recreate that table, perhaps through CREATE OR REPLACE\nstatements, include it in a backup set for the schema or database that contains the table. That way, you can keep using\nthe same backup set regardless of changes to",
    " making more backups of it by adding more backups to\nthe same backup set. In that case, the output of SHOW BACKUP SETS changes to reflect the OBJECT_NAME value of the\nrenamed object.\nIf you want to make backups of a table but you frequently drop and recreate that table, perhaps through CREATE OR REPLACE\nstatements, include it in a backup set for the schema or database that contains the table. That way, you can keep using\nthe same backup set regardless of changes to the table.\nWhen you restore a table from a backup, the restored table starts with a different name than the original. Suppose that\nyou want to completely replace the contents of the original table with the backup data, and continue to use the same backup set\nfor more backups of that same table. In that case, use a TRUNCATE or DELETE statement to remove the contents of the original table,\nand an INSERT \u2026 SELECT statement to copy the data from the restored table. Don\u2019t drop the original table and rename the restored table\nto the name of the original table.\nBackups and encryption\n\u00b6\nThe data within backup sets is protected by the same end-to-end encryption as other Snowflake objects and table data.\nFor more information about Snowflake encryption, see\nUnderstanding end-to-end encryption in Snowflake\n.\nKey rotation also applies to the data within backups.\nBackups and data lineage\n\u00b6\nSnowflake doesn\u2019t preserve\ndata lineage\nmetadata with database, schema, and table\nbackups. After you restore an object from a backup, you can\u2019t use Snowsight to view lineage information for the\nrestored data.\nCost for backups\n\u00b6\nThe following table describes charges for backups.\nFor information about credit consumption, see the\nSnowflake Service Consumption Table\n.\nCost component\nDescription\nBilled\nBackup compute\nSnowflake-managed compute service generates scheduled backup creation and expiration.\nYes\nRestore compute\nSnowflake-managed warehouses are used to restore objects from backups.\nYes\nBackup storage\nSnowflake-managed cloud object storage to store backup data.\nBilled for bytes retained for backups, similar to bytes retained for clones.\nYou can monitor costs for backup storage in the\nTABLE_STORAGE_METRICS\nview using the\nRETAINED_FOR_CLONE_BYTES\ncolumn, and in the\nBACKUP_STORAGE_USAGE\nview.\nAccess control privileges\n\u00b6\nThe following table lists privileges and the object type on which the privilege is granted for managing and using",
    "Redirecting client connections\n\u00b6\nBusiness Critical Feature\nRequires Business Critical Edition (or higher). To inquire about upgrading, please contact\nSnowflake Support\n.\nClient Redirect enables redirecting your client connections to Snowflake accounts in different\nregions\nwithout changing the connection settings for your application.\nYou can use Client Redirect in combination with the\naccount replication\nfeature for business continuity\nand disaster recovery. You can also use Client Redirect to minimize changes needed in your application\nsettings when migrating your account to another region or cloud platform.\nIntroduction to Client Redirect\n\u00b6\nClient Redirect is implemented through a Snowflake\nconnection\nobject. The connection object stores a secure\nconnection URL\nthat you use\nwith a Snowflake client to connect to Snowflake.\nThe hostname in the connection URL is composed of your organization name and the connection object name in addition to a common domain name:\norganization_name\n-\nconnection_name\n.snowflakecomputing.com\nNote that this hostname does not specify the account to which you are connecting. An account administrator determines the account to use by\ndesignating the connection in that account to serve as the\nprimary connection\n. When you use the connection URL to connect to Snowflake,\nyou are connecting to the account that contains the primary connection.\nIf an outage occurs in a region or cloud platform and the outage affects the account with the primary connection, the administrator can\npromote a connection in a different account in a different region or cloud platform to serve as the primary connection.\nThrough this outage, you can continue to use the same connection URL to connect to Snowflake. Snowflake resolves the connection URL to the\naccount with the newly promoted connection (the account outside of the region or cloud platform affected by the outage).\nNote\nThe Snowflake accounts that store the primary and secondary connections must be hosted in different\nregions\n.\nClient Redirect flow\n\u00b6\nComplete the steps in\nConfiguring Client Redirect\n(in this topic) to create a connection URL for client connections. This\nincludes creating a primary connection and linked secondary connection(s).\nUpdate Snowflake clients to connect using the connection URL.\nUsing a connection URL\n(in this topic) contains a list of\nsupported clients and connection details.\nIn the event of a service outage in the region where the primary connection is located, complete the steps in\nRedirecting client connections\n(in this topic) to update the connection URL to redirect to a secondary connection.\nWhen the outage is resolved, complete the steps in\n",
    " connections. This\nincludes creating a primary connection and linked secondary connection(s).\nUpdate Snowflake clients to connect using the connection URL.\nUsing a connection URL\n(in this topic) contains a list of\nsupported clients and connection details.\nIn the event of a service outage in the region where the primary connection is located, complete the steps in\nRedirecting client connections\n(in this topic) to update the connection URL to redirect to a secondary connection.\nWhen the outage is resolved, complete the steps in\nRedirecting client connections\nto redirect client connections back to the\noriginal primary connection.\nThe following diagrams illustrate the Client Redirect flow for two accounts in the same organization but different regions (\nRegion\nA\nand\nRegion\nB\n) on either the same or different cloud platforms.\nThe primary connection is in\nAccount\n1\nin\nRegion\nA\n. Snowflake clients using the connection URL connect to\nAccount\n1\n.\nA service outage in\nRegion\nA\nresults in failed client connections:\nThe connection in\nAccount\n2\nin\nRegion\nB\nis promoted to act as the primary connection. Snowflake clients using the connection URL\nnow connect to\nAccount\n2\n.\nExample\n\u00b6\nThe following SQL statements go through the client redirect workflow. Each step is explained in detail in the sections that follow in this\ntopic.\nNormal client connections: Configure Client Redirect\n\u00b6\nCreate a primary connection in the source account\n\u00b6\nCreate a new primary connection and enable failover to other accounts in your organization. Each account that is enabled for failover\nmust be in a different region than the account with the primary connection.\nNote the\naccount_name\ncolumn in the output of\nSHOW REPLICATION ACCOUNTS\nfor each account to be\nenabled for failover.\nExecute the following statements in the\nsource\naccount:\n-- Create a new primary connection\nCREATE\nCONNECTION\nmyconnection\n;\n-- View accounts in your organization that are enabled for replication\nSHOW\nREPLICATION\nACCOUNTS\n;\n-- Configure failover accounts for the primary connection\nALTER\nCONNECTION\nmyconnection\nENABLE\nFAILOVER\nTO\nACCOUNTS\nmyorg\n.\nmyaccount2\n,\nmyorg\n.\nmyaccount3\n;\n-- View the details for the connection\nSHOW\nCONNECTIONS\n;\nCopy\nIf private connectivity to the Snowflake service is enabled for your Snowflake account, you must create\nand manage a DNS",
    "+-------------------------------------------+-------------------+-------------------+\nCopy\nNote that the output of this command in the CONNECTION_URL column should match the\nprivatelink-connection-urls\nlist when calling\nthe\nSYSTEM$GET_PRIVATELINK_CONFIG\nfunction in either\nmyaccount1\nor\nmyaccount2\n. This list already\ncontains the connection URL formatted with the\nprivatelink\nsegment (as shown in the next step). You can optionally run the command\nin this step or call the function. If calling the function, use the URLs as is without any further modification.\nRecord the CONNECTION_URL column value, and create two URLs to support private connectivity and\nOCSP\n.\nAdd a\nprivatelink\nsegment to the URL just before\nsnowflakecomputing.com\n(\nmyorg-myconnection.privatelink.snowflakecomputing.com\n, in this example).\nAdd an\nocsp\nsegment to the beginning of the URL (\nocsp.myorg-myconnection.privatelink.snowflakecomputing.com\n, in this example).\nUsing a tool provided by your DNS provider, create a CNAME record for the connection URL and the OCSP URL:\nSet the domain (or alias) using the modified CONNECTION_URL column value.\nConfigure the record to have the connection URL resolve to the primary Snowflake account URL. Be sure to include all URL segments for\nthe cloud region and AWS PrivateLink based on the URL format that you choose. This is the primary account URL and it is where client\nconnections to the connection URL will redirect.\nConfigure the record to have the OCSP URL resolve to either the private endpoint IP address for an account on Azure or the private\nendpoint ID value for an account on AWS.\nIn the case of failover, you must manually update the DNS setting to have the connection URL point to the secondary account URL as\nshown in\nModifying the DNS settings for private connectivity to the Snowflake service\n. Similarly, you must update your OCSP settings to point to the private endpoint IP\naddress or private endpoint ID value.\nFor example:\nmyaccount1.us-west-2.privatelink.snowflakecomputing.com.\nocsp.myaccount1.us-west-2.privatelink.snowflakecomputing.com.\nCopy\nAlternatively, use the organization and account name URL.\nFor example:\nmyorg-myaccount1.privatelink.snowflakecomputing.com.\nocsp.myorg-myaccount1.privatelink.s",
    " update your OCSP settings to point to the private endpoint IP\naddress or private endpoint ID value.\nFor example:\nmyaccount1.us-west-2.privatelink.snowflakecomputing.com.\nocsp.myaccount1.us-west-2.privatelink.snowflakecomputing.com.\nCopy\nAlternatively, use the organization and account name URL.\nFor example:\nmyorg-myaccount1.privatelink.snowflakecomputing.com.\nocsp.myorg-myaccount1.privatelink.snowflakecomputing.com.\nCopy\nNote the trailing period, which must be included.\nUsers connect to Snowflake using the following connection URL format:\norganization_name\n-\nconnection_name\n.privatelink.snowflakecomputing.com\nWhere:\norganization_name\nName of your Snowflake organization. The Snowflake accounts that your users connect to are contained in this organization.\nconnection_name\nName of the connection object.\nFor more information, see:\nUsing a connection URL\n(in this topic).\nModifying the DNS settings for private connectivity to the Snowflake service\n(in this topic).\nConfiguring Client Redirect and reader accounts\n\u00b6\nIf you are a data provider with\nreader accounts\n, you can use Client Redirect\nto provide continued access to shared data in the event of a service outage. The configuration steps for creating connections are\nthe same as those described in the\nConfiguring Client Redirect\nsection for source and target reader accounts:\nCreate two reader accounts. Each reader account must be in a different region.\nCreate a primary connection\nin the source reader account. Enable failover to the other reader account.\nCreate a secondary connection in each target account\nin the reader account that you enabled for failover from the source account.\nShare the connection URL with your data consumers.\nIf a service outage occurs,\nredirect client connections\n. Data consumers using the\nconnection URL to connect to your reader account now connect to the newly promoted source reader account.\nUsing a connection URL\n\u00b6\nThis section provides instructions for referencing a connection URL in the configuration for various Snowflake clients.\nSupported Snowflake clients\n\u00b6\nClient Redirect is supported by\nSnowsight\nand\nClassic Console\n.\nIn addition, the following Snowflake client versions (and higher) support Client Redirect:\nSnowflake Client\nMinimum Supported Version\nSnowflake CLI\n3.0.0\nSnowSQL\n1.1.82\nSnowflake Connector for Python\n1.8.3\nSnowflake Connector for Spark\n",
    " instructions for referencing a connection URL in the configuration for various Snowflake clients.\nSupported Snowflake clients\n\u00b6\nClient Redirect is supported by\nSnowsight\nand\nClassic Console\n.\nIn addition, the following Snowflake client versions (and higher) support Client Redirect:\nSnowflake Client\nMinimum Supported Version\nSnowflake CLI\n3.0.0\nSnowSQL\n1.1.82\nSnowflake Connector for Python\n1.8.3\nSnowflake Connector for Spark\nAll versions\nNode.js Driver\n1.2.0\nGo Snowflake Driver\n1.2.0\n.NET Driver\n1.0.0\nJDBC Driver\n3.8.4\nODBC Driver\n2.19.4\nSnowpark\nAll versions\nConfigure Snowflake clients\n\u00b6\nUse the following host name for the connection URL when connecting to Snowflake:\nHost name:\norganization_name\n-\nconnection_name\n.snowflakecomputing.com\nWhere:\norganization_name\nName of your Snowflake organization. The Snowflake accounts that your users connect to are contained in this organization.\nconnection_name\nName of the connection object.\nImportant\nPrivate Connectivity to the Snowflake Service\nCustomers using private connectivity to the Snowflake service need to add a\nprivatelink\nsegment to the URL just before\nsnowflakecomputing.com\n:\norganization_name\n-\nconnection_name\n.privatelink.snowflakecomputing.com\nSnowsight\n\u00b6\nEnter the following in the account name field on\napp.snowflake.com\n:\n<organization-name>-<connection-name>\nCopy\nFor example:\nmyorg-myconnection\nCopy\nWhen using\norganization\n-\nconnection\nto log in, Snowsight navigates to the specific region and locator of the current\nprimary connection. During an outage, once the connection has been redirected, users must log in again via\norganization\n-\nconnection\nto connect to the new primary.\nClassic Console\n\u00b6\nEnter the following URL in a web browser:\nhttps://<organization_name>-<connection_name>.snowflakecomputing.com/\nCopy\nFor example:\nhttps://myorg-myconnection.snowflakecomputing.com/\nCopy\nSnowflake CLI\n\u00b6\nSpecify the host name for the connection URL in the\naccount\nconnection parameter in the Snowflake CLI\nconfig.toml\nfile. For information\nabout the\nconfig.toml\nfile, see\nConfiguring Snowflake CLI\n.\n",
    "\nEnter the following URL in a web browser:\nhttps://<organization_name>-<connection_name>.snowflakecomputing.com/\nCopy\nFor example:\nhttps://myorg-myconnection.snowflakecomputing.com/\nCopy\nSnowflake CLI\n\u00b6\nSpecify the host name for the connection URL in the\naccount\nconnection parameter in the Snowflake CLI\nconfig.toml\nfile. For information\nabout the\nconfig.toml\nfile, see\nConfiguring Snowflake CLI\n.\naccount = <organization_name>-<connection_name>\nusername = <username>\npassword = <password>\nCopy\nFor example:\n[connections.myconnection]\naccount\n=\n\"myaccount\"\nuser\n=\n\"jondoe\"\npassword\n=\n\"password\"\nCopy\nSnowSQL\n\u00b6\nSpecify the host name for the connection URL in the\naccountname\nconnection parameter in the SnowSQL\nconfig\nfile. For information\nabout the\nconfig\nfile, see\nConfiguring SnowSQL\n.\naccountname\n=\n<organization_name>-<connection_name>\nusername\n=\n<username>\npassword\n=\n<password>\nCopy\nFor example:\naccountname\n=\nmyorg-myconnection\nusername\n=\njsmith\npassword\n=\nmySecurePassword\nCopy\nSnowflake Connector for Python\n\u00b6\nSpecify the host name for the connection URL in the\naccount\nconnection parameter when calling the connect function. For more\ninformation, see\nPython Connector API\nand\nUsing the Python Connector\n.\ncon\n=\nsnowflake.connector.connect\n(\naccount\n=\n<organization_name>-<connection_name>\nuser\n=\n<username>\npassword\n=\n<password>\n)\nCopy\nFor example:\ncon\n=\nsnowflake.connector.connect\n(\naccount\n=\nmyorg-myconnection\nuser\n=\njsmith\npassword\n=\nmySecurePassword\n)\nCopy\nSnowflake Connector for Spark\n\u00b6\nSpecify the connection URL in the\nURL\nproperty in the properties file or\nMap\nthat you use\nto establish the session.\n# Properties file (a text file) for establishing a Connector for Spark session\nURL\n=\nhttps://<organization_name>-<connection_name>.snowflakecomputing.com\nCopy\nFor example:\nURL\n=\nhttps://myorg-myconnection.snowflakecomputing.com\nCopy\nFor more information about using the Snowflake Connector for Spark, see\nSnowflake Connector for Spark\n.\nFor configuration options, see\nSetting Configuration Options for the Connector\n",
    " you use\nto establish the session.\n# Properties file (a text file) for establishing a Connector for Spark session\nURL\n=\nhttps://<organization_name>-<connection_name>.snowflakecomputing.com\nCopy\nFor example:\nURL\n=\nhttps://myorg-myconnection.snowflakecomputing.com\nCopy\nFor more information about using the Snowflake Connector for Spark, see\nSnowflake Connector for Spark\n.\nFor configuration options, see\nSetting Configuration Options for the Connector\n.\nDepending upon which language you use with the connector, also see\nUsing the Connector in Scala\nor\nUsing the Connector with Python\n.\nJDBC Driver\n\u00b6\nSpecify the host name for the connection URL in the connection string. For more information, see\nConfiguring the JDBC Driver\n.\njdbc:snowflake://<organization_name>-<connection_name>.snowflakecomputing.com/?user\n=\n<username>\n&\npassword\n=\n<password>\nCopy\nFor example:\njdbc:snowflake://myorg-myconnection.snowflakecomputing.com/?user\n=\njsmith\n&\npassword\n=\nmySecurePassword\nCopy\nODBC Driver\n\u00b6\nSpecify the host name for the connection URL in the Server connection parameter. For more information about the connection parameters, see\nODBC configuration and connection parameters\n.\n[\nODBC\nData\nSources\n]\n<account_name>\n=\nSnowflakeDSIIDriver\n[\n<dsn_name>\n]\nDescription\n=\nSnowflakeDB\nDriver\n=\nSnowflakeDSIIDriver\nLocale\n=\nen-US\nSERVER\n=\n<organization_name>-<connection_name>.snowflakecomputing.com\nCopy\nFor example:\n[\nODBC\nData\nSources\n]\nmyaccount\n=\nSnowflakeDSIIDriver\n[\nclient_redirect\n]\nDescription\n=\nSnowflakeDB\nDriver\n=\nSnowflakeDSIIDriver\nLocale\n=\nen-US\nSERVER\n=\nmyorg-myconnection.snowflakecomputing.com\nCopy\nNode.js Driver\n\u00b6\nSpecify the host name for the connection URL in the\naccount\nconnection option. For more information about the connection parameters,\nsee\nNode.js options reference\n.\nvar\nconfiguration\n=\n{\nusername:\n'<username>'\n,\npassword:\n'<password>'\n,\naccount:\n<organization_name>-<connection_name>.\n}\nvar\nconnection\n=\nsnowflake.createConnection\n(\nconfiguration\n)\nCopy\nFor example:\nvar\nconfiguration\n=\n{\nusername:\n'jsmith'\n,\npassword",
    "Node.js Driver\n\u00b6\nSpecify the host name for the connection URL in the\naccount\nconnection option. For more information about the connection parameters,\nsee\nNode.js options reference\n.\nvar\nconfiguration\n=\n{\nusername:\n'<username>'\n,\npassword:\n'<password>'\n,\naccount:\n<organization_name>-<connection_name>.\n}\nvar\nconnection\n=\nsnowflake.createConnection\n(\nconfiguration\n)\nCopy\nFor example:\nvar\nconfiguration\n=\n{\nusername:\n'jsmith'\n,\npassword:\n'mySecurePassword'\n,\naccount:\nmyorg-myconnection.\n}\nvar\nconnection\n=\nsnowflake.createConnection\n(\nconfiguration\n)\nCopy\nGo Snowflake Driver\n\u00b6\nSpecify the host name for the connection URL in the\nAccount\nparameter. For more information, see\nGo Snowflake Driver\n.\ncfg\n:\n=\n&\nConfig\n{\nAccount:\n\"<organization_name>-<connection_name>\"\n,\nUser:\n\"<username>\"\n,\nPassword:\n\"<password>\"\n}\ndsn,\nerr\n:\n=\nDSN\n(\ncfg\n)\nCopy\nFor example:\ncfg\n:\n=\n&\nConfig\n{\nAccount:\n\"myorg-myconnection\"\n,\nUser:\n\"jsmith\"\n,\nPassword:\n\"mySecurePassword\"\n}\ndsn,\nerr\n:\n=\nDSN\n(\ncfg\n)\nCopy\nSnowpark\n\u00b6\nSnowpark Python\n\u00b6\nSpecify the host name for the connection URL in the\naccount\nconnection parameter in the Python dictionary (\ndict\n) used to\nestablish a session. For more information about creating a session, see\nCreating a Session for Snowpark Python\n.\nconnection_parameters\n=\n{\n\"account\"\n:\n\"<organization_name>-<connection_name>\"\n,\n\"user\"\n:\n\"<snowflake_user>\"\n,\n\"password\"\n:\n\"<snowflake_password>\"\n}\nCopy\nFor example:\nconnection_parameters\n=\n{\n\"account\"\n:\n\"myorg-myconnection\"\n,\n\"user\"\n:\n\"jsmith\"\n,\n\"password\"\n:\n\"mySecurePassword\"\n}\nCopy\nSnowpark Java\n\u00b6\nSpecify the connection URL in the\nURL\nproperty in the properties file or\nMap\nthat you use to establish the session. For more\ninformation about creating a session, see\nCreating a Session for Snowpark Java\n.\n# Properties file (a text file) for establishing a Snowpark session\nURL\n=\nhttps://<organization_name>-<connection_name>.snowflakecomputing.com\nCopy\nFor example:\n# Properties file (a text file) for establishing",
    "park Java\n\u00b6\nSpecify the connection URL in the\nURL\nproperty in the properties file or\nMap\nthat you use to establish the session. For more\ninformation about creating a session, see\nCreating a Session for Snowpark Java\n.\n# Properties file (a text file) for establishing a Snowpark session\nURL\n=\nhttps://<organization_name>-<connection_name>.snowflakecomputing.com\nCopy\nFor example:\n# Properties file (a text file) for establishing a Snowpark session\nURL\n=\nhttps://myorg-myconnection.snowflakecomputing.com\nCopy\nSnowpark Scala\n\u00b6\nSpecify the connection URL in the\nURL\nproperty in the properties file or\nMap\nthat you use to establish the session. For more\ninformation about creating a session, see\nCreating a Session for Snowpark Scala\n.\n# Properties file (a text file) for establishing a Snowpark session\nURL\n=\nhttps://<organization_name>-<connection_name>.snowflakecomputing.com\nCopy\nFor example:\n# Properties file (a text file) for establishing a Snowpark session\nURL\n=\nhttps://myorg-myconnection.snowflakecomputing.com\nCopy\nAuthentication and Client Redirect\n\u00b6\nUsers must be provisioned in the source account and on each target account if security integrations are not\nreplicated\n.\nFederated authentication & SSO\n\u00b6\nConfigure federated authentication separately in each target account. Provide the identity provider (IdP) details using the setup\noptions in\nConfiguring Snowflake to use federated authentication\n:\nNote\nSnowflake recommends configuring your SAML 2.0-compliant identity provider (IdP) with the connection URL rather than an account URL so\nusers are redirected to the correct account in case of failover.\nOAuth\n\u00b6\nConfigure a security integration object for OAuth in each target account. The security integration object must be identical to the same\nobject in the source account. For instructions, see the appropriate topic:\nSnowflake OAuth\nExternal OAuth\nTo retrieve security integration properties, query the\nDESCRIBE INTEGRATION\ncommand for each security integration\nin the source account. Then recreate each security integration in a target account by executing the\nCREATE INTEGRATION\ncommand.\nOAuth redirect behavior\n\u00b6\nIf you are using Snowflake OAuth for authenticating a client connection and are connecting to Snowflake using a connection URL, you\nare prompted",
    " the source account. For instructions, see the appropriate topic:\nSnowflake OAuth\nExternal OAuth\nTo retrieve security integration properties, query the\nDESCRIBE INTEGRATION\ncommand for each security integration\nin the source account. Then recreate each security integration in a target account by executing the\nCREATE INTEGRATION\ncommand.\nOAuth redirect behavior\n\u00b6\nIf you are using Snowflake OAuth for authenticating a client connection and are connecting to Snowflake using a connection URL, you\nare prompted to re-authenticate if the connection URL is redirected to another account (e.g. in case of failover). Snowflake OAuth\ntokens are valid for use in a specific account. When a connection URL is updated to point to an account in a different region, the\nexisting OAuth token becomes invalid.\nIn the case of a failover, when the connection URL is updated to the new account, the client will disconnect with an\ninvalid\nOAuth\naccess\ntoken\nerror. You must re-authenticate and consent to permissions to re-establish the connection.\nNote\nYou will\nnot\nbe prompted for re-authentication when the connection URL is updated to a new account if the\nOAuth security integration is replicated\nto that account. For more\ninformation, refer to\nReplicating OAuth security integrations\n.\nRedirecting client connections\n\u00b6\nIn the event of a service outage in the region where the primary connection is located, redirect the client connection to an account that\nstores a secondary connection.\nPromoting a secondary connection to serve as the primary connection\n\u00b6\nInitiating the redirect involves promoting a secondary connection in an available region to serve as the primary connection using\nALTER CONNECTION\n. Concurrently, the former primary connection becomes a secondary connection.\nSign in to the target account in an available region that contains the secondary connection to be promoted to serve\nas the primary connection.\nExecute the SQL statements in this section:\nView all connections in the account:\nSHOW\nCONNECTIONS\n;\nCopy\nThe statement returns the following output:\n+--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\n| snowflake_region   | created_on                    | account_name        | name              | comment         | is_primary    | primary                       | failover_allowed_to_accounts        | connection_url                            | organization_name | account_locator   |\n|--------------------+-------------------------------+---------------------+-------------------+",
    "\n;\nCopy\nThe statement returns the following output:\n+--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\n| snowflake_region   | created_on                    | account_name        | name              | comment         | is_primary    | primary                       | failover_allowed_to_accounts        | connection_url                            | organization_name | account_locator   |\n|--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------|\n| AWS_US_WEST_2      | 2020-07-19 14:49:11.183 -0700 | MYORG.MYACCOUNT1    | MYCONNECTION      | NULL            | true          | MYORG.MYACCOUNT1.MYCONNECTION | MYORG.MYACCOUNT2, MYORG.MYACCOUNT3  | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR1 |\n| AWS_US_EAST_1      | 2020-07-22 13:52:04.925 -0700 | MYORG.MYACCOUNT2    | MYCONNECTION      | NULL            | false         | MYORG.MYACCOUNT1.MYCONNECTION |                                     | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR2 |\n+--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\nPromote a secondary connection to serve as the primary connection:\nALTER\nCONNECTION\nmyconnection\nPRIMARY\n;\nCopy\nVerify that the former secondary connection was promoted successfully:\nSHOW\nCONNECTIONS\n;\nCopy\nThe statement returns the following output:\n+--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\n| snowflake_region   | created_on                    | account_name        | name              | comment         | is_primary    | primary                       | failover_allowed_to_accounts        | connection_url                            | organization_name | account_locator   |\n|--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------|\n| AWS_US_WEST_2      | 2020-",
    "+-------------------------------------------+-------------------+-------------------+\n| snowflake_region   | created_on                    | account_name        | name              | comment         | is_primary    | primary                       | failover_allowed_to_accounts        | connection_url                            | organization_name | account_locator   |\n|--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------|\n| AWS_US_WEST_2      | 2020-07-19 14:49:11.183 -0700 | MYORG.MYACCOUNT1    | MYCONNECTION      | NULL            | false         | MYORG.MYACCOUNT1.MYCONNECTION | MYORG.MYACCOUNT2, MYORG.MYACCOUNT3  | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR1 |\n| AWS_US_EAST_1      | 2020-07-22 13:52:04.925 -0700 | MYORG.MYACCOUNT2    | MYCONNECTION      | NULL            | true          | MYORG.MYACCOUNT1.MYCONNECTION |                                     | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR2 |\n+--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\nModifying the DNS settings for private connectivity to the Snowflake service\n\u00b6\nTo redirect client connections to a secondary account, your network administrator must modify the DNS setting created in\nConfiguring the DNS settings for private connectivity to the Snowflake service\n.\nUsing a tool provided by your DNS provider, modify the DNS setting for the connection URL.\nSet the destination hostname as the complete Snowflake account URL for the account that stores your new primary connection, including the\nadditional segments that identify the region and cloud platform where your account is hosted and the support for AWS PrivateLink, Azure\nPrivate Link, or Google Cloud Private Service Connect. This is the account name where client connections to the connection URL will now\nredirect. Be sure to include the private connectivity OCSP URL when updating the DNS settings.\nFor example:\nmyaccount1.us-east-1.privatelink.snowflakecomputing.com.\nocsp.myaccount1.us-east-1.privatelink.snowflakecomputing.com.\nCopy\n(Note the trailing period,",
    " replication\nSHOW\nREPLICATION\nACCOUNTS\n;\n-- Configure failover accounts for the primary connection\nALTER\nCONNECTION\nmyconnection\nENABLE\nFAILOVER\nTO\nACCOUNTS\nmyorg\n.\nmyaccount2\n,\nmyorg\n.\nmyaccount3\n;\n-- View the details for the connection\nSHOW\nCONNECTIONS\n;\nCopy\nIf private connectivity to the Snowflake service is enabled for your Snowflake account, you must create\nand manage a DNS CNAME record for your connection URL. For more details, see\nConfiguring the DNS settings for private connectivity to the Snowflake service\n.\nExecuted on target account\n\u00b6\nCreate a secondary connection linked to the primary connection. The name of the secondary connection must be the same name as the primary\nconnection.\nCREATE\nCONNECTION\nmyconnection\nAS\nREPLICA\nOF\nmyorg\n.\nmyaccount1\n.\nmyconnection\n;\nCopy\nIf private connectivity to the Snowflake service is enabled for your Snowflake account, you must create\nor update a DNS CNAME record for your connection URL. For more details, see\nModifying the DNS settings for private connectivity to the Snowflake service\n.\nOutage occurs in source region: Failover\n\u00b6\nIf an outage occurs in the region where the primary connection is located, promote a secondary connection in a different region\nto serve as the primary connection.\nExecuted on target account\n\u00b6\nSign in to the target account that you want to promote to serve as the new source account.\nPromote the secondary connection to serve as the primary connection:\nALTER\nCONNECTION\nmyconnection\nPRIMARY\n;\nCopy\nIf private connectivity to the Snowflake service is enabled for your Snowflake account, you must create\nor update a DNS CNAME record for your connection URL. For more details, see\nModifying the DNS settings for private connectivity to the Snowflake service\n.\nOutage resolved: Failback\n\u00b6\nOnce the outage is resolved, promote the original primary connection to serve as the primary connection again.\nExecuted on the target account that previously served as the source account\n\u00b6\nSign in to the target account that served as the source account prior to the outage.\nPromote the secondary connection back to primary connection:\nALTER\nCONNECTION\nmyconnection\nPRIMARY\n;\nCopy\nIf private connectivity to the Snowflake service is enabled for your Snowflake account, you must create\nor",
    " hosted and the support for AWS PrivateLink, Azure\nPrivate Link, or Google Cloud Private Service Connect. This is the account name where client connections to the connection URL will now\nredirect. Be sure to include the private connectivity OCSP URL when updating the DNS settings.\nFor example:\nmyaccount1.us-east-1.privatelink.snowflakecomputing.com.\nocsp.myaccount1.us-east-1.privatelink.snowflakecomputing.com.\nCopy\n(Note the trailing period, which must be included.)\nNote\nYou can configure private connectivity and client redirect to work with Snowsight. Ensure your DNS updates include the Snowsight\nvalues from the output of the SYSTEM$GET_PRIVATELINK_CONFIG function. For details, refer to\nprivate connectivity and Snowsight\n.\nVerifying the connection URL is updated\n\u00b6\nTo verify the connection URL has been updated, you can confirm the region of your current connection. Use the connection URL to connect to\nSnowflake and execute the\nCURRENT_REGION\nfunction.\nSELECT\nCURRENT_REGION\n();\nCopy\nModifying a connection\n\u00b6\nYou can edit the target accounts for a connection after creating it using Snowsight or SQL.\nModify target accounts for a connection using Snowsight\n\u00b6\nYou can modify the target account for a connection after creating it, but you cannot change the connection name.\nNote\nTo edit a connection, you must be signed in as a user with the ACCOUNTADMIN role to the following accounts:\nThe source account with the primary connection.\nThe current target account with the secondary connection.\nThe new target account you want to add for the primary connection.\nYou can only add one target account for a primary connection using Snowsight. To add additional\ntarget accounts,\nuse the ALTER CONNECTION command\n.\nCurrently, if your account uses private connectivity, you can\u2019t use Snowsight to modify target\naccounts for a connection.\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nClient Redirect\n.\nLocate the connection you want to edit. Select the\nMore\nmenu (\n\u2026\n) in the last column of the row.\nModify target accounts for a connection using SQL\n\u00b6\nYou can add more than one target account for a primary connection using the\nALTER CONNECTION\ncommand.\nFor an example, see\nExamples\n.\nDropping a connection\n\u00b6\nYou can drop a connection using Snowsight or SQL.\n",
    " select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nClient Redirect\n.\nLocate the connection you want to edit. Select the\nMore\nmenu (\n\u2026\n) in the last column of the row.\nModify target accounts for a connection using SQL\n\u00b6\nYou can add more than one target account for a primary connection using the\nALTER CONNECTION\ncommand.\nFor an example, see\nExamples\n.\nDropping a connection\n\u00b6\nYou can drop a connection using Snowsight or SQL.\nDrop a connection using Snowsight\n\u00b6\nNote\nCurrently, if your account uses private connectivity, you can\u2019t use Snowsight to drop a connection.\nTo delete a connection, you must sign in as a user with the ACCOUNTADMIN role to the\nsource\naccount with the primary connection.\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nClient Redirect\n.\nLocate the connection you want to delete. Select the\nMore\nmenu (\n\u2026\n) in the last column of the row.\nSelect\nDrop\n, then select\nDrop Connection\nDrop a connection using SQL\n\u00b6\nYou can use the\nDROP CONNECTION\ncommand to delete a connection.\nDelete all secondary connections in target accounts.\nDelete the primary connection in the source account.\nFor an example, see\nExamples\n.\nMonitoring Client Redirect\n\u00b6\nYou can monitor Client Redirect connections and usage for accounts in an organization using Snowsight or SQL.\nMonitor Client Redirect using Snowsight\n\u00b6\nNote\nOnly a user with the ACCOUNTADMIN role can view connection details using Snowsight.\nYou must be signed in to the target account as a user with the ACCOUNTADMIN role. If you are not, you will be\nprompted to sign in.\nCurrently, if your account uses private connectivity, you can\u2019t use Snowsight to monitor Client Redirect.\nTo view the Client Redirect connection details, complete the following steps:\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\nand then select\nClient Redirect\n.\nIf there is no active warehouse for the session, you will be prompted to select a warehouse.\nMonitor a specific connection using search and filters.\nYou can search by connection name. In the\n(search) box, enter the connection name to filter results.\nChoose\nRedirecting\nto filter the results by primary (\nTo\n) or secondary",
    "\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\nand then select\nClient Redirect\n.\nIf there is no active warehouse for the session, you will be prompted to select a warehouse.\nMonitor a specific connection using search and filters.\nYou can search by connection name. In the\n(search) box, enter the connection name to filter results.\nChoose\nRedirecting\nto filter the results by primary (\nTo\n) or secondary (\nFrom\n) connection.\nChoose the\n(accounts) menu to filter the results by account name.\nYou can review the following information about each connection:\nColumn\nDescription\nName\nConnection name.\nRedirecting\nIndicates if the connection is\nTo\na target account or\nFrom\na source account and the account name.\nIf this column contains\ndestinations available\n, there are no secondary connections.\nThe number of destinations available indicates the number of target accounts the primary connection can be replicated to.\nIf there is more than one secondary connection, each connection is detailed in a separate row.\nUsage\nDisplays the number of times the connection has been used in the last 7 days. You must sign in to the target account to view\nusage data for that account.\nConnection URL\nThe connection URL to use with Snowflake clients. Select the connection URL in the column to copy the URL.\nMonitor Client Redirect using SQL\n\u00b6\nYou can view connection details and monitor usage using the SHOW CONNECTIONS command and LOGIN_HISTORY function.\nView connection details\n\u00b6\nYou can retrieve connection names and details using the SHOW CONNECTIONS command:\nSHOW\nCONNECTIONS\n;\nCopy\nReturns:\n+--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\n| snowflake_region   | created_on                    | account_name        | name              | comment         | is_primary    | primary                       | failover_allowed_to_accounts        | connection_url                            | organization_name | account_locator   |\n|--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------|\n| AWS_US_WEST_2      | 2023-07-05 08:57:11.143 -0700 | MYORG.MYACCOUNT1    | MYCONNECTION      | NULL            | true          | MYORG.MYACCOUNT1",
    "_to_accounts        | connection_url                            | organization_name | account_locator   |\n|--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------|\n| AWS_US_WEST_2      | 2023-07-05 08:57:11.143 -0700 | MYORG.MYACCOUNT1    | MYCONNECTION      | NULL            | true          | MYORG.MYACCOUNT1.MYCONNECTION | MYORG.MYACCOUNT2, MYORG.MYACCOUNT3  | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR1 |\n| AWS_US_EAST_1      | 2023-07-08 09:15:11.143 -0700 | MYORG.MYACCOUNT2    | MYCONNECTION      | NULL            | false         | MYORG.MYACCOUNT1.MYCONNECTION | MYORG.MYACCOUNT2, MYORG.MYACCOUNT3  | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR1 |\n|--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------|\nVerify the connection URL used by your users\n\u00b6\nQuery the\nLOGIN_HISTORY , LOGIN_HISTORY_BY_USER\nfamily of table functions to view the login activity for your users within the last\n7 days. The output indicates which users and Snowflake clients have been using a connection URL. The REPORTED_CLIENT_TYPE and\nREPORTED_CLIENT_VERSION columns display the client and version used for each connection to Snowflake, and the CONNECTION column displays\nthe connection URL used, if any.\nNote\nIf a client authenticates through an identity provider (IdP) that is configured with the account URL rather than the connection URL, the\nIdP directs the client to the account URL after authentication is complete. The CONNECTION column for this login event is NULL. See\nAuthentication and Client Redirect\n(in this topic).\nFor example, retrieve up to 100 login events of every user your current role is allowed to monitor in the last 72 hours:\nSELECT\nevent_timestamp\n,\nuser_name\n,\nclient_ip\n,\nreported_client_type\n,\nis_success\n,\nconnection\nFROM\nTABLE\n(\nINFORMATION_SCHEMA\n.\nLOGIN_HISTORY\n(\nDATEADD\n(\n'HOURS",
    " account URL after authentication is complete. The CONNECTION column for this login event is NULL. See\nAuthentication and Client Redirect\n(in this topic).\nFor example, retrieve up to 100 login events of every user your current role is allowed to monitor in the last 72 hours:\nSELECT\nevent_timestamp\n,\nuser_name\n,\nclient_ip\n,\nreported_client_type\n,\nis_success\n,\nconnection\nFROM\nTABLE\n(\nINFORMATION_SCHEMA\n.\nLOGIN_HISTORY\n(\nDATEADD\n(\n'HOURS'\n,-\n72\n,\nCURRENT_TIMESTAMP\n()),\nCURRENT_TIMESTAMP\n()))\nORDER\nBY\nEVENT_TIMESTAMP\n;\nCopy\nCurrent limitations of Client Redirect\n\u00b6\nClient connections using a connection URL and OAuth integration require re-authentication when the connection URL is updated to point to a\ndifferent account if the OAuth security integration is not replicated to that account. For more information, refer to\nOAuth redirect behavior\n.\nWeb browsers may take several minutes to redirect due to browser cache.\nIf you need to verify that the redirect works, you can connect to Snowflake with a\ndifferent client\n.\nAlternatively, open a new private browser window (e.g. incognito mode in Google Chrome) to avoid browser caching issues. Note that some web\nbrowsers in private or incognito mode might still cache data. To avoid using the browser cache, close any open private browsers windows and\ntabs before you open a new private browser window.\nYou can only add one target account using Snowsight. To add more than one target account to the list of allowed failover\naccounts, use the\nALTER CONNECTION \u2026 ENABLE FAILOVER TO ACCOUNTS\ncommand.\nOn this page\nIntroduction to Client Redirect\nClient Redirect flow\nConfiguring Client Redirect\nConfiguring the DNS settings for private connectivity to the Snowflake service\nConfiguring Client Redirect and reader accounts\nUsing a connection URL\nAuthentication and Client Redirect\nRedirecting client connections\nModifying a connection\nDropping a connection\nMonitoring Client Redirect\nCurrent limitations of Client Redirect",
    "\nCurrent limitations of Client Redirect",
    "back\n\u00b6\nOnce the outage is resolved, promote the original primary connection to serve as the primary connection again.\nExecuted on the target account that previously served as the source account\n\u00b6\nSign in to the target account that served as the source account prior to the outage.\nPromote the secondary connection back to primary connection:\nALTER\nCONNECTION\nmyconnection\nPRIMARY\n;\nCopy\nIf private connectivity to the Snowflake service is enabled for your Snowflake account, you must create\nor update a DNS CNAME record for your connection URL. For more details, see\nModifying the DNS settings for private connectivity to the Snowflake service\n.\nConfiguring Client Redirect\n\u00b6\nThis section describes how to create a primary connection and one or more secondary connections in a connection group.\nPrerequisite\n\u00b6\nTo enable the Client Redirect feature for your accounts, an\norganization administrator\nmust enable\nreplication for two or more accounts. To enable replication, see\nPrerequisite: Enable replication for accounts in the organization\nfor detailed instructions.\nCreate a primary connection\n\u00b6\nImportant\nSnowflake assigned your organization a unique, generated name when it was created in the system. The organization name is a part of the\nconnection URL defined in a connection object and submitted by Snowflake clients to access an account. Before you create any connection\nobjects, verify that your organization name in Snowflake is satisfactory. To change your organization name in the system, contact\nSnowflake Support\n.\nYou can create a primary connection in the source account using Snowsight or\nSQL\n.\nCreate a primary and secondary connection using Snowsight\n\u00b6\nTo create a connection using Snowsight, complete the following steps:\nNote\nOnly a user with the ACCOUNTADMIN role can create a connection using Snowsight.\nYou must be signed in to the target account as a user with the ACCOUNTADMIN role. If not, you will be prompted to sign in.\nCurrently, if your account uses private connectivity, you can\u2019t use Snowsight to create a primary and secondary\nconnection.\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nClient Redirect\n.\nSelect\n+ Connection\n.\nSelect\nTarget Account\n.\nIn the\nConnection Name\nbox, enter a connection name that meets the following requirements:\nMust start with an alphabetic character and may only contain letters, decimal digits (0-9), and underscores",
    " private connectivity, you can\u2019t use Snowsight to create a primary and secondary\nconnection.\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nClient Redirect\n.\nSelect\n+ Connection\n.\nSelect\nTarget Account\n.\nIn the\nConnection Name\nbox, enter a connection name that meets the following requirements:\nMust start with an alphabetic character and may only contain letters, decimal digits (0-9), and underscores (_).\nMust be unique across connection names and account names in the organization.\nSelect\nCreate Connection\n.\nCreate a primary connection using SQL\n\u00b6\nNote\nOnly a user with the ACCOUNTADMIN role can execute the SQL commands in this section.\nCreate a new primary connection using the\nCREATE CONNECTION\ncommand. The name of each primary\nconnection must be unique across all connection and account names in the organization.\nThe connection name is included as part of the connection URL used to connect to Snowflake accounts.\nFor example, to create a connection named\nmyconnection\n:\nCREATE\nCONNECTION\nmyconnection\n;\nCopy\nModify this primary connection using an\nALTER CONNECTION \u2026 ENABLE FAILOVER TO ACCOUNTS\nstatement. Provide a comma-separated list of accounts in your organization that can store a failover option for this connection (i.e. a\nsecondary connection).\nAny account that stores a secondary connection must be hosted in a region different from the account that stores the primary connection.\nClient Redirect only operates successfully across regions. For example, if you try to redirect client connections from\naccount1\nto\naccount2\nin the same region, client redirect does not work.\nTo see the complete list of accounts in your organization that are enabled for replication, execute\nSHOW REPLICATION ACCOUNTS\n.\nFor example, allow accounts\nmyaccount2\nand\nmyaccount3\nin the\nmyorg\norganization to each store a secondary connection for the\nmyconnection\nconnection:\nALTER\nCONNECTION\nmyconnection\nENABLE\nFAILOVER\nTO\nACCOUNTS\nmyorg\n.\nmyaccount2\n,\nmyorg\n.\nmyaccount3\n;\nCopy\nExecute the\nSHOW CONNECTIONS\ncommand to view the details for the connection.\nSHOW\nCONNECTIONS\n;\n+\n--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\n| snowflake_region   |",
    "\nCONNECTION\nmyconnection\nENABLE\nFAILOVER\nTO\nACCOUNTS\nmyorg\n.\nmyaccount2\n,\nmyorg\n.\nmyaccount3\n;\nCopy\nExecute the\nSHOW CONNECTIONS\ncommand to view the details for the connection.\nSHOW\nCONNECTIONS\n;\n+\n--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\n| snowflake_region   | created_on                    | account_name        | name              | comment         | is_primary    | primary                       | failover_allowed_to_accounts        | connection_url                            | organization_name | account_locator   |\n|--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------|\n| AWS_US_WEST_2      | 2020-07-19 14:49:11.183 -0700 | MYORG.MYACCOUNT1    | MYCONNECTION      | NULL            | true          | MYORG.MYACCOUNT1.MYCONNECTION | MYORG.MYACCOUNT2, MYORG.MYACCOUNT3  | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR1 |\n+\n--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\nCopy\nCreate a secondary connection in each target account\n\u00b6\nCreate a secondary connection in one or more accounts, linked to a primary connection using\nCREATE CONNECTION \u2026 AS REPLICA OF\n. Note that you can only create a secondary connection in\nan account specified in the ALTER CONNECTION \u2026 ENABLE FAILOVER TO ACCOUNTS statement in\nCreate a Primary Connection\n.\nExecute a CREATE CONNECTION \u2026 AS REPLICA OF statement in each target account to create a replica of the specified primary connection.\nImportant\nEach secondary connection\nmust\nhave the same name as its primary connection. The connection name is included in the connection\nURL.\nExecute the SQL statements in this section in the\ntarget\naccount where you want to create a secondary connection.\nNote\nOnly a user with the ACCOUNTADMIN role can execute the SQL commands in this section.\nExecute the SHOW CONNECTIONS command to view all connections. Copy the value of the\nprimary\ncolumn for the primary connection.\nYou will use this value when creating the secondary connection",
    " connection\nmust\nhave the same name as its primary connection. The connection name is included in the connection\nURL.\nExecute the SQL statements in this section in the\ntarget\naccount where you want to create a secondary connection.\nNote\nOnly a user with the ACCOUNTADMIN role can execute the SQL commands in this section.\nExecute the SHOW CONNECTIONS command to view all connections. Copy the value of the\nprimary\ncolumn for the primary connection.\nYou will use this value when creating the secondary connection in the next step.\nSHOW\nCONNECTIONS\n;\n+\n--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\n| snowflake_region   | created_on                    | account_name        | name              | comment         | is_primary    | primary                       | failover_allowed_to_accounts        | connection_url                            | organization_name | account_locator   |\n|--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------|\n| AWS_US_WEST_2      | 2020-07-19 14:49:11.183 -0700 | MYORG.MYACCOUNT1    | MYCONNECTION      | NULL            | true          | MYORG.MYACCOUNT1.MYCONNECTION | MYORG.MYACCOUNT2, MYORG.MYACCOUNT3  | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR1 |\n+\n--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\nCopy\nExecute the CREATE CONNECTION \u2026 AS REPLICA OF command to create a secondary connection.\nFor example, create a secondary connection named\nmyconnection\nthat is linked to the\nmyorg.myaccount1.myconnection\nprimary\nconnection. After\nAS\nREPLICA\nOF\n, paste in the fully qualified name of the primary connection (the name that you copied from the\nSHOW CONNECTIONS output in the previous step).\nCREATE\nCONNECTION\nmyconnection\nAS\nREPLICA\nOF\nMYORG\n.\nMYACCOUNT1\n.\nMYCONNECTION\n;\nCopy\nExecute the SHOW CONNECTIONS command to verify the secondary connection was created.\nSHOW\nCONNECTIONS\n;\n+\n--------------------+-------------------------------+---------------------+-------------------+----------------",
    "PLICA\nOF\n, paste in the fully qualified name of the primary connection (the name that you copied from the\nSHOW CONNECTIONS output in the previous step).\nCREATE\nCONNECTION\nmyconnection\nAS\nREPLICA\nOF\nMYORG\n.\nMYACCOUNT1\n.\nMYCONNECTION\n;\nCopy\nExecute the SHOW CONNECTIONS command to verify the secondary connection was created.\nSHOW\nCONNECTIONS\n;\n+\n--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\n| snowflake_region   | created_on                    | account_name        | name              | comment         | is_primary    | primary                       | failover_allowed_to_accounts        | connection_url                            | organization_name | account_locator   |\n|--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------|\n| AWS_US_WEST_2      | 2020-07-19 14:49:11.183 -0700 | MYORG.MYACCOUNT1    | MYCONNECTION      | NULL            | true          | MYORG.MYACCOUNT1.MYCONNECTION | MYORG.MYACCOUNT2, MYORG.MYACCOUNT3  | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR1 |\n| AWS_US_EAST_1      | 2020-07-22 13:52:04.925 -0700 | MYORG.MYACCOUNT2    | MYCONNECTION      | NULL            | false         | MYORG.MYACCOUNT1.MYCONNECTION |                                     | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR2 |\n+\n--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\nCopy\nGrant the FAILOVER Privilege to a Role \u2014\nOptional\n\u00b6\nAn account administrator can grant the FAILOVER privilege on a connection object to an account role.\nThis enables a user other than the account administrator to promote a secondary connection to serve as the primary connection.\nFor example, to grant the role\nmy_failover_role\nthe ability to fail over the connection\nmyconnection\n, execute\nthe following statement on the\ntarget\naccount:\n",
    "-------------------+\nCopy\nGrant the FAILOVER Privilege to a Role \u2014\nOptional\n\u00b6\nAn account administrator can grant the FAILOVER privilege on a connection object to an account role.\nThis enables a user other than the account administrator to promote a secondary connection to serve as the primary connection.\nFor example, to grant the role\nmy_failover_role\nthe ability to fail over the connection\nmyconnection\n, execute\nthe following statement on the\ntarget\naccount:\nGRANT\nFAILOVER\nON\nCONNECTION\nmyconnection\nTO\nROLE\nmy_failover_role\n;\nCopy\nA user with the role\nmy_failover_role\ncan now promote the secondary connection\nmyconnection\nto serve as\nprimary connection in the case of failover:\nUSE\nROLE\nmy_failover_role\n;\nALTER\nCONNECTION\nmyconnection\nPRIMARY\n;\nCopy\nFor more information on redirecting client connections, see\nRedirecting client connections\n.\nConfiguring the DNS settings for private connectivity to the Snowflake service\n\u00b6\nIf private connectivity to the Snowflake service is enabled for your Snowflake account, then your network administrator must create and\nmanage a DNS record for your connection URL. Your network administrator can use a CNAME record, alias record, or an alias based on the\nconfiguration of the network architecture. For consistency, the following example uses a CNAME record.\nThese steps use AWS PrivateLink as an example, and the steps are the same if your Snowflake account uses Azure Private Link or Google Cloud\nPrivate Service Connect:\nExecute SHOW CONNECTIONS in one of your accounts in which client redirect is enabled. For example, suppose AWS PrivateLink is enabled\nfor\nmyaccount1\nand\nmyaccount2\n.\nSHOW\nCONNECTIONS\n;\n+\n--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\n| snowflake_region   | created_on                    | account_name        | name              | comment         | is_primary    | primary                       | failover_allowed_to_accounts        | connection_url                            | organization_name | account_locator   |\n|--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------|\n| AWS_US_WEST_2      | 2020-07-19 14:49:11.",
    "| snowflake_region   | created_on                    | account_name        | name              | comment         | is_primary    | primary                       | failover_allowed_to_accounts        | connection_url                            | organization_name | account_locator   |\n|--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------|\n| AWS_US_WEST_2      | 2020-07-19 14:49:11.183 -0700 | MYORG.MYACCOUNT1    | MYCONNECTION      | NULL            | true          | MYORG.MYACCOUNT1.MYCONNECTION | MYORG.MYACCOUNT2, MYORG.MYACCOUNT3  | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR1 |\n|--------------------|-------------------------------|---------------------|-------------------|-----------------|---------------|-------------------------------|-------------------------------------|-------------------------------------------|-------------------|-------------------|\n| AWS_US_WEST_2      | 2020-07-19 14:49:11.183 -0700 | MYORG.MYACCOUNT1    | MYCONNECTION      | NULL            | true          | MYORG.MYACCOUNT1.MYCONNECTION | MYORG.MYACCOUNT2, MYORG.MYACCOUNT3  | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR1 |\n| AWS_US_EAST_1      | 2020-07-22 13:52:04.925 -0700 | MYORG.MYACCOUNT2    | MYCONNECTION      | NULL            | false         | MYORG.MYACCOUNT1.MYCONNECTION |                                     | myorg-myconnection.snowflakecomputing.com | MYORG             | MYACCOUNTLOCATOR2 |\n+\n--------------------+-------------------------------+---------------------+-------------------+-----------------+---------------+-------------------------------+-------------------------------------+-------------------------------------------+-------------------+-------------------+\nCopy\nNote that the output of this command in the CONNECTION_URL column should match the\nprivatelink-connection-urls\nlist when calling\nthe\nSYSTEM$GET_PRIVATELINK_CONFIG\nfunction in either\nmyaccount1\nor\nmyaccount2\n. This list already\ncontains the connection URL formatted with the\nprivatelink\nsegment (as shown in the next step). You can optionally run the command\nin this step or",
    "Replicating databases and account objects across multiple accounts\n\u00b6\nStandard & Business Critical Feature\nDatabase and share replication are available to all accounts.\nReplication of other account objects & failover/failback require Business Critical Edition (or higher).\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nThis topic describes the steps necessary to replicate account objects and data across Snowflake accounts in the same organization,\nand keep the objects and data synchronized. Account replication can occur across Snowflake accounts in different\nregions\nand across\ncloud platforms\n.\nNote\nWhen you upgrade an account to Business Critical Edition (or higher), it might take up to 12 hours for failover capabilities\nto become available.\nRegion support for replication and failover/failback\n\u00b6\nCustomers can replicate across all regions within a Region Group. To replicate between regions in different\nRegion groups\n(for example, from a Snowflake commercial region to a Snowflake government region), please contact\nSnowflake Support\nto enable\naccess.\nTransitioning from database replication to group-based replication\n\u00b6\nDatabases that have been enabled for replication using\nALTER DATABASE\nmust have replication\ndisabled\nbefore\nthey can be added to a replication or failover group.\nNote\nExecute the SQL statements in this section using the ACCOUNTADMIN role.\nStep 1. Disable replication for a replication enabled database\n\u00b6\nExecute the\nSYSTEM$DISABLE_DATABASE_REPLICATION\nfunction to disable replication for a primary database,\nalong with any secondary databases linked to it, in order to add it to a replication or failover group.\nExecute the following SQL statement from the source account with the primary database:\nSELECT\nSYSTEM$DISABLE_DATABASE_REPLICATION\n(\n'mydb'\n);\nCopy\nStep 2. Add the database to a primary failover group and create a secondary failover group\n\u00b6\nOnce you have successfully disabled replication for a database, you can add the primary database to a failover group in the source account.\nThen create a secondary failover group in the target account. When the secondary failover\ngroup is refreshed in the target account, the previously secondary database will automatically be added as a member of the secondary\nfailover group and refreshed with the changes from the primary database.\nFor more details on creating primary and secondary failover groups, see\nWorkflow\n.\nNote\nWhen you add a previously replicated database to a replication or failover group, Snowflake does\nnot\nre-replicate the data",
    "Then create a secondary failover group in the target account. When the secondary failover\ngroup is refreshed in the target account, the previously secondary database will automatically be added as a member of the secondary\nfailover group and refreshed with the changes from the primary database.\nFor more details on creating primary and secondary failover groups, see\nWorkflow\n.\nNote\nWhen you add a previously replicated database to a replication or failover group, Snowflake does\nnot\nre-replicate the data that\nhas already been replicated for that database. Only changes since the last refresh are replicated when the group is refreshed.\nWorkflow\n\u00b6\nThe following SQL statements demonstrate the workflow for enabling account and database object replication and refreshing objects. Each step\nis discussed in detail below.\nNote\nThe following examples require replication be enabled for the source and target accounts. For details, see\nPrerequisite: Enable replication for accounts in the organization\n.\nExamples\n\u00b6\nExecute the following SQL statements in your preferred Snowflake client to enable account and database object replication and failover,\nand refresh objects.\nExecuted on source account\n\u00b6\nCreate a role and grant it the CREATE FAILOVER GROUP privilege. This step is\noptional\n:\nUSE\nROLE\nACCOUNTADMIN\n;\nCREATE\nROLE\nmyrole\n;\nGRANT\nCREATE\nFAILOVER\nGROUP\nON\nACCOUNT\nTO\nROLE\nmyrole\n;\nCopy\nCreate a failover group in the source account and enable replication to specific target accounts.\nNote\nIf you have databases to add to a replication or failover group that have been previously enabled for database replication and failover\nusing\nALTER DATABASE\n, follow the\nTransitioning from database replication to group-based replication\ninstructions (in this\ntopic) before adding them to a group.\nTo add a database to a failover group, the active role must have the MONITOR privilege on the database. For details\non database privileges, see\nDatabase privileges\n(in a separate topic).\nUSE\nROLE\nmyrole\n;\nCREATE\nFAILOVER\nGROUP\nmyfg\nOBJECT_TYPES\n=\nUSERS\n,\nROLES\n,\nWAREHOUSES\n,\nRESOURCE MONITORS\n,\nDATABASES\nALLOWED_DATABASES\n=\ndb1\n,\ndb2\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmyaccount2\n,\nmyorg\n.\nmyaccount3\nREPLICATION_SCHEDULE\n=\n'10 MINUTE'\n;\n",
    "\n, execute the following statement from the target account:\nUSE\nROLE\nmy_replication_role\n;\nALTER\nFAILOVER\nGROUP\nmyfg\nREFRESH\n;\nCopy\nStep 7. Grant the FAILOVER privilege on failover group to role \u2014\nOptional\n\u00b6\nTo execute the command to fail over a secondary failover group in a target account, you must use a role with the\nFAILOVER privilege\non the failover group. The FAILOVER privilege is currently\nnot\nreplicated and must be granted in each source and target account.\nFor more information, see\nReplication of roles and grants\n.\nFor example, to grant the FAILOVER privilege to role\nmy_failover_role\non failover group\nmy_fg\n, execute the\nfollowing statement in the\ntarget account\nusing a role with the OWNERSHIP privilege on the group:\nGRANT\nFAILOVER\nON\nFAILOVER\nGROUP\nmyfg\nTO\nROLE\nmy_failover_role\n;\nCopy\nFor instructions on creating a custom role with a specified set of privileges, see\nCreating custom roles\n.\nFor general information about roles and privilege grants for performing SQL actions on\nsecurable objects\n, see\nOverview of Access Control\n.\nSchema-level replication for failover groups\n\u00b6\nFor databases in failover groups, you can optionally configure the REPLICABLE_WITH_FAILOVER_GROUPS parameter on the database\nand/or individual schemas in the database to specify a subset of schemas for replication.\nThis feature enables you to control the schemas in a failover group that are replicated, which is useful if only a subset of data\nin a database needs the added disaster recovery protection provided by failover.\nBecause this parameter is enabled by default for all databases and the schemas they contain, you adjust the replication\ngranularity by choosing which databases and/or schemas to omit from replication. You can further fine-tune the\nreplication settings by allowing certain schemas to be replicated even though the database that contains them isn\u2019t replicated.\nSpecify schemas to replicate or skip\n\u00b6\nYou can explicitly specify schemas to replicate or skip in a database in a failover group using the\noptional\nREPLICABLE_WITH_FAILOVER_GROUPS\nparameter.\nREPLICABLE_WITH_FAILOVER_GROUPS parameter\n\u00b6\nThe REPLICABLE_WITH_FAILOVER_GROUPS parameter specifies whether a schema that belongs",
    " further fine-tune the\nreplication settings by allowing certain schemas to be replicated even though the database that contains them isn\u2019t replicated.\nSpecify schemas to replicate or skip\n\u00b6\nYou can explicitly specify schemas to replicate or skip in a database in a failover group using the\noptional\nREPLICABLE_WITH_FAILOVER_GROUPS\nparameter.\nREPLICABLE_WITH_FAILOVER_GROUPS parameter\n\u00b6\nThe REPLICABLE_WITH_FAILOVER_GROUPS parameter specifies whether a schema that belongs to a database in a failover group is replicated.\nThis parameter can be set on a database and any/all schemas in the database. If the parameter is set for a database, all of the schemas\nin the database inherit the value, unless a different value is explicitly set for any given schema.\nThe parameter accepts two values,\n'YES'\nor\n'NO'\n(case-insensitive), and is optional:\nIf REPLICABLE_WITH_FAILOVER_GROUPS is not explicitly set on a database (or explicitly unset), the database follows the standard replication\nbehavior, which is equivalent to setting the parameter to\n'YES'\n.\nIf REPLICABLE_WITH_FAILOVER_GROUPS is not explicitly set on a schema (or explicitly unset), the replication behavior is inherited from its\nparent database.\nALTER\nDATABASE\n<name>\nSET\nREPLICABLE_WITH_FAILOVER_GROUPS\n=\n{\n'YES'\n|\n'NO'\n}\nALTER\nDATABASE\n<name>\nUNSET\nREPLICABLE_WITH_FAILOVER_GROUPS\nALTER\nSCHEMA\n<name>\nSET\nREPLICABLE_WITH_FAILOVER_GROUPS\n=\n{\n'YES'\n|\n'NO'\n}\nALTER\nSCHEMA\n<name>\nUNSET\nREPLICABLE_WITH_FAILOVER_GROUPS\nCopy\nSecurity requirements\n\u00b6\nTo set or unset this parameter on a database or a schema, the following privileges are required:\nREPLICATE (\naccount-level privilege\n). Prior to the schema-level replication feature, this privilege was\nonly an object-level privilege on replication groups and failover groups.\nUsers with the ACCOUNTADMIN role can grant this privilege to other roles.\nUSAGE (\ndatabase\nand\nschema\nprivilege)\nor any similar privileges that enable taking action on the database and schema.\nExamples\n\u00b6\nGrant necessary privileges on a pre-existing role,\nreplicationadmin\n:\nUSE\nROLE\nACCOUNTADMIN\n;\nGRANT\nREPLICATE\nON\nACCOUNT\nTO\nROLE\nreplicationadmin",
    ", this privilege was\nonly an object-level privilege on replication groups and failover groups.\nUsers with the ACCOUNTADMIN role can grant this privilege to other roles.\nUSAGE (\ndatabase\nand\nschema\nprivilege)\nor any similar privileges that enable taking action on the database and schema.\nExamples\n\u00b6\nGrant necessary privileges on a pre-existing role,\nreplicationadmin\n:\nUSE\nROLE\nACCOUNTADMIN\n;\nGRANT\nREPLICATE\nON\nACCOUNT\nTO\nROLE\nreplicationadmin\n;\nGRANT\nUSAGE\nON\nDATABASE\ndb1\nTO\nROLE\nreplicationadmin\n;\nGRANT\nUSAGE\nON\nSCHEMA\ndb1\n.\nsch1\nTO\nROLE\nreplicationadmin\n;\nCopy\nReplicate only one schema,\nsch1\n, in the\ndb1\ndatabase:\nUSE\nROLE\nreplicationadmin\n;\nALTER\nDATABASE\ndb1\nSET\nREPLICABLE_WITH_FAILOVER_GROUPS\n=\n'NO'\n;\nALTER\nSCHEMA\nsch1\nSET\nREPLICABLE_WITH_FAILOVER_GROUPS\n=\n'YES'\n;\nCopy\nReplicate all schemas except one schema,\nsch2\n, in the\ndb2\ndatabase:\nUSE\nROLE\nreplicationadmin\n;\nALTER\nDATABASE\ndb2\nSET\nREPLICABLE_WITH_FAILOVER_GROUPS\n=\n'YES'\n;\nALTER\nSCHEMA\nsch2\nSET\nREPLICABLE_WITH_FAILOVER_GROUPS\n=\n'NO'\n;\nCopy\nRefresh of schemas with REPLICABLE_WITH_FAILOVER_GROUPS set in target accounts\n\u00b6\nDuring a database refresh:\nSchemas with REPLICABLE_WITH_FAILOVER_GROUPS set to\n'YES'\nare replicated from the source account to the target account.\nSchemas with REPLICABLE_WITH_FAILOVER_GROUPS set to\n'NO'\nare not replicated, except in the following two scenarios:\nThe target schema is a replica of the source account schema. In this case, the target schema is always synchronized with its source schema.\nThe target schema has a name conflict with the source account schema. In this situation, the replication job fails due to the name\nconflict.\nList databases and schemas with REPLICABLE_WITH_FAILOVER_GROUPS set in your account\n\u00b6\nYou can list the values set for the REPLICABLE_WITH_FAILOVER_GROUPS parameter in the current account\nby querying the ACCOUNT_USAGE and INFORMATION_SCHEMA",
    " source account schema. In this case, the target schema is always synchronized with its source schema.\nThe target schema has a name conflict with the source account schema. In this situation, the replication job fails due to the name\nconflict.\nList databases and schemas with REPLICABLE_WITH_FAILOVER_GROUPS set in your account\n\u00b6\nYou can list the values set for the REPLICABLE_WITH_FAILOVER_GROUPS parameter in the current account\nby querying the ACCOUNT_USAGE and INFORMATION_SCHEMA views.\nTip\nIf you aren\u2019t familiar with why you might use the ACCOUNT_USAGE or INFORMATION_SCHEMA views,\nsee\nDifferences between Account Usage and Information Schema\n.\nExamples\n\u00b6\nFor these examples, we\u2019ll use the INFORMATION_SCHEMA views. That way, you can see the settings immediately\nafter making any changes.\nUsing the pre-existing\nreplicationadmin\nrole, return all the parameter values for the account, which has two databases:\ndb1\ndatabase explicitly set to\nNO\nand\nsch1\nschema in the database explicitly set to\nYES\n. Only that\none schema in the database is eligible for replication.\ndb2\ndatabase explicitly set to\nYES\nand\nsch2\nschema in the database explicitly set to\nNO\n. All schemas\nin the database are eligible for replication except for that one schema.\nUSE\nROLE\nreplicationadmin\n;\nSELECT\ndatabase_name\n,\nreplicable_with_failover_groups\nFROM\ndb1\n.\nINFORMATION_SCHEMA\n.\nDATABASES\n;\nCopy\n+---------------+---------------------------------+\n| DATABASE_NAME | REPLICABLE_WITH_FAILOVER_GROUPS |\n+---------------+---------------------------------+\n| DB1           | NO                              |\n| DB2           | YES                             |\n| DB3           | UNSET                           |\n+---------------+---------------------------------+\nSELECT\nschema_name\n,\ncatalog_name\n,\nreplicable_with_failover_groups\nFROM\ndb1\n.\nINFORMATION_SCHEMA\n.\nSCHEMATA\nORDER\nBY\ncatalog_name\n;\nCopy\n+--------------------+--------------+---------------------------------+\n| SCHEMA_NAME        | CATALOG_NAME | REPLICABLE_WITH_FAILOVER_GROUPS |\n+--------------------+--------------+---------------------------------+\n| PUBLIC             | DB1          | NO                              |\n| SCH1               | DB1          | YES                             |\n| SCH2               | DB1          | NO                              |\n| SCH3               | DB1          | NO                              |\n| INFORMATION_SCHEMA | DB1         ",
    "ORDER\nBY\ncatalog_name\n;\nCopy\n+--------------------+--------------+---------------------------------+\n| SCHEMA_NAME        | CATALOG_NAME | REPLICABLE_WITH_FAILOVER_GROUPS |\n+--------------------+--------------+---------------------------------+\n| PUBLIC             | DB1          | NO                              |\n| SCH1               | DB1          | YES                             |\n| SCH2               | DB1          | NO                              |\n| SCH3               | DB1          | NO                              |\n| INFORMATION_SCHEMA | DB1          | UNSET                           |\n+--------------------+--------------+---------------------------------+\nUSE\nROLE\nreplicationadmin\n;\nSELECT\nschema_name\n,\ncatalog_name\n,\nreplicable_with_failover_groups\nFROM\ndb2\n.\nINFORMATION_SCHEMA\n.\nSCHEMATA\nORDER\nBY\ncatalog_name\n;\nCopy\n+--------------------+--------------+---------------------------------+\n| SCHEMA_NAME        | CATALOG_NAME | REPLICABLE_WITH_FAILOVER_GROUPS |\n+--------------------+--------------+---------------------------------+\n| PUBLIC             | DB2          | YES                             |\n| SCH1               | DB2          | YES                             |\n| SCH2               | DB2          | NO                              |\n| SCH3               | DB2          | YES                             |\n| INFORMATION_SCHEMA | DB2          | UNSET                           |\n+--------------------+--------------+---------------------------------+\nApply global IDs to objects created by scripts in target accounts\n\u00b6\nIf you created account objects, for example, users and roles, in your target account by any means\nother than\nvia replication (for example,\nusing scripts), these users and roles have no global identifier by default. The refresh operation uses global identifiers to synchronize\nthese objects to the same objects in the source account.\nIn most cases, when a target account is refreshed from the source account, the refresh operation\ndrops\nany account objects of the\ntypes in the\nOBJECT_TYPES\nlist in the target account that have no global identifier. The initial replication of users and roles to\na target account, however, might cause the first refresh operation to fail. For details on this behavior, refer to\nInitial replication of users and roles\n.\nUse SYSTEM$LINK_ACCOUNT_OBJECTS_BY_NAME() to apply global IDs\n\u00b6\nYou can prevent the loss of some object types by linking matching objects with the same name in the source and target accounts. The\nSYSTEM$LINK_ACCOUNT_OBJECTS_BY_NAME function adds a global identifier to account objects in the target account.\nNote\nGlobal identifiers",
    "a target account, however, might cause the first refresh operation to fail. For details on this behavior, refer to\nInitial replication of users and roles\n.\nUse SYSTEM$LINK_ACCOUNT_OBJECTS_BY_NAME() to apply global IDs\n\u00b6\nYou can prevent the loss of some object types by linking matching objects with the same name in the source and target accounts. The\nSYSTEM$LINK_ACCOUNT_OBJECTS_BY_NAME function adds a global identifier to account objects in the target account.\nNote\nGlobal identifiers are only added to account objects that are included in a replication or failover group for the\nfollowing object types:\nRESOURCE_MONITOR\nROLE\nUSER\nWAREHOUSE\nApply global identifiers to account objects in the target account of the types included in the\nobject_types\nlist for failover\ngroup\nmyfg\n:\nExecute the following SQL statement using the ACCOUNTADMIN role:\nSELECT\nSYSTEM$LINK_ACCOUNT_OBJECTS_BY_NAME\n(\n'myfg'\n);\nCopy\nInitial replication of users and roles\n\u00b6\nThe behavior of the initial refresh operation for USERS and ROLES object types can vary depending on whether or not there are matching\nobjects with the same name in the target account.\nNote\nThe behavior described in this section applies only the\nfirst\ntime these object types are replicated to the target account.\nThe scenarios below describe the replication of USERS. The same also applies to the replication of ROLES.\nIf there are existing users in the target account with the same name as users in the source account, the initial refresh operation\nfails and describes the two options you have to continue:\nForce the refresh operation and allow any existing users in the target account to be dropped. The users in the source account\nwill be replicated to the target account.\nTo force a refresh for a group, use the FORCE parameter for the refresh command. For example, to force the refresh of a failover\ngroup, execute the following command:\nALTER\nFAILOVER\nGROUP\n<\nfg_name\n>\nREFRESH\nFORCE\n;\nCopy\nLink the account objects by name. The\nSYSTEM$LINK_ACCOUNT_OBJECTS_BY_NAME\nfunction links\nusers with the same name in both the target account and the source account. Users in the target account that are linked are\nnot deleted.\nTo link account objects by name, execute the following command:\nSELECT\nSYSTEM$LINK_ACCOUNT_OBJECTS_BY_NAME\n(\n'<rg_name>'\n);\nCopy\nNote\nAny user in the target account that\ndoes",
    ">\nREFRESH\nFORCE\n;\nCopy\nLink the account objects by name. The\nSYSTEM$LINK_ACCOUNT_OBJECTS_BY_NAME\nfunction links\nusers with the same name in both the target account and the source account. Users in the target account that are linked are\nnot deleted.\nTo link account objects by name, execute the following command:\nSELECT\nSYSTEM$LINK_ACCOUNT_OBJECTS_BY_NAME\n(\n'<rg_name>'\n);\nCopy\nNote\nAny user in the target account that\ndoes not\nhave a matching user in the source account with the same name is\ndropped\n.\nIf there are\nno\nusers in the target account with names matching users in the source account, the initial refresh operation in\nthe target account drops all users. This can result in the following data and metadata loss:\nIf USERS are included in the OBJECT_TYPES list for a replication or failover group:\nWorksheets are lost.\nQuery history is lost.\nIf USERS are included in the OBJECT_TYPES list, but ROLES is not:\nPrivilege grants to users are lost.\nIf ROLES are included in the OBJECT_TYPES list:\nPrivilege grants to share objects are lost.\nTo avoid dropping users or roles in the target account:\nIn the source account, manually recreate any users or roles that exist\nonly\nin the target account before the initial replication.\nIn the target account, link matching objects with the same name in both accounts using the\nSYSTEM$LINK_ACCOUNT_OBJECTS_BY_NAME\nfunction.\nConfigure cloud storage access for secondary storage integrations\n\u00b6\nIf you enable storage integration replication, you must take additional steps after the\nstorage integration is replicated to target accounts.\nThe replicated integration has its own identity and access management (IAM) entity that is different from the identity\nand IAM entity of the primary integration. Therefore, you must update your cloud provider permissions\nto grant the replicated integration access to your cloud storage.\nYou only need to configure this trust relationship on target accounts one time.\nThe process is similar to granting access in the source account.\nSee the following pages for more information:\nConfiguring a Snowflake storage integration to access Amazon S3\nConfiguring an integration for Google Cloud Storage\nConfiguring a Snowflake storage integration for Azure\nConfigure automated refresh for directory tables on secondary stages\n\u00b6\nIf you replicate an external stage with a directory table, and you have configured automated refresh for the source directory table,\nyou must take steps to configure\nautomated refresh\nfor the secondary",
    " process is similar to granting access in the source account.\nSee the following pages for more information:\nConfiguring a Snowflake storage integration to access Amazon S3\nConfiguring an integration for Google Cloud Storage\nConfiguring a Snowflake storage integration for Azure\nConfigure automated refresh for directory tables on secondary stages\n\u00b6\nIf you replicate an external stage with a directory table, and you have configured automated refresh for the source directory table,\nyou must take steps to configure\nautomated refresh\nfor the secondary directory table.\nThe process is similar to setting up automated refresh in your source account. See the following for more information:\nAmazon S3: The configuration process depends on how you set up event notifications.\nIf you use Amazon S3 Event Notifications with Amazon Simple Queue Service (SQS),\nfollow the instructions in\nStep 2: Configure event notifications\n.\nYou can also migrate from SQS to SNS. For more information, see\nMigrate to Amazon Simple Notification Service (SNS)\n.\nIf you use Amazon Simple Notification Service (SNS), see\nSubscribing the Snowflake SQS Queue to your SNS topic\n.\nGoogle Cloud Storage: Create a new subscription to your Pub/Sub topic and a new notification integration in your target account.\nThen, grant Snowflake access to the Pub/Sub subscription. For instructions,\nsee\nConfiguring Automation Using GCS Pub/Sub\n.\nAzure Blob Storage: Create a new Event Grid subscription and storage queue. Then, create a new notification integration in the\ntarget account and grant Snowflake access to your storage queue. For instructions,\nsee\nConfiguring Automation With Azure Event Grid\n.\nImportant\nAfter you complete these configuration steps in your target account,\nyou should perform a full refresh of your directory table to ensure that it has not missed any notifications.\nFor Google Cloud Storage and Azure Blob Storage, the name of the notification integration in each target account must match the name of\nthe notification integration in the source account.\nConfigure notifications for secondary auto-ingest pipes\n\u00b6\nYou must take additional steps to configure cloud notifications for secondary auto-ingest pipes before failover.\nThis section covers why this additional configuration is required, and how to complete it for each supported cloud provider.\nAmazon S3\n\u00b6\nThe configuration process depends on how you set up event notifications. For example,\nsuppose you have an auto-ingest pipe that relies on an Amazon Simple Notification Service (SNS) topic\nto publish messages about the Snowflake stage location.\n",
    "-ingest pipes\n\u00b6\nYou must take additional steps to configure cloud notifications for secondary auto-ingest pipes before failover.\nThis section covers why this additional configuration is required, and how to complete it for each supported cloud provider.\nAmazon S3\n\u00b6\nThe configuration process depends on how you set up event notifications. For example,\nsuppose you have an auto-ingest pipe that relies on an Amazon Simple Notification Service (SNS) topic\nto publish messages about the Snowflake stage location.\nWhen you replicate the pipe to a target account, Snowflake automatically creates a new Amazon Simple Queue Service (SQS) queue.\nYou must subscribe this SQS queue for your target account to the SNS topic to get notifications about the stage location.\nIf you use Amazon S3 Event Notifications with Amazon Simple Queue Service (SQS),\nfollow the instructions in\nStep 4: Configure event notifications\n.\nImportant\nTo ensure that the pipe has not missed any notifications, you should refresh the pipe after switching to the new SQS queue.\nYou can also migrate from SQS to SNS. For more information, see\nMigrate to Amazon Simple Notification Service (SNS)\n.\nIf you use Amazon Simple Notification Service (SNS), see\nSubscribing the Snowflake SQS Queue to your SNS topic\n.\nIf you use Amazon EventBridge, see\nOption 3: Setting up Amazon EventBridge to automate Snowpipe\n.\nMicrosoft Azure Blob Storage\n\u00b6\nA pipe that automatically loads data from files located on a stage in Microsoft Azure blob storage requires an Event Grid\nsubscription, storage queue, and a notification integration bound to the storage queue. A secondary pipe in a target account needs a separate\nEvent Grid, storage queue, and notification integration bound to the storage queue. The Event Grid in both source and target accounts must be\nconfigured as endpoints for the same Azure Storage source.\nSee the diagram below for configuration details:\nCreate a new Event Grid subscription and storage queue. Then, create a new notification integration in the target account and\ngrant Snowflake access to your storage queue. For instructions, see\nConfiguring Automation With Azure Event Grid\n.\nImportant\nThe name of the notification integration in each target account\nmust\nmatch the name of the notification integration in\nthe source account.\nExternal stage for Google Cloud Storage\n\u00b6\nA pipe that automatically loads data from files located in Google Cloud Storage requires a\nGoogle Pub/Sub subscription and a notification integration\nthat references",
    " Then, create a new notification integration in the target account and\ngrant Snowflake access to your storage queue. For instructions, see\nConfiguring Automation With Azure Event Grid\n.\nImportant\nThe name of the notification integration in each target account\nmust\nmatch the name of the notification integration in\nthe source account.\nExternal stage for Google Cloud Storage\n\u00b6\nA pipe that automatically loads data from files located in Google Cloud Storage requires a\nGoogle Pub/Sub subscription and a notification integration\nthat references that subscription. Each replicated pipe in a target account also requires a Google Pub/Sub subscription and a\nnotification integration that references that subscription.\nThe Pub/Sub subscription in each source and target account must be subscribed to the same Pub/Sub Topic\nthat receives notifications from the Google Cloud Storage source.\nSee the diagram below for configuration details:\nCreate a new subscription to your Pub/Sub topic and a new notification integration in your target account.\nThen, grant Snowflake access to the Pub/Sub subscription. For instructions,\nsee\nConfiguring Automation Using GCS Pub/Sub\n.\nImportant\nThe name of the notification integration in each target account\nmust\nmatch the name of the notification integration in\nthe source account.\nUpdating the remote service for API integrations\n\u00b6\nIf you have enabled API integration replication, additional steps are required after the API integration is replicated to the target account.\nThe replicated integration has its own identity and access management (IAM) entity that are different from the identity and IAM entity\nof the primary integration. Therefore, you must update the permissions on the remote service to grant access to replicated functions.\nThe process is similar to granting access to the functions on the primary account. See the below links for more details:\nAmazon Web Services\nSet up the trust relationship(s) between Snowflake and the new IAM role\n.\nGoogle Cloud Platform:\nCreate a GCP Security Policy for the Proxy Service\n.\nMicrosoft Azure:\nStep 1.\nLink the API integration for Azure\nStep 2.\nCreate a validate-JWT policy\nComparing data sets in primary and secondary databases\n\u00b6\nSnowflake performs automatic verification checks as part of each replication refresh operation.\nIf a verification failure occurs, the refresh fails. Therefore, you don\u2019t have to manually verify\nthe replicated data. If you need additional verification for compliance reasons, you\ncan perform manual verification steps after the refresh operation finishes.\nAutomatic verification by Snowflake\n\u00b6\nSnowflake currently performs the following checks between the primary and secondary account, after",
    " privileges\n(in a separate topic).\nUSE\nROLE\nmyrole\n;\nCREATE\nFAILOVER\nGROUP\nmyfg\nOBJECT_TYPES\n=\nUSERS\n,\nROLES\n,\nWAREHOUSES\n,\nRESOURCE MONITORS\n,\nDATABASES\nALLOWED_DATABASES\n=\ndb1\n,\ndb2\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmyaccount2\n,\nmyorg\n.\nmyaccount3\nREPLICATION_SCHEDULE\n=\n'10 MINUTE'\n;\nCopy\nExecuted on target account\n\u00b6\nCreate a role in the target account and grant it the CREATE FAILOVER GROUP privilege. This step is\noptional\n:\nUSE\nROLE\nACCOUNTADMIN\n;\nCREATE\nROLE\nmyrole\n;\nGRANT\nCREATE\nFAILOVER\nGROUP\nON\nACCOUNT\nTO\nROLE\nmyrole\n;\nCopy\nCreate a failover group in the target account as a replica of the failover group in the source account.\nNote\nIf account objects (for example, users or roles) exist in the target account that do not exist in the source account, refer to\nInitial replication of users and roles\nbefore creating a secondary group.\nUSE\nROLE\nmyrole\n;\nCREATE\nFAILOVER\nGROUP\nmyfg\nAS\nREPLICA\nOF\nmyorg\n.\nmyaccount1\n.\nmyfg\n;\nCopy\nManually refresh the secondary failover group. This is an\noptional\nstep. If the primary failover group is created with\na replication schedule, the initial refresh of the secondary failover group is automatically executed when the secondary\nfailover group is created.\nCreate a role with the REPLICATE privilege on the failover group. This step is\noptional\n.\nExecute in the target account using a role with the OWNERSHIP privilege on the failover group:\nGRANT\nREPLICATE\nON\nFAILOVER\nGROUP\nmyfg\nTO\nROLE\nmy_replication_role\n;\nCopy\nExecute the refresh statement using a role with the REPLICATE privilege:\nUSE\nROLE\nmy_replication_role\n;\nALTER\nFAILOVER\nGROUP\nmyfg\nREFRESH\n;\nCopy\nCreate a role with the FAILOVER privilege on the failover group. This step is\noptional\n.\nExecute in the target account using a role with the OWNERSHIP privilege on the failover group:\nGRANT\nFAILO",
    " validate-JWT policy\nComparing data sets in primary and secondary databases\n\u00b6\nSnowflake performs automatic verification checks as part of each replication refresh operation.\nIf a verification failure occurs, the refresh fails. Therefore, you don\u2019t have to manually verify\nthe replicated data. If you need additional verification for compliance reasons, you\ncan perform manual verification steps after the refresh operation finishes.\nAutomatic verification by Snowflake\n\u00b6\nSnowflake currently performs the following checks between the primary and secondary account, after each refresh operation:\nSnowflake compares the hash values between the primary and secondary account, for all files that were replicated.\nFor each table, Snowflake compares the following values between the primary and secondary account:\nFile count.\nRow count.\nByte count.\nManual verification\n\u00b6\nIf database objects are replicated in a replication or failover group, you can use the\nHASH_AGG\nfunction to compare the rows in some or all tables in a\nprimary and secondary database to verify data consistency. The HASH_AGG function returns an\naggregate signed 64-bit hash value over the set of input rows. The hash value is the same regardless\nof the ordering of the input rows.\nQuery this function on all tables, or a random subset of tables, in both the secondary account and\nthe primary account. On the primary account, use an\nAT | BEFORE\nclause\nto specify the point in time of the latest refresh for the associated database. Compare the output\nbetween the queries on both accounts.\nExample of manually verifying data after a refresh\n\u00b6\nIn the following examples, the database\nmydb\nis included in the failover group\nmyfg\n. The\ndatabase\nmydb\ncontains the table\nmyschema.mytable\n.\nCommands to run on target account\n\u00b6\nQuery the\nREPLICATION_GROUP_REFRESH_PROGRESS\ntable function\n(in the\nSnowflake Information Schema\n). Note the\nprimarySnapshotTimestamp\nin the\nDETAILS\ncolumn for the\nPRIMARY_UPLOADING_METADATA\nphase. This is the timestamp for the latest refresh of that database on the primary account.\nSELECT\nPARSE_JSON\n(\ndetails\n)[\n'primarySnapshotTimestamp'\n]\nFROM\nTABLE\n(\ninformation_schema\n.\nreplication_group_refresh_progress\n(\n'myfg'\n))\nWHERE\nPHASE_NAME\n=\n'PRIMARY_UPLOADING_METADATA'\n;\nCopy\nQuery the HASH_AGG function for a specified table in the secondary account. The following query returns a hash value for all",
    "PRIMARY_UPLOADING_METADATA\nphase. This is the timestamp for the latest refresh of that database on the primary account.\nSELECT\nPARSE_JSON\n(\ndetails\n)[\n'primarySnapshotTimestamp'\n]\nFROM\nTABLE\n(\ninformation_schema\n.\nreplication_group_refresh_progress\n(\n'myfg'\n))\nWHERE\nPHASE_NAME\n=\n'PRIMARY_UPLOADING_METADATA'\n;\nCopy\nQuery the HASH_AGG function for a specified table in the secondary account. The following query returns a hash value for all rows\nin the\nmyschema.mytable\ntable:\nSELECT\nHASH_AGG\n(\n*\n)\nFROM\nmydb\n.\nmyschema\n.\nmytable\n;\nCopy\nCommands to run on source account\n\u00b6\nQuery the HASH_AGG function for the same table in the primary account. Using Time Travel, specify the timestamp when the latest\nrefresh was performed for the secondary database:\nSELECT\nHASH_AGG\n(\n*\n)\nFROM\nmydb\n.\nmyschema\n.\nmytable\nAT\n(\nTIMESTAMP\n=>\n'<primarySnapshotTimestamp>'\n::TIMESTAMP\n);\nCopy\nCompare the results from the two queries. The output should be identical.\nModifying a replication or failover group in a source account\n\u00b6\nYou can edit the name, included objects, and replication schedule of a replication or failover group in a source\naccount using Snowsight or\nSQL\n.\nNote\nReplication groups can\u2019t be changed to failover groups or vice versa. To enable or disable failover, delete\nthe group and recreate it with the correct failover setting.\nModify a replication or failover group in a source account using Snowsight\n\u00b6\nNote\nOnly account administrators can edit a replication or failover group using Snowsight (refer to\nLimitations of using Snowsight for replication configuration\n).\nTo perform these actions, you must be signed in to the source account. If you are not signed in, the\nStatus\ncolumn\ndisplays a sign in message instead of the refresh status.\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\n, then select\nGroups\n.\nLocate the replication or failover group you want to edit, and select the\nMore\nmenu (\n\u2026\n) in the last column of the row.\nSelect\nEdit\n.\nTo change the group name, enter a new name in the\nGroup name\n",
    "\ndisplays a sign in message instead of the refresh status.\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\n, then select\nGroups\n.\nLocate the replication or failover group you want to edit, and select the\nMore\nmenu (\n\u2026\n) in the last column of the row.\nSelect\nEdit\n.\nTo change the group name, enter a new name in the\nGroup name\nbox that meets the following requirements:\nMust start with an alphabetic character and cannot contain spaces or special characters unless the identifier string is\nenclosed in double quotes (for example, \u201cMy object\u201d). Identifiers enclosed in double quotes are also case-sensitive.\nFor more information, see\nIdentifier requirements\n.\nNames for failover groups and replication groups in an account must be unique.\nChoose\nEdit objects\nto add or remove share and account objects.\nNote\nAccount objects can only be added to one replication or failover group. If a replication or failover group with any account\nobjects already exists in your account, you can\u2019t select those objects.\nChoose\nSelect databases\nto add or remove database objects.\nSelect the\nReplication frequency\nto change the replication schedule for a group.\nSelect\nSave\nto update the group.\nIf saving the changes to the group is unsuccessful, refer to\nTroubleshoot issues with creating and editing replication groups using Snowsight\nfor common errors\nand how to resolve them.\nModify a replication or failover group in a source account using SQL\n\u00b6\nYou can modify a replication or failover group properties using the\nALTER REPLICATION GROUP\nor\nALTER FAILOVER GROUP\ncommand.\nPause or resume a replication schedule in a target account\n\u00b6\nYou can pause (suspend) or resume a replication schedule in a target account using Snowsight or\nSQL\n.\nPause or resume a replication schedule in a target account using Snowsight\n\u00b6\nNote\nOnly account administrators can edit a replication or failover group using Snowsight (refer to\nLimitations of using Snowsight for replication configuration\n).\nTo pause or resume a replication schedule, you must be signed in to the target account.\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\n, then select\nGroups\n.\nLocate the replication or failover group you",
    "sight\n\u00b6\nNote\nOnly account administrators can edit a replication or failover group using Snowsight (refer to\nLimitations of using Snowsight for replication configuration\n).\nTo pause or resume a replication schedule, you must be signed in to the target account.\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\n, then select\nGroups\n.\nLocate the replication or failover group you want to edit, and select the\nMore\nmenu (\n\u2026\n) in the last column of the row.\nSelect\nPause\nor\nResume\n.\nPause or resume a replication schedule in a target account using SQL\n\u00b6\nYou can pause or resume a replication schedule in a target account using the\nALTER REPLICATION GROUP\nor\nALTER FAILOVER GROUP\ncommand. To pause, specify the\nSUSPEND\nparameter. To resume, specify the\nRESUME\nparameter.\nDropping a secondary replication or failover group\n\u00b6\nYou can drop a secondary replication or failover using the\nDROP REPLICATION GROUP\nor the\nDROP FAILOVER GROUP\ncommand. Only the replication or failover group owner (that is, the role with the OWNERSHIP\nprivilege on the group) can drop the group.\nTo drop a secondary replication or failover group using Snowsight, you must drop the group in the source account. See\nDrop a replication or failover group using Snowsight\n.\nDropping a primary replication or failover group\n\u00b6\nYou can drop a primary replication or failover group using Snowsight or SQL. If you are deleting a primary group using SQL,\nyou must first drop all secondary groups. See\nDropping a secondary replication or failover group\n.\nDrop a primary replication or failover group using SQL\n\u00b6\nA primary replication or failover group can only be dropped after all the replicas of the group (that is, secondary replication or failover\ngroups) have been dropped. Alternatively, you can promote a secondary failover group to serve as the primary failover group,\nthen drop the former primary failover group.\nNote that only the group owner can drop the group.\nDrop a replication or failover group using Snowsight\n\u00b6\nNote\nOnly account administrators can delete a replication or failover group using Snowsight (refer to\nLimitations of using Snowsight for",
    " the group (that is, secondary replication or failover\ngroups) have been dropped. Alternatively, you can promote a secondary failover group to serve as the primary failover group,\nthen drop the former primary failover group.\nNote that only the group owner can drop the group.\nDrop a replication or failover group using Snowsight\n\u00b6\nNote\nOnly account administrators can delete a replication or failover group using Snowsight (refer to\nLimitations of using Snowsight for replication configuration\n).\nYou can delete a primary replication or failover group and any linked secondary groups.\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\n, select\nGroups\n.\nLocate the replication or failover group you want to delete. Select the\nMore\nmenu (\n\u2026\n) in the last column of the row.\nSelect\nDrop\n, then select\nDrop group\n.\nTroubleshoot issues with creating and editing replication groups using Snowsight\n\u00b6\nThe following scenarios can help you troubleshoot common issues that can occur when creating or editing replication or\nfailover group using Snowsight.\nYou cannot add a database to a group\nYou cannot add a share to a group\nYou cannot add a database to a group\n\u00b6\nError\nDatabase '<database_name>' is already configured to replicate to\naccount '<account_name>' by replication group '<group_name>'.\nCause\nA database can only be in one replication or failover group. One of the databases you selected for the group is already\nincluded in another replication or failover group.\nSolution\nChoose\nSelect Databases\nand unselect any database(s) that are already included in another group.\nError\nCannot directly add previously replicated object '<database_name>' to a\nreplication group. Please use the provided system functions to convert\nthis object first.\nCause\nThe database you want to add to a replication or failover group was previously configured for database replication.\nSolution\nDisable database replication for the database. See\nTransitioning from database replication to group-based replication\n.\nYou cannot add a share to a group\n\u00b6\nError\nShare '<share_name>' is already configured to replicate to\naccount '<account_name>' by replication group '<group_name>'.\nCause\nA share can only be in one replication or failover group. One of the shares you selected for the group is already included\nin another replication or",
    "over group was previously configured for database replication.\nSolution\nDisable database replication for the database. See\nTransitioning from database replication to group-based replication\n.\nYou cannot add a share to a group\n\u00b6\nError\nShare '<share_name>' is already configured to replicate to\naccount '<account_name>' by replication group '<group_name>'.\nCause\nA share can only be in one replication or failover group. One of the shares you selected for the group is already included\nin another replication or failover group.\nSolution\nChoose\nSelect Objects\nand unselect any share(s) that are already included in another group.\nLimitations of using Snowsight for replication configuration\n\u00b6\nOnly a user with the ACCOUNTADMIN role can create a replication or failover group using Snowsight. A user with a role with the\nCREATE REPLICATION GROUP or CREATE FAILOVER GROUP privilege can create a group using the respective SQL commands.\nOnly a user with the ACCOUNTADMIN role can edit or drop a replication or failover group using Snowsight. A user with a role\nwith the OWNERSHIP privilege on a replication or failover group can edit and drop groups using the respective SQL commands.\nIf your account uses private connectivity, you can\u2019t use Snowsight to create, modify, or drop groups. You can use SQL\nto complete these actions.\nOn this page\nRegion support for replication and failover/failback\nTransitioning from database replication to group-based replication\nWorkflow\nReplicating account objects and databases\nSchema-level replication for failover groups\nApply global IDs to objects created by scripts in target accounts\nConfigure cloud storage access for secondary storage integrations\nConfigure automated refresh for directory tables on secondary stages\nConfigure notifications for secondary auto-ingest pipes\nUpdating the remote service for API integrations\nComparing data sets in primary and secondary databases\nModifying a replication or failover group in a source account\nPause or resume a replication schedule in a target account\nDropping a secondary replication or failover group\nDropping a primary replication or failover group\nTroubleshoot issues with creating and editing replication groups using Snowsight\nLimitations of using Snowsight for replication configuration\nRelated content\nIntroduction to replication and failover across multiple accounts\nReplication considerations",
    " target account\nDropping a secondary replication or failover group\nDropping a primary replication or failover group\nTroubleshoot issues with creating and editing replication groups using Snowsight\nLimitations of using Snowsight for replication configuration\nRelated content\nIntroduction to replication and failover across multiple accounts\nReplication considerations",
    "ROLE\nmy_replication_role\n;\nCopy\nExecute the refresh statement using a role with the REPLICATE privilege:\nUSE\nROLE\nmy_replication_role\n;\nALTER\nFAILOVER\nGROUP\nmyfg\nREFRESH\n;\nCopy\nCreate a role with the FAILOVER privilege on the failover group. This step is\noptional\n.\nExecute in the target account using a role with the OWNERSHIP privilege on the failover group:\nGRANT\nFAILOVER\nON\nFAILOVER\nGROUP\nmyfg\nTO\nROLE\nmy_failover_role\n;;\nCopy\nReplicating account objects and databases\n\u00b6\nThe instructions in this section explain how to prepare your accounts for replication, enable the replication of specific objects from the\nsource account to the target account, and synchronize the objects in the target account.\nImportant\nTarget accounts do not have Tri-Secret Secure or private connectivity to the Snowflake service, such as\nAWS PrivateLink\n, enabled by default. If you require Tri-Secret Secure or private\nconnectivity to the Snowflake service for compliance, security or other purposes, it is your responsibility to configure and enable\nthose features in the target account.\nPrerequisite: Enable replication for accounts in the organization\n\u00b6\nThe organization administrator must enable replication for the source and target accounts.\nTo enable replication for accounts, an\norganization administrator\nuses the\nSYSTEM$GLOBAL_ACCOUNT_SET_PARAMETER\nfunction to set the\nENABLE_ACCOUNT_DATABASE_REPLICATION\nparameter to\ntrue\n.\nAs an organization administrator\n, enable replication for each source and target account in your organization.\n-- View the list of the accounts in your organization\n-- Note the organization name and account name for each account for which you are enabling replication\nSHOW\nACCOUNTS\n;\n-- Enable replication by executing this statement for each source and target account in your organization\nSELECT\nSYSTEM$GLOBAL_ACCOUNT_SET_PARAMETER\n(\n'<organization_name>.<account_name>'\n,\n'ENABLE_ACCOUNT_DATABASE_REPLICATION'\n,\n'true'\n);\nCopy\nThough the SYSTEM$GLOBAL_ACCOUNT_SET_PARAMETER function supports the legacy\naccount locator\nidentifier,\nit causes unexpected results when an organization has multiple accounts that share the same locator (in different regions).\nStep 1: Create a role with the CREATE FAILOVER GROUP privilege in the source account \u2014\nOptional\n\u00b6\nCreate a role and grant it the CREATE FAILOVER GROUP privilege. This step is optional.",
    ",\n'ENABLE_ACCOUNT_DATABASE_REPLICATION'\n,\n'true'\n);\nCopy\nThough the SYSTEM$GLOBAL_ACCOUNT_SET_PARAMETER function supports the legacy\naccount locator\nidentifier,\nit causes unexpected results when an organization has multiple accounts that share the same locator (in different regions).\nStep 1: Create a role with the CREATE FAILOVER GROUP privilege in the source account \u2014\nOptional\n\u00b6\nCreate a role and grant it the CREATE FAILOVER GROUP privilege. This step is optional. If you have already created this role, skip to\nStep 3: Create a primary failover group in a source account\n.\nUSE\nROLE\nACCOUNTADMIN\n;\nCREATE\nROLE\nmyrole\n;\nGRANT\nCREATE\nFAILOVER\nGROUP\nON\nACCOUNT\nTO\nROLE\nmyrole\n;\nCopy\nStep 2: Identify accounts enabled for replication and group membership\n\u00b6\nBefore creating a primary failover group, identify the accounts enabled for replication and the existing\nfailover and replication groups.\nView all accounts enabled for replication\n\u00b6\nTo retrieve the list of accounts in your organization that are enabled for replication, use\nSHOW REPLICATION ACCOUNTS\n.\nExecute the following SQL statement using the ACCOUNTADMIN role:\nSHOW\nREPLICATION\nACCOUNTS\n;\nCopy\nReturns:\n+------------------+-------------------------------+--------------+-----------------+-----------------+-------------------+--------------+\n| snowflake_region | created_on                    | account_name | account_locator | comment         | organization_name | is_org_admin |\n+------------------+-------------------------------+--------------+-----------------+-----------------+-------------------+--------------+\n| AWS_US_WEST_2    | 2020-07-15 21:59:25.455 -0800 | myaccount1   | myacctlocator1  |                 | myorg             | true         |\n+------------------+-------------------------------+--------------+-----------------+-----------------+-------------------+--------------+\n| AWS_US_EAST_1    | 2020-07-23 14:12:23.573 -0800 | myaccount2   | myacctlocator2  |                 | myorg             | false        |\n+------------------+-------------------------------+--------------+-----------------+-----------------+-------------------+--------------+\n| AWS_US_EAST_2    | 2020-07-25 19:25:04.412 -0800 | myaccount3   | myacctlocator3 ",
    "_1    | 2020-07-23 14:12:23.573 -0800 | myaccount2   | myacctlocator2  |                 | myorg             | false        |\n+------------------+-------------------------------+--------------+-----------------+-----------------+-------------------+--------------+\n| AWS_US_EAST_2    | 2020-07-25 19:25:04.412 -0800 | myaccount3   | myacctlocator3  |                 | myorg             | false        |\n+------------------+-------------------------------+--------------+-----------------+-----------------+-------------------+--------------+\nSee the complete list of\nRegion IDs\n.\nView failover and replication group membership\n\u00b6\nAccount, database, and share objects have\nconstraints on group membership\n. Before creating new\ngroups or adding objects to existing groups, you can review the list of existing failover groups and the objects in each group.\nNote\nOnly an account administrator (user with the ACCOUNTADMIN role) or the group owner (role with the OWNERSHIP privilege on the group) can\nexecute the SQL statements in this section.\nView all failover groups linked to the current account, and the object types in each group:\nSHOW FAILOVER GROUPS\n;\nCopy\nView all the databases in failover group\nmyfg\n:\nSHOW DATABASES\nIN\nFAILOVER\nGROUP\nmyfg\n;\nCopy\nView all the shares in failover group\nmyfg\n:\nSHOW SHARES\nIN\nFAILOVER\nGROUP\nmyfg\n;\nCopy\nStep 3: Create a primary failover group in a source account\n\u00b6\nCreate a primary failover group and enable the replication and failover of specific objects from the current (source) account to one or more\ntarget accounts in the same organization.\nYou can create a replication or failover group using\nSnowsight\nor\nSQL\n.\nCreate a replication or failover group using Snowsight\nCreate a failover group using SQL\nNote\nIf you have databases to add to a replication or failover group that have been previously enabled for database replication\nusing\nALTER DATABASE\n, follow the\nTransitioning from database replication to group-based replication\ninstructions (in this\ntopic) before adding them to a group.\nCreate a replication or failover group using Snowsight\n\u00b6\nNote\nOnly account administrators can create a replication or fail",
    " replication or failover group using Snowsight\nCreate a failover group using SQL\nNote\nIf you have databases to add to a replication or failover group that have been previously enabled for database replication\nusing\nALTER DATABASE\n, follow the\nTransitioning from database replication to group-based replication\ninstructions (in this\ntopic) before adding them to a group.\nCreate a replication or failover group using Snowsight\n\u00b6\nNote\nOnly account administrators can create a replication or failover group using Snowsight (refer to\nLimitations of using Snowsight for replication configuration\n).\nYou must be signed in to the target account as a user with the ACCOUNTADMIN role. If you are not, you will be\nprompted to sign in.\nBoth the source account and the target account must use the same connection type (public internet). Otherwise, signing\nin to the target account fails.\nComplete the following steps to create a new replication or failover group:\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\n, then complete one of these actions on the\nGroups\ntab:\nFor Business Critical Edition (or higher) accounts, complete one of these actions:\nIf there are no replication groups or connections, select\nGet started\nto configure a replication group and\na connection. The\nSetup business continuity\nwizard appears.\nSelect\n+ Group\nto configure a replication group without configuring a connection. The\nCreate a group\nwizard appears.\nFor Standard Edition and Enterprise Edition accounts, complete one of these actions:\nIf there are no replication groups or connections, select\nGet started\nto configure a replication group.\nThe\nSetup replication\nwizard appears.\nIf one or more replication groups exist, select\n+ Group\nto configure a replication group. The\nCreate a group\nwizard appears.\nOn the\nSelect a target account\npage, select a target account and sign into it, then select\nNext\n.\nOn the\nCreate a group\npage, in the\nGroup name\nbox, enter a name for the group that meets the\nfollowing requirements:\nMust start with an alphabetic character and cannot contain spaces or special characters unless the identifier string is\nenclosed in double quotes (for example, \u201cMy object\u201d). Identifiers enclosed in double quotes are also case-sensitive.\nFor more information, see\nIdentifier requirements\n.\nMust be unique across failover and replication",
    " it, then select\nNext\n.\nOn the\nCreate a group\npage, in the\nGroup name\nbox, enter a name for the group that meets the\nfollowing requirements:\nMust start with an alphabetic character and cannot contain spaces or special characters unless the identifier string is\nenclosed in double quotes (for example, \u201cMy object\u201d). Identifiers enclosed in double quotes are also case-sensitive.\nFor more information, see\nIdentifier requirements\n.\nMust be unique across failover and replication groups in an account.\nChoose\nEdit objects\nto add share and account objects to your group.\nNote\nAccount objects can only be added to one replication or failover group. If a replication or failover group with any account\nobjects already exists in your account, you cannot select those objects.\nChoose\nSelect databases\nto add database objects to your group.\nSelect the\nReplication frequency\n.\nIf the account is Business Critical Edition or higher, a failover group is created by default. You can choose to create a replication group\ninstead. To create a replication group, select\nAdvanced options\n, then unselect\nEnable failover\n.\nComplete one of the following actions:\nFor Business Critical Edition (or higher) accounts, select\nNext\n.\nFor Standard Edition and Enterprise Edition accounts, select\nStart replication\nto create the replication group.\nFor Business Critical Edition (or higher) accounts, on the\nCreate connection\npage, enter a connection name in\nthe\nConnection name\nbox, then select\nStart replication\n.\nIf creating the replication group is unsuccessful, refer to\nTroubleshoot issues with creating and editing replication groups using Snowsight\nfor common errors\nand how to resolve them.\nCreate a failover group using SQL\n\u00b6\nCreate a failover group of specified account and database objects in the source account and enable replication and failover to a list of\ntarget accounts. See\nCREATE FAILOVER GROUP\nfor syntax.\nFor example, enable replication of users, roles, warehouses, resources monitors, and databases\ndb1\nand\ndb2\nfrom the source account\nto the\nmyaccount2\naccount in the same organization. Set the replication schedule to automatically refresh\nmyaccount2\nevery 10\nminutes.\nExecute the following statement on the source account:\nUSE\nROLE\nmyrole\n;\nCREATE\nFAILOVER\nGROUP\nmyfg\nOBJECT_TYPES\n=\nUSERS\n,\nROLES\n",
    ", enable replication of users, roles, warehouses, resources monitors, and databases\ndb1\nand\ndb2\nfrom the source account\nto the\nmyaccount2\naccount in the same organization. Set the replication schedule to automatically refresh\nmyaccount2\nevery 10\nminutes.\nExecute the following statement on the source account:\nUSE\nROLE\nmyrole\n;\nCREATE\nFAILOVER\nGROUP\nmyfg\nOBJECT_TYPES\n=\nUSERS\n,\nROLES\n,\nWAREHOUSES\n,\nRESOURCE MONITORS\n,\nDATABASES\n,\nINTEGRATIONS\n,\nNETWORK POLICIES\nALLOWED_DATABASES\n=\ndb1\n,\ndb2\nALLOWED_INTEGRATION_TYPES\n=\nAPI INTEGRATIONS\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmyaccount2\nREPLICATION_SCHEDULE\n=\n'10 MINUTE'\n;\nCopy\nStep 4: Create a role with the CREATE FAILOVER GROUP privilege in the target account \u2014\nOptional\n\u00b6\nCreate a role in the target account and grant it the CREATE FAILOVER GROUP privilege. This step is optional. If you have already created\nthis role, skip to\nStep 5: Create a secondary failover group in the target account\n.\nUSE\nROLE\nACCOUNTADMIN\n;\nCREATE\nROLE\nmyrole\n;\nGRANT\nCREATE\nFAILOVER\nGROUP\nON\nACCOUNT\nTO\nROLE\nmyrole\n;\nCopy\nStep 5: Create a secondary failover group in the target account\n\u00b6\nNote\nIf account objects (for example, users or roles) exist in the target account that do not exist in the source account, refer to\nInitial replication of users and roles\nbefore creating a secondary group.\nCreate a secondary failover group in the target account as a replica of the primary failover group in the source account.\nExecute a\nCREATE FAILOVER GROUP \u2026 AS REPLICA OF\nstatement in each target account for which you\nenabled replication in\nStep 3: Create a primary failover group in a source account\n(in this topic).\nExecuted from each target account:\nUSE\nROLE\nmyrole\n;\nCREATE\nFAILOVER\nGROUP\nmyfg\nAS\nREPLICA\nOF\nmyorg\n.\nmyaccount1\n.\nmyfg\n;\nCopy\nStep 6. Refresh a secondary failover group in the target account manually \u2014\nOptional\n\u00b6",
    " each target account for which you\nenabled replication in\nStep 3: Create a primary failover group in a source account\n(in this topic).\nExecuted from each target account:\nUSE\nROLE\nmyrole\n;\nCREATE\nFAILOVER\nGROUP\nmyfg\nAS\nREPLICA\nOF\nmyorg\n.\nmyaccount1\n.\nmyfg\n;\nCopy\nStep 6. Refresh a secondary failover group in the target account manually \u2014\nOptional\n\u00b6\nTo manually refresh the objects in a target account, execute the\nALTER FAILOVER GROUP \u2026 REFRESH\ncommand.\nAs a best practice, we recommend scheduling your secondary refreshes by setting the REPLICATION_SCHEDULE parameter using\nCREATE FAILOVER GROUP\nor\nALTER FAILOVER GROUP\n.\nNote\nIf the user who calls the function in the target account was dropped in the source account, the refresh operation fails.\nGrant the REPLICATE privilege on failover group to role \u2014\nOptional\n\u00b6\nTo execute the command to refresh a secondary replication or failover group in the target account, you must use a role with the\nREPLICATE privilege on the failover group. The REPLICATE privilege is currently\nnot\nreplicated and must be granted on a\nfailover (or replication) group in both the source and target accounts.\nExecute this statement from the source account using a role with the OWNERSHIP privilege on the group:\nGRANT\nREPLICATE\nON\nFAILOVER\nGROUP\nmyfg\nTO\nROLE\nmy_replication_role\n;\nCopy\nExecute this statement from the target account using a role with the OWNERSHIP privilege on the group:\nGRANT\nREPLICATE\nON\nFAILOVER\nGROUP\nmyfg\nTO\nROLE\nmy_replication_role\n;\nCopy\nManually refresh a secondary failover group\n\u00b6\nFor example, to refresh the objects in the failover group\nmyfg\n, execute the following statement from the target account:\nUSE\nROLE\nmy_replication_role\n;\nALTER\nFAILOVER\nGROUP\nmyfg\nREFRESH\n;\nCopy\nStep 7. Grant the FAILOVER privilege on failover group to role \u2014\nOptional\n\u00b6\nTo execute the command to fail over a secondary failover group in a target account, you must use a role with the\nFAILOVER privilege\non the failover group. The",
    "Replication considerations\n\u00b6\nStandard & Business Critical Feature\nDatabase and share replication are available to all accounts.\nReplication of other account objects & failover/failback require Business Critical Edition (or higher).\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nThis topic describes the behavior of certain Snowflake features in secondary databases and objects when replicated with\nreplication or failover groups\nor\ndatabase replication\n, and provides general guidance for working with replicated\nobjects and data.\nIf you have previously enabled database replication for individual databases using the ALTER DATABASE \u2026 ENABLE REPLICATION TO ACCOUNTS\ncommand, see\nDatabase replication considerations\nfor additional considerations specific to database replication.\nReplication group and failover group constraints\n\u00b6\nThe following sections explain the constraints around adding account objects, databases, and shares to replication and failover groups.\nDatabase and share objects\n\u00b6\nThe following constraints apply to database and share objects:\nAn object can only be in one failover group.\nAn object can be in multiple replication groups as long as each group is replicated to a\ndifferent\ntarget account.\nAn object cannot be in both a failover group and a replication group.\nSecondary (replica) objects cannot be added to a primary replication or failover group.\nYou can only replicate outbound shares. Replication of\ninbound shares\n(shares from providers)\nis\nnot\nsupported.\nAccount objects\n\u00b6\nAn account can only have one replication or failover group that contains objects\nother than\ndatabases or shares.\nReplication privileges\n\u00b6\nThis section describes the replication privileges that are available to be granted to roles to specify the operations users can perform on\nreplication and failover group objects in the system. For the syntax of the GRANT command, see\nGRANT <privileges> \u2026 TO ROLE\n.\nNote\nFor\ndatabase replication\n, only a user with the ACCOUNTADMIN role can enable\nand manage database replication and failover. For additional information on required privileges for database replication,\nsee the\nrequired privileges table\nin\nStep 6. Refreshing a secondary database on a schedule\n.\nPrivilege\nObject\nUsage\nNotes\nOWNERSHIP\nReplication Group\nFailover Group\nGrants the ability to delete, alter, and grant or revoke access to an object.\nCan be granted by:\nThe ACCOUNTADMIN role\nor\nA role that has the MANAGE GRANTS privilege\nor\nA role that has the",
    " information on required privileges for database replication,\nsee the\nrequired privileges table\nin\nStep 6. Refreshing a secondary database on a schedule\n.\nPrivilege\nObject\nUsage\nNotes\nOWNERSHIP\nReplication Group\nFailover Group\nGrants the ability to delete, alter, and grant or revoke access to an object.\nCan be granted by:\nThe ACCOUNTADMIN role\nor\nA role that has the MANAGE GRANTS privilege\nor\nA role that has the OWNERSHIP privilege on the group.\nCREATE REPLICATION GROUP\nAccount\nGrants the ability to create a replication group.\nMust be granted by the ACCOUNTADMIN role.\nCREATE FAILOVER GROUP\nAccount\nGrants the ability to create a failover group.\nMust be granted by the ACCOUNTADMIN role.\nFAILOVER\nFailover Group\nGrants the ability to promote a secondary failover group to serve as primary failover group.\nCan be granted or revoked by a role with the OWNERSHIP privilege on the group.\nREPLICATE\nReplication Group\nFailover Group\nGrants the ability to refresh a secondary group.\nCan be granted or revoked by a role with the OWNERSHIP privilege on the group.\nMODIFY\nReplication Group\nFailover Group\nGrants the ability to change the settings or properties of an object.\nCan be granted or revoked by a role with the OWNERSHIP privilege on the group.\nMONITOR\nReplication Group\nFailover Group\nGrants the ability to view details within an object.\nCan be granted or revoked by a role with the OWNERSHIP privilege on the group.\nFor instructions on creating a custom role with a specified set of privileges, see\nCreating custom roles\n.\nFor general information about roles and privilege grants for performing SQL actions on\nsecurable objects\n, see\nOverview of Access Control\n.\nReplication and references across replication groups\n\u00b6\nObjects in a replication (or failover) group that have dangling references (i.e. references to objects in another replication or failover\ngroup) might successfully replicate to a target account in some circumstances. If the replication operation results in behavior in the\ntarget account consistent with behavior that can occur in the source account, replication succeeds.\nFor example, if a column in a table in failover group\nfg_a\nreferences a sequence in failover group\nfg_b\n, replication of both\ngroups succeeds. If\nfg_a\nis replicated",
    ".\nIf a secondary database contains clustered tables and the database is promoted to become the primary database, Snowflake begins Automatic\nClustering of the tables in this database while simultaneously suspending the monitoring of clustered tables in the previous primary\ndatabase.\nSee\nReplication and Materialized Views\n(in this topic) for information about Automatic Clustering for materialized views.\nReplication and large, high-churn tables\n\u00b6\nWhen one or more rows of a table are updated or deleted, all of the impacted micro-partitions that store this data in a primary database\nare re-created and must be synchronized to secondary databases. For large, high-churn dimension tables, the replication costs can be\nsignificant.\nFor large, high-churn dimension tables that incur significant replication costs, the following mitigations are available:\nReplicate any primary databases that store such tables at a lower frequency.\nChange your data model to reduce churn.\nFor more information, see\nManaging costs for large, high-churn tables\n.\nReplication and Time Travel\n\u00b6\nTime Travel\nand\nFail-safe\ndata is maintained independently for a\nsecondary database and is not replicated from a primary database. Querying tables and views in a secondary database using Time Travel\ncan produce different results than when executing the same query in the primary database.\nHistorical Data\n:\nHistorical data available to query in a primary database using Time Travel is not replicated to secondary databases.\nFor example, suppose data is loaded continuously into a table every 10 minutes using Snowpipe, and a secondary database is refreshed\nevery hour. The refresh operation only replicates the latest version of the table. While every hourly version of the table within the\nretention window is available for query using Time Travel, none of the iterative versions within each hour (the individual Snowpipe\nloads) are available.\nData Retention Period\n:\nThe data retention period for tables in a secondary database begins when the secondary database is refreshed with the DML operations\n(i.e. changing or deleting data) written to tables in the primary database.\nNote\nThe data retention period parameter,\nDATA_RETENTION_TIME_IN_DAYS\n, is only replicated to database objects in the secondary\ndatabase, not to the database itself. For more details about parameter replication, see\nParameters\n.\nReplication and materialized views\n\u00b6\nIn a primary database, Snowflake performs automatic background maintenance of materialized views. When a base table changes, all\nmaterialized views defined on the",
    ".e. changing or deleting data) written to tables in the primary database.\nNote\nThe data retention period parameter,\nDATA_RETENTION_TIME_IN_DAYS\n, is only replicated to database objects in the secondary\ndatabase, not to the database itself. For more details about parameter replication, see\nParameters\n.\nReplication and materialized views\n\u00b6\nIn a primary database, Snowflake performs automatic background maintenance of materialized views. When a base table changes, all\nmaterialized views defined on the table are updated by a background service that uses compute resources provided by Snowflake. In addition,\nif Automatic Clustering is enabled for a materialized view, then the view is monitored and reclustered as necessary in a primary database.\nA refresh operation replicates the materialized view\ndefinitions\nto a secondary database; the materialized view\ndata\nis not\nreplicated. Automatic background maintenance of materialized views in a secondary database is enabled by default. If Automatic\nClustering is enabled for a materialized view in a primary database, automatic monitoring and reclustering of the materialized view in the\nsecondary database is also enabled.\nNote\nThe charges for automated background synchronization of materialized views are billed to each account that contains a secondary\ndatabase.\nReplication and Apache Iceberg\u2122 tables\n\u00b6\nConsider the following points when you use replication for Iceberg tables:\nSnowflake currently supports replication of Snowflake-managed tables only.\nReplicating converted Iceberg tables isn\u2019t supported. Snowflake skips converted tables during refresh operations.\nFor replicated tables, you must configure access to a storage location in the\nsame region\nas the target account.\nIf you drop or alter a storage location that is used for replication on the primary external volume, refresh operations might fail.\nSecondary tables in the target account are read-only until you promote the target account to serve as the source account.\nSnowflake maintains the\ndirectory hierarchy\nof the primary Iceberg table for the secondary table.\nReplication costs apply for this feature. For more information, see\nUnderstanding replication cost\n.\nFor considerations about the account objects for replication and failover groups, see\nAccount objects\n.\nReplication and dynamic tables\n\u00b6\nDynamic table replication behavior varies based on whether the primary database containing the dynamic table is part\nof a replication group or a failover group.\nDynamic tables and replication groups\n\u00b6\nA database that contains a dynamic table can be replicated using a replication group. The source object(s)",
    " costs apply for this feature. For more information, see\nUnderstanding replication cost\n.\nFor considerations about the account objects for replication and failover groups, see\nAccount objects\n.\nReplication and dynamic tables\n\u00b6\nDynamic table replication behavior varies based on whether the primary database containing the dynamic table is part\nof a replication group or a failover group.\nDynamic tables and replication groups\n\u00b6\nA database that contains a dynamic table can be replicated using a replication group. The source object(s) it depends\non are not required to be in the same replication group.\nReplicated objects in each target account are referred to as\nsecondary\nobjects and are replicas of the\nprimary\nobjects in the source account. Secondary objects are\nread-only\nin\nthe target account. If a secondary replication group is dropped in a target account, the databases that were\nincluded in the group become read/write. However, any dynamic tables included in a replication group\nremain\nread-only even after the secondary group is dropped in the target account. No DML or dynamic table refreshes can\nhappen on these read-only dynamic tables.\nDynamic tables and failover groups\n\u00b6\nA database that contains a dynamic table can be replicated using a failover group. If a dynamic table references\nsource objects outside the failover group or database replication, it can still be replicated. After a failover, the\ndynamic table resolves source objects using name resolution during refresh. The refresh might succeed or fail,\ndepending on the state of the source objects. If successful, the dynamic table is reinitialized with the latest data\nfrom the source objects.\nSecondary dynamic tables are read-only and do not get refreshed. After a failover occurs and a secondary dynamic\ntable is promoted to primary dynamic table, the first refresh is a reinitialization followed by incremental\nrefreshes if the dynamic table is configured for incremental refresh of data.\nNote\nThe reinitialized dynamic table might differ from the original replica because the source objects and dynamic table\nare not guaranteed to share the same replication snapshot.\nExample: Refresh failure due to missing source objects\nIf a dynamic table depends on a source table outside the failover group, it cannot refresh after a failover. In the\nabove diagram, the dynamic table\ndt\nin the primary account is replicated to the secondary account.\ndt\ndepends on\nsource_table\n, which is not included in the same failover group as the primary account. After failover,\n",
    " source objects and dynamic table\nare not guaranteed to share the same replication snapshot.\nExample: Refresh failure due to missing source objects\nIf a dynamic table depends on a source table outside the failover group, it cannot refresh after a failover. In the\nabove diagram, the dynamic table\ndt\nin the primary account is replicated to the secondary account.\ndt\ndepends on\nsource_table\n, which is not included in the same failover group as the primary account. After failover,\nthe refresh in the secondary account fails because\nsource_table\ncannot be resolved.\nExample: Successful refresh when source objects exist in secondary account via separate replication\nIn the above diagram, the dynamic table\ndt\ndepends on\nsource_table\n. Both\ndt\nand\nsource_table\nin the\nprimary account are replicated to the secondary account through independent failover groups. After replication and\nfailover, when\ndt\nis refreshed in the secondary account, the refresh succeeds because\nsource_table\ncan be\nfound through name resolution.\nExample: Successful refresh when source objects exist in secondary account locally\nIn the above diagram, the dynamic table\ndt\ndepends on\nsource_table\nand is replicated through a failover group\nfrom the primary account to the secondary account. A\nsource_table\nis created locally in the secondary account.\nAfter failover, when\ndt1\nis refreshed in the secondary account, the refresh can succeed because\nsource_table\ncan be found through name resolution.\nReplication and Snowpipe Streaming\n\u00b6\nA table populated by\nSnowpipe Streaming\nin a primary database is replicated to the secondary database in a target account.\nIn the primary database, tables are created and rows are inserted through\nchannels\n.\nOffset tokens\ntrack the ingestion progress. A refresh operation replicates the table object, table data, and the channel offsets associated with the table from the primary database to the secondary database.\nSnowflake Streaming read-only operations to retrieve offsets are available in the source and target accounts:\nThe channel\ngetLatestCommittedOffsetToken\nAPI\nSHOW CHANNELS\ncommand\nSnowflake Streaming write operations are only available in the source account:\nThe client\nopenChannel\nAPI\nThe channel\ninsertRow\nAPI\nThe channel\ninsertRows\nAPI\nAvoiding data loss\n\u00b6\nTo avoid data loss in the case of failover, the data retention time for successfully inserted rows in your upstream data source must be\ngreater than the configured",
    " in the source and target accounts:\nThe channel\ngetLatestCommittedOffsetToken\nAPI\nSHOW CHANNELS\ncommand\nSnowflake Streaming write operations are only available in the source account:\nThe client\nopenChannel\nAPI\nThe channel\ninsertRow\nAPI\nThe channel\ninsertRows\nAPI\nAvoiding data loss\n\u00b6\nTo avoid data loss in the case of failover, the data retention time for successfully inserted rows in your upstream data source must be\ngreater than the configured\nreplication schedule\n. If data is inserted into a table in a primary database, and failover occurs before the data can\nbe replicated to the secondary database, the same data will need to be inserted into the table in the newly promoted primary database.\nThe following is an example scenario:\nTable\nt1\nin primary database\nrepl_db\nis populated with data with Snowpipe Streaming and the Kafka connector.\nThe\noffsetToken\nis 100 for channel 1 and 100 for channel 2 for\nt1\nin the\nprimary\ndatabase.\nA refresh operation completes successfully in the target account.\nThe\noffsetToken\nis 100 for channel 1 and 100 for channel 2 for the\nt1\nin the\nsecondary\ndatabase.\nMore rows are inserted into\nt1\nin the primary database.\nThe\noffsetToken\nis now 200 for channel 1 and 200 for channel 2 for the\nt1\nin the\nprimary\ndatabase.\nA failover occurs before the additional rows and new channel offsets can be replicated to the secondary database.\nIn this case, there are 100 missing offsets in each channel for table\nt1\nin the newly promoted primary database. To insert the missing data, see\nReopen active channels for Snowpipe Streaming in newly promoted source account\n.\nRequirements\n\u00b6\nSnowpipe Streaming replication support requires the following minimum versions:\nSnowflake Ingest SDK version 1.1.1 and later\nIf you are using the Kafka connector: Kafka connector version 1.9.3 and later\nThe data retention time for successfully inserted rows in your upstream data source must be\ngreater than the configured replication schedule. If you are using the Kafka connector, ensure your\nlog.retention\nconfiguration is set with a sufficient buffer.\nReplication and stages\n\u00b6\nThe following constraints apply to stage objects:\nSnowflake currently supports stage replication as part of group-based replication (replication",
    "1 and later\nIf you are using the Kafka connector: Kafka connector version 1.9.3 and later\nThe data retention time for successfully inserted rows in your upstream data source must be\ngreater than the configured replication schedule. If you are using the Kafka connector, ensure your\nlog.retention\nconfiguration is set with a sufficient buffer.\nReplication and stages\n\u00b6\nThe following constraints apply to stage objects:\nSnowflake currently supports stage replication as part of group-based replication (replication and failover groups).\nStage replication is not supported for database replication.\nYou can replicate an external stage. However, the files on an external stage are not replicated.\nYou can replicate an internal stage. To replicate the files on an internal stage, you must enable a directory table on the stage.\nSnowflake replicates only the files that are mapped by the directory table.\nWhen you replicate an internal stage with a directory table, you cannot disable the directory table on the primary or secondary stage.\nThe directory table contains critical information about replicated files and files loaded using a COPY statement.\nA refresh operation will fail if the directory table on an internal stage contains a file that is larger than 5GB. To work around this\nlimitation, move any files larger than 5GB to a different stage.\nYou cannot disable the directory table on a primary or secondary stage, or any stage that has previously been replicated. Follow\nthese steps\nbefore\nyou add the database that contains the stage to a replication or failover group.\nDisable the directory table\non the primary stage.\nMove the files that are larger than 5GB to another stage that does not have a directory table enabled.\nAfter you move the files to another stage, re-enable the directory table on the primary stage.\nFiles on user stages and table stages are not replicated.\nFor named external stages that use a storage integration, you must configure the trust relationship for secondary storage integrations\nin your target accounts prior to failover. For more information, see\nConfigure cloud storage access for secondary storage integrations\n.\nIf you replicate an external stage with a directory table, and you have configured\nautomated refresh\nfor the source\ndirectory table, you must configure automated refresh for the\nsecondary\ndirectory table before failover. For more information,\nsee\nConfigure automated refresh for directory tables on secondary stages\n.\nA copy command might take longer than expected if the directory table on a replicated stage is not consistent with the\nreplicated files",
    " information, see\nConfigure cloud storage access for secondary storage integrations\n.\nIf you replicate an external stage with a directory table, and you have configured\nautomated refresh\nfor the source\ndirectory table, you must configure automated refresh for the\nsecondary\ndirectory table before failover. For more information,\nsee\nConfigure automated refresh for directory tables on secondary stages\n.\nA copy command might take longer than expected if the directory table on a replicated stage is not consistent with the\nreplicated files on the stage. To make a directory table consistent, refresh it with an\nALTER STAGE \u2026 REFRESH\nstatement.\nTo check the consistency status of a directory table, use the\nSYSTEM$GET_DIRECTORY_TABLE_STATUS\nfunction.\nReplication and pipes\n\u00b6\nThe following constraints apply to pipe objects:\nSnowflake currently supports pipe replication as part of group-based replication (replication and failover groups).\nPipe replication is not supported for database replication.\nSnowflake replicates the copy history of a pipe only when the pipe belongs to the same replication group as its target table.\nReplication of notification integrations is not supported.\nSnowflake only replicates load history after the latest table truncate.\nTo receive notifications, you must configure a secondary auto-ingest pipe in a target account\nprior to failover\n.\nFor more information, see\nConfigure notifications for secondary auto-ingest pipes\n.\nUse the\nSYSTEM$PIPE_STATUS\nfunction to resolve any pipes not in their expected execution state after failover.\nSnowflake doesn\u2019t support replication and failover for Snowpipe with the Kafka connector, but Snowflake does support replication and failover for Snowpipe Streaming with the Kafka connector. For more information, see\nSnowpipe Streaming and the Kafka connector\n.\nReplication of data metric functions (DMFs)\n\u00b6\nThe following behaviors apply to\nDMF\nreplication:\nEvent tables\nThe event table that stores the results of manually calling or scheduling a DMF to run is not replicated because the event table is local\nto your Snowflake account, and Snowflake does not support replicating event tables.\nReplication groups\nWhen you add the database(s) that contain your DMFs to a replication group, the following occurs in the target account:\nDMFs are replicated from the source account.\nTables or views that the\nDMF definition\nspecifies, such as with a\nforeign key reference\nare replicated from the source account, unless the table\nor view is associated with\n",
    " the event table is local\nto your Snowflake account, and Snowflake does not support replicating event tables.\nReplication groups\nWhen you add the database(s) that contain your DMFs to a replication group, the following occurs in the target account:\nDMFs are replicated from the source account.\nTables or views that the\nDMF definition\nspecifies, such as with a\nforeign key reference\nare replicated from the source account, unless the table\nor view is associated with\nCross-Cloud Auto-Fulfillment\n.\nScheduled DMFs in the target account are suspended. The secondary DMFs resume their schedule when you promote the target account to\nsource account and the secondary DMFs become primary DMFs.\nFailover groups\nWhen you replicate the database(s) that contain your DMFs using a failover group, the following occurs in the case of failover:\nResumes the schedule of suspended DMFs when you promote the target account to source account.\nSuspends scheduled DMFs in the target account after you promote a different account to source account.\nIf you do not replicate the database that contains the DMF to a target account, the DMF associations to a table or view are\ndropped when the target account is promoted to source account because they are not available in the\nnewly promoted source account.\nTip\nPrior to failing over your account,\ncheck the DMF references\nby calling the\nDATA_METRIC_FUNCTION_REFERENCES Information Schema table function to determine the table objects that are associated with a DMF\nbefore the promotion and refresh operations.\nReplication of stored procedures and user-defined functions (UDFs)\n\u00b6\nStored procedures and UDFs are replicated from a primary database to secondary databases.\nStored Procedures and UDFs and Stages\n\u00b6\nIf a stored procedure or UDF depends on files in a stage (for example, if the stored\nprocedure is defined in Python code that is uploaded from a stage), you must replicate the stage and its files to the secondary\ndatabase. For more information about replicating stages, see\nStage, pipe, and load history replication\n.\nFor example, if a primary database has an in-line Python UDF that imports any code that is stored on a stage, the UDF does not work unless\nthe stage and its imported code are replicated in the secondary database.\nStored Procedures and UDFs and External Network Access\n\u00b6\nIf a stored procedure or UDF depends on access to an\n",
    " its files to the secondary\ndatabase. For more information about replicating stages, see\nStage, pipe, and load history replication\n.\nFor example, if a primary database has an in-line Python UDF that imports any code that is stored on a stage, the UDF does not work unless\nthe stage and its imported code are replicated in the secondary database.\nStored Procedures and UDFs and External Network Access\n\u00b6\nIf a stored procedure or UDF depends on access to an\nexternal network location\n, you must\nreplicate the following objects:\nEXTERNAL ACCESS INTEGRATIONS must be included in the\nallowed_integration_types\nlist for the replication or\nfailover group.\nThe database that contains the network rule.\nThe database that contains the secret that stores the credentials to authenticate with the external network location.\nIf the secret object references a security integration, you must include SECURITY INTEGRATIONS in the\nallowed_integration_types\nlist for the replication or failover group.\nReplication and storage lifecycle policies\n\u00b6\nSnowflake replicates\nstorage lifecycle policies\nand their associations with tables to target accounts, but doesn\u2019t run the policies.\nSnowflake doesn\u2019t replicate archived data in the COOL or COLD tiers.\nArchived data in your source account isn\u2019t available in the target account.\nAfter failover to a target account, Snowflake pauses storage lifecycle policy execution in the\noriginal source account. After\nfailback\nto the source account, Snowflake resumes\npolicy execution.\nSnowflake never automatically runs secondary storage lifecycle policies on secondary tables,\neven after failover. However, you can use secondary policies in a target account by attaching\nthem to new tables. For those new tables, Snowflake runs the policies.\nReplication and streams\n\u00b6\nThis section describes recommended practices and potential areas of concern when replicating streams in\nReplicating databases across multiple accounts\nor\nAccount Replication and Failover/Failback\n.\nSupported Source Objects for Streams\n\u00b6\nReplicated streams can successfully track the change data for tables and views in the same database.\nCurrently, the following source object types are\nnot\nsupported:\nExternal tables\nTables or views in databases separate from the stream databases, unless\nboth\nthe stream database and the database that stores the source object are included in the same\nreplication or failover group\n.\nTables or views in a shared databases (i.e. databases shared from provider accounts to your account)\nReplicating streams on",
    "\nReplicated streams can successfully track the change data for tables and views in the same database.\nCurrently, the following source object types are\nnot\nsupported:\nExternal tables\nTables or views in databases separate from the stream databases, unless\nboth\nthe stream database and the database that stores the source object are included in the same\nreplication or failover group\n.\nTables or views in a shared databases (i.e. databases shared from provider accounts to your account)\nReplicating streams on directory tables is supported when you enable\nStage, pipe, and load history replication\n.\nA database replication or refresh operation fails if the primary database includes a stream with an unsupported source object. The operation also fails if the source object for any stream has been dropped.\nAppend-only streams are not supported on replicated source objects.\nAvoiding Data Duplication\n\u00b6\nNote\nIn addition to the scenario described in this section, streams in a secondary database could return duplicate rows the first time they are included in a refresh operation. In this case,\nduplicate rows\nrefers to a single row with multiple METADATA$ACTION column values.\nAfter the initial refresh operation, you should not encounter this specific issue in a secondary database.\nData duplication occurs when DML operations write the same change data from a stream multiple times without a uniqueness check. This can occur if a stream and a destination table for the stream change data are stored in separate databases, and these databases are not replicated and failed over in the same group.\nFor example, suppose you regularly insert change data from stream\ns\ninto table\ndt\n. (For this example, the source object for the stream does not matter.) Separate databases store the stream and destination table.\nAt timestamp\nt1\n, a row is inserted into the source table for stream\ns\n, creating a new table version. The stream stores the offset for this table version.\nAt timestamp\nt2\n, the secondary database that stores the stream is refreshed. Replicated stream\ns\nnow stores the offset.\nAt timestamp\nt3\n, the change data for stream\ns\nis inserted into table\ndt\n.\nAt timestamp\nt4\n, the secondary database that stores stream\ns\nis failed over.\nAt timestamp\nt5\n, the change data for stream\ns\nis inserted again into table\ndt\n.\nTo avoid this situation, replicate and fail over together the databases that store streams and their destination tables.\nStream References in",
    " that have dangling references (i.e. references to objects in another replication or failover\ngroup) might successfully replicate to a target account in some circumstances. If the replication operation results in behavior in the\ntarget account consistent with behavior that can occur in the source account, replication succeeds.\nFor example, if a column in a table in failover group\nfg_a\nreferences a sequence in failover group\nfg_b\n, replication of both\ngroups succeeds. If\nfg_a\nis replicated\nbefore\nfg_b\n, insert operations (after failover) on the table that references the\nsequence\nfails\nif\nfg_b\nwas not replicated. This behavior can occur in a source account. If a sequence is dropped in a\nsource account, insert operations on a table with a column referencing the dropped sequence fails.\nWhen the dangling reference is a security policy that protects data, the replication (or failover) group with the security policy\nmust\nbe replicated before any replication group that contains objects that reference the policy is replicated.\nAttention\nMaking updates to security policies that protect data in separate replication or failover groups may result in inconsistencies\nand should be done with care.\nFor database objects, you can view\nobject dependencies\nin the Account Usage\nOBJECT_DEPENDENCIES view\n.\nSession policies and secondary roles\n\u00b6\nFor details, see\nSession policies with secondary roles\nDangling references and network policies\n\u00b6\nDangling references in network policies can cause replication to fail with the following error message:\nDangling references in the snapshot. Correct the errors before refreshing again.\nThe following references are missing (referred entity <- [referring entities])\nTo avoid dangling references, specify the following object types in the\nOBJECT_TYPES\nlist when executing the CREATE or\nALTER command for the replication or failover group:\nIf a network policy uses a network rule, include the database that contains the schema where the network rule was created.\nIf a network policy is associated with the account, include\nNETWORK\nPOLICIES\nand\nACCOUNT\nPARAMETERS\nin the\nOBJECT_TYPES\nlist.\nIf a network policy is associated with a user, include\nNETWORK\nPOLICIES\nand\nUSERS\nin the\nOBJECT_TYPES\nlist.\nFor more details, see\nReplicating network policies\n.\nDangling references and packages policies\n\u00b6\nIf there is a\npackages policy\nset on the account, the following dangling references\nerror occurs during the",
    "licated stream\ns\nnow stores the offset.\nAt timestamp\nt3\n, the change data for stream\ns\nis inserted into table\ndt\n.\nAt timestamp\nt4\n, the secondary database that stores stream\ns\nis failed over.\nAt timestamp\nt5\n, the change data for stream\ns\nis inserted again into table\ndt\n.\nTo avoid this situation, replicate and fail over together the databases that store streams and their destination tables.\nStream References in Task WHEN Clause\n\u00b6\nTo avoid unexpected behavior when running replicated tasks that reference streams in the\nWHEN\nboolean_expr\nclause, we recommend that you either:\nCreate the tasks and streams in the same database,\nor\nIf streams are stored in a different database from the tasks that reference them, include both databases in the same\nfailover group\n.\nIf a task references a stream in a separate database, and both databases are not included in the same failover group, then the database that contains the task could be failed over without the database that contains the stream. In this scenario, when the task is resumed in the failed over database, it records an error when it attempts to run and cannot find the referenced stream. This issue can be resolved by either failing over the database that contains the stream or recreating the database and stream in the same account as the failed over database that contains the task.\nStream Staleness\n\u00b6\nIf a stream in the primary database has become\nstale\n, the replicated stream in a secondary database is also stale and cannot be queried or its change data consumed. To resolve this issue, recreate the stream in the primary database (using\nCREATE OR REPLACE STREAM\n). When the secondary database is refreshed, the replicated stream is readable again.\nNote that the offset for a recreated stream is the current table version by default. You can recreate a stream that points to an earlier table version using Time Travel; however, the replicated stream would remain unreadable. For more information, see\nStream Replication and Time Travel\n(in this topic).\nStream Replication and Time Travel\n\u00b6\nAfter a primary database is failed over, if a stream in the database uses\nTime Travel\nto read a\ntable version\nfor the source object from a point in time before the last refresh timestamp, the replicated stream cannot be queried or the change data consumed. Likewise, querying the change data for a source object from a point in time before the last refresh timestamp",
    ". For more information, see\nStream Replication and Time Travel\n(in this topic).\nStream Replication and Time Travel\n\u00b6\nAfter a primary database is failed over, if a stream in the database uses\nTime Travel\nto read a\ntable version\nfor the source object from a point in time before the last refresh timestamp, the replicated stream cannot be queried or the change data consumed. Likewise, querying the change data for a source object from a point in time before the last refresh timestamp using the\nCHANGES\nclause for\nSELECT\nstatements fails with an error.\nThis is because a refresh operation collapses the table history into a single table version. Iterative table versions created before the refresh operation timestamp are not preserved in the table history for the replicated source objects.\nConsider the following example:\nTable\nt1\nis created in the primary database with change tracking enabled (table version\ntv0\n). Subsequent DML transactions create table versions\ntv1\nand\ntv2\n.\nA secondary database that contains table\nt1\nis refreshed. The table version for this replicated table is\ntv2\n; however, the table history is not replicated.\nA stream is created in the primary database with its offset set to table version\ntv1\nusing Time Travel.\nThe secondary database is failed over, becoming the primary database.\nQuerying stream\ns1\nreturns an error, because table version\ntv1\nis not in the table history.\nNote that when a subsequent DML transaction on table\nt1\niterates the table version to\ntv3\n, the offset for stream\ns1\nis advanced. The stream is readable again.\nAvoiding Data Loss\n\u00b6\nData loss can occur when the most recent refresh operation for a secondary database is not completed prior to the failover operation. We recommend refreshing your secondary databases frequently to minimize the risk.\nReplication and tasks\n\u00b6\nThis section describes task replication in\nReplicating databases across multiple accounts\nor\nAccount Replication and Failover/Failback\n.\nNote\nDatabase replication does not work for task graphs if the graph is owned by a different role than the role that performs replication.\nReplication Scenarios\n\u00b6\nThe following table describes different task scenarios and specifies whether the tasks are replicated or not. Except where noted, the scenarios pertain to both standalone tasks and tasks in a\ntask graph\n:\nScenario\nReplicated\nNotes\nTask was created and either",
    " across multiple accounts\nor\nAccount Replication and Failover/Failback\n.\nNote\nDatabase replication does not work for task graphs if the graph is owned by a different role than the role that performs replication.\nReplication Scenarios\n\u00b6\nThe following table describes different task scenarios and specifies whether the tasks are replicated or not. Except where noted, the scenarios pertain to both standalone tasks and tasks in a\ntask graph\n:\nScenario\nReplicated\nNotes\nTask was created and either resumed or executed manually (using\nEXECUTE TASK\n). Resuming or executing a task creates an initial task version.\n\u2714\nTask was created but never resumed or executed.\n\u274c\nTask was recreated (using\nCREATE OR REPLACE TASK\nbut never resumed or executed).\n\u2714\nThe latest version\nbefore\nthe task was recreated is replicated.\nResuming or manually executing the task commits a new version. When the database is replicated again, the new, or latest, version is replicated to the secondary database.\nTask was created and resumed or executed, but subsequently dropped.\n\u274c\nTask graph was created and resumed or executed. Subsequently, a task in the task graph was modified, but the task graph\u2019s root task wasn\u2019t resumed or executed again. Examples of modifications include the following:\nUsing\nALTER TASK \u2026 SET/UNSET/MODIFY\non a root task, child task, or finalizer task.\nUsing\nALTER TASK \u2026 SUSPEND\non a child task or finalizer task.\n\u2714\nThe latest version of the task graph\nbefore\nthe task was modified is replicated.\nResuming or manually executing a task commits a new version that includes any changes to the parameters of the tasks within the task graph. Because the new changes were never committed, only the previous version of the task graph is replicated.\nNote that if the modified task graph is not resumed within a retention period (currently 30 days), the latest version of the task is dropped. After this period, the task is not replicated to a secondary database unless it\u2019s resumed again.\nRoot task in a task graph was created and resumed or executed, but was subsequently suspended and dropped.\n\u274c\nThe entire task graph is not replicated to a secondary database.\nChild task in a task graph is created and resumed or executed, but is subsequently suspended and dropped.\n\u2714\nThe latest version of the task graph (before the task was suspended and dropped) is replicated to a secondary database.\nResumed or Suspended",
    " the task is not replicated to a secondary database unless it\u2019s resumed again.\nRoot task in a task graph was created and resumed or executed, but was subsequently suspended and dropped.\n\u274c\nThe entire task graph is not replicated to a secondary database.\nChild task in a task graph is created and resumed or executed, but is subsequently suspended and dropped.\n\u2714\nThe latest version of the task graph (before the task was suspended and dropped) is replicated to a secondary database.\nResumed or Suspended State of Replicated Tasks\n\u00b6\nIf all of the following conditions are met, a task is replicated to a secondary database in a resumed state:\nA standalone or root task is in a resumed state in the primary database when the replication or refresh operation begins until the operation is completed. If a task is in a resumed state during only part of this period, it might still be replicated in a resumed state.\nA child task is in a resumed state in the latest version of the task.\nThe parent database was replicated to the target account along with role objects in the same, or different,\nreplication or failover group\n.\nAfter the roles and database are replicated, you must refresh the objects in the target account by executing either\nALTER REPLICATION GROUP \u2026 REFRESH\nor\nALTER FAILOVER GROUP \u2026 REFRESH\n, respectively. If you refresh the database by executing\nALTER DATABASE \u2026 REFRESH\n, the state of the tasks in the database is changed to suspended.\nA replication or refresh operation includes the privilege grants for a task that were current when the latest table version was committed. For more information, see\nReplicated Tasks and Privilege Grants\n(in this topic).\nIf these conditions are not met, the task is replicated to a secondary database in a suspended state.\nNote\nSecondary tasks aren\u2019t scheduled until after a failover, regardless of their\nstate\n. For more details, refer to\nTask Runs After a Failover\nReplicated Tasks and Privilege Grants\n\u00b6\nIf the parent database is replicated to a target account along with role objects in the same, or different, replication or failover group, the privileges granted on the tasks in the database are replicated as well.\nThe following logic determines which task privileges are replicated in a replication or refresh operation:\nIf the current task owner (that is, the role that has the OWNERSHIP privilege on a task) is the same role as when the task was resumed last, then all current grants on the",
    "\n\u00b6\nIf the parent database is replicated to a target account along with role objects in the same, or different, replication or failover group, the privileges granted on the tasks in the database are replicated as well.\nThe following logic determines which task privileges are replicated in a replication or refresh operation:\nIf the current task owner (that is, the role that has the OWNERSHIP privilege on a task) is the same role as when the task was resumed last, then all current grants on the task are replicated to the secondary database.\nIf the current task owner is\nnot\nthe same role as when the task was resumed last, then only the OWNERSHIP privilege granted to the owner role in the task version is replicated to the secondary database.\nIf the current task owner role is not available (for example, a child task is dropped but a new version of the task graph is not committed yet), then only the OWNERSHIP privilege granted to the owner role in the task version is replicated to the secondary database.\nTask Runs After a Failover\n\u00b6\nAfter a secondary failover group is promoted to serve as the primary group, any resumed tasks in databases within the failover group are scheduled gradually. The amount of time required to restore normal scheduling of all resumed standalone tasks and task graphs depends on the number of resumed tasks in a database.\nReplication and tags\n\u00b6\nTags and their assignments can be replicated from a source account to a target account.\nTag assignments cannot be modified in the target account after the initial replication from the source account. For example,\nsetting a tag on a secondary (i.e. replicated) database is not allowed. To modify tag assignments in the target account, modify\nthem in the source account and replicate them to the target account.\nTo successfully replicate tags, ensure that the replication or failover group includes:\nThe database containing the tags in the\nALLOWED_DATABASES\nproperty.\nOther account-level objects that have a tag in the\nOBJECT_TYPES\nproperty (e.g.\nROLES\n,\nWAREHOUSES\n).\nFor more information, see\nCREATE REPLICATION GROUP\nand\nCREATE FAILOVER GROUP\n.\nReplication and instances of Snowflake classes\n\u00b6\nAn instance of the\nCUSTOM_CLASSIFIER\nclass is replicated when the database that contains\nthe instance is replicated. Replication of instances of other Snowflake\nclasses\nis\nnot\nsupported.\nHistorical usage data\n\u00b6\nHistorical usage data for activity in",
    "property (e.g.\nROLES\n,\nWAREHOUSES\n).\nFor more information, see\nCREATE REPLICATION GROUP\nand\nCREATE FAILOVER GROUP\n.\nReplication and instances of Snowflake classes\n\u00b6\nAn instance of the\nCUSTOM_CLASSIFIER\nclass is replicated when the database that contains\nthe instance is replicated. Replication of instances of other Snowflake\nclasses\nis\nnot\nsupported.\nHistorical usage data\n\u00b6\nHistorical usage data for activity in a primary database is not replicated to secondary databases. Each account has its own query history,\nlogin history, etc.\nHistorical usage data includes the query data returned by the following\nSnowflake Information Schema\ntable functions or\nAccount Usage\nviews:\nCOPY_HISTORY\nLOGIN_HISTORY\nQUERY_HISTORY\netc.\nOn this page\nReplication group and failover group constraints\nDatabase and share objects\nAccount objects\nReplication privileges\nReplication and references across replication groups\nSession policies and secondary roles\nDangling references and network policies\nDangling references and packages policies\nDangling references and secrets\nDangling references and streams\nReplication and read-only secondary objects\nReplication and objects in target accounts\nObjects recreated in target accounts\nReplication and security policies\nAuthentication, password, & session policies\nPrivacy policies\nSession policies with secondary roles\nReplication and secrets\nReplication and cloning\nLogical replication of clones\nReplication and automatic clustering\nReplication and large, high-churn tables\nReplication and Time Travel\nReplication and materialized views\nReplication and Apache Iceberg\u2122 tables\nReplication and dynamic tables\nDynamic tables and replication groups\nDynamic tables and failover groups\nReplication and Snowpipe Streaming\nAvoiding data loss\nRequirements\nReplication and stages\nReplication and pipes\nReplication of data metric functions (DMFs)\nReplication of stored procedures and user-defined functions (UDFs)\nStored Procedures and UDFs and Stages\nStored Procedures and UDFs and External Network Access\nReplication and storage lifecycle policies\nReplication and streams\nSupported Source Objects for Streams\nAvoiding Data Duplication\nStream References in Task WHEN Clause\nStream Staleness\nStream Replication and Time Travel\nAvoiding Data Loss\nReplication and tasks\nReplication Scenarios\nResumed or Suspended State of Replicated Tasks\nReplicated Tasks and Privilege Grants\nTask Runs After a Failover\nReplication and tags\n",
    "Stored Procedures and UDFs and External Network Access\nReplication and storage lifecycle policies\nReplication and streams\nSupported Source Objects for Streams\nAvoiding Data Duplication\nStream References in Task WHEN Clause\nStream Staleness\nStream Replication and Time Travel\nAvoiding Data Loss\nReplication and tasks\nReplication Scenarios\nResumed or Suspended State of Replicated Tasks\nReplicated Tasks and Privilege Grants\nTask Runs After a Failover\nReplication and tags\nReplication and instances of Snowflake classes\nHistorical usage data\nRelated content\nIntroduction to replication and failover across multiple accounts\nReplicating databases and account objects across multiple accounts",
    " account, include\nNETWORK\nPOLICIES\nand\nACCOUNT\nPARAMETERS\nin the\nOBJECT_TYPES\nlist.\nIf a network policy is associated with a user, include\nNETWORK\nPOLICIES\nand\nUSERS\nin the\nOBJECT_TYPES\nlist.\nFor more details, see\nReplicating network policies\n.\nDangling references and packages policies\n\u00b6\nIf there is a\npackages policy\nset on the account, the following dangling references\nerror occurs during the refresh operation for a replication or failover group that contains account objects:\n003131 (55000): Dangling references in the snapshot. Correct the errors before refreshing again.\nThe following references are missing (referred entity <- [referring entities]):\nPOLICY '<policy_db>.<policy_schema>.<packages_policy_name>' <- [ACCOUNT '<account_locator>']\nTo avoid dangling references, replicate the database that contains the packages policy to the target account. The database containing the\npolicy can be in the same or different replication or failover group.\nDangling references and secrets\n\u00b6\nFor details, see\nReplication and secrets\n.\nDangling references and streams\n\u00b6\nDangling references for streams cause replication to fail with the following error message:\nPrimary database: the source object ''<object_name>'' for this stream ''<stream_name>'' is not included in the replication group.\nStream replication does not support replication across databases in different replication groups. Please see Streams Documentation\nhttps://docs.snowflake.com/en/user-guide/account-replication-considerations#replication-and-streams for options.\nTo avoid dangling reference errors:\nThe primary database must include both the stream and its base object\nor\nThe database that contains the stream and the database that contains the base object referenced by the stream must be included in the\nsame replication or failover group.\nReplication and read-only secondary objects\n\u00b6\nAll secondary objects in a target account, including secondary databases and shares, are read-only. Changes to replicated objects or object types\ncannot be made locally in a target account. For example, if the\nUSERS\nobject type is replicated from a source\naccount to a target account, new users cannot be created or modified in the target account.\nNew, local databases and shares\ncan\nbe created and modified in a target account. If\nROLES\nare also replicated\nto the target account, new roles cannot be created or modified in that target account.",
    " are read-only. Changes to replicated objects or object types\ncannot be made locally in a target account. For example, if the\nUSERS\nobject type is replicated from a source\naccount to a target account, new users cannot be created or modified in the target account.\nNew, local databases and shares\ncan\nbe created and modified in a target account. If\nROLES\nare also replicated\nto the target account, new roles cannot be created or modified in that target account. Therefore, privileges cannot be granted to (or revoked from)\na\nrole on a secondary object in the target account. However, privileges\ncan\nbe granted to (or revoked from) a secondary role on local\nobjects (for example, databases, shares, or replication or failover groups) created in the target account.\nReplication and objects in target accounts\n\u00b6\nIf you created account objects, for example, users and roles, in your target account by\nany means other than via replication\n(for example,\nusing scripts), these users and roles have no global identifier by default. When a target account is refreshed from the source account, the\nrefresh operation\ndrops\nany account objects of the types in the\nOBJECT_TYPES\nlist in the target account that have no\nglobal identifier.\nNote\nThe initial refresh operation to replicate USERS or ROLES might result in an error. This is to help prevent accidental deletion of\ndata and metadata associated with users and roles. For more information about the circumstances that determine whether these\nobject types are dropped or the refresh operation fails, see\nInitial replication of users and roles\n.\nTo avoid dropping these objects, see\nApply global IDs to objects created by scripts in target accounts\n.\nObjects recreated in target accounts\n\u00b6\nIf an existing object in the source account is replaced using a CREATE OR REPLACE statement, the existing object is dropped, and then\na new object with the same name is created in a single transaction. For example, if you execute a CREATE OR REPLACE statement for an\nexisting table\nt1\n, table\nt1\nis dropped, and then a new table\nt1\nis created. For more information, see the\nusage notes for CREATE TABLE\n.\nWhen objects are replaced on the target account, the DROP and CREATE statements do not execute atomically during a refresh operation.\nThis means the object might disappear briefly from the target account while it is being recreated as a new object.\nReplication and",
    " if you execute a CREATE OR REPLACE statement for an\nexisting table\nt1\n, table\nt1\nis dropped, and then a new table\nt1\nis created. For more information, see the\nusage notes for CREATE TABLE\n.\nWhen objects are replaced on the target account, the DROP and CREATE statements do not execute atomically during a refresh operation.\nThis means the object might disappear briefly from the target account while it is being recreated as a new object.\nReplication and security policies\n\u00b6\nThe database containing a security policy and the references (i.e. assignments) can be replicated using replication and failover\ngroups. Security policies include:\nAggregation policies\nAuthentication policies\nMasking policies\nPassword policies\nPrivacy policies\nProjection policies\nRow access policies\nSession policies\n, including\nsession policies with secondary roles\nTag-based masking policies\nIf you are using\ndatabase replication\n,\nsee\nDatabase replication and security objects\n.\nAuthentication, password, & session policies\n\u00b6\nAuthentication, password, and session policy references for users are replicated when specifying the database containing policy\n(\nALLOWED_DATABASES\n=\npolicy_db\n) and\nUSERS\nin a replication group or failover group.\nIf either the policy database or users have already been replicated to a target account, update the replication or failover group\nin the source account to include the databases and object types required to successfully replicate the policy. Then execute a refresh\noperation to update the target account.\nIf user-level policies are not in use,\nUSERS\ndo not need to be included in the replication or failover group.\nNote\nThe policy must be in the same account as the account-level policy assignment and the user-level policy assignment.\nIf you have a security policy set on the account or a user in the account and you do not update the\nreplication or failover group to include the\npolicy_db\ncontaining the policy and\nUSERS\n, a dangling reference occurs in\nthe target account. In this case, a dangling reference means that Snowflake cannot locate the policy in the target account because the\nfully-qualified name of the policy points to the database in the source account. Consequently, the target account or users in the target\naccount are not required to comply with the security policy.\nTo successfully replicate a security policy, verify the replication or failover group includes the object types and databases required\nto prevent a dangling reference.\nPrivacy policies\n\u00b6\n",
    " occurs in\nthe target account. In this case, a dangling reference means that Snowflake cannot locate the policy in the target account because the\nfully-qualified name of the policy points to the database in the source account. Consequently, the target account or users in the target\naccount are not required to comply with the security policy.\nTo successfully replicate a security policy, verify the replication or failover group includes the object types and databases required\nto prevent a dangling reference.\nPrivacy policies\n\u00b6\nConsider the following when replicating privacy policies and privacy-protected tables and views associated with\ndifferential privacy\n:\nIf a privacy policy is assigned to a table or view in the source account, the policy needs to be replicated in the target account.\nCumulative privacy loss for a privacy budget is not replicated.\nCumulative privacy loss in the target and source accounts are tracked separately.\nAdministrators in the target account cannot adjust the replicated privacy budget. The privacy budget is synced with the one in the source\naccount.\nIf an analyst has access to the privacy-protected table or view in both the source account and the target account, they can incur twice\nthe amount of privacy loss before reaching the privacy budget\u2019s limit.\nPrivacy domains set on the columns are also replicated.\nSession policies with secondary roles\n\u00b6\nIf you are using session policies with secondary roles, you must specify the policy database\nin the same replication group that contains the roles. For example:\nCREATE\nREPLICATION\nGROUP\nmyrg\nOBJECT_TYPES\n=\nDATABASES\n,\nROLES\n,\nUSERS\nALLOWED_DATABASES\n=\nsession_policy_db\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmyaccount\nREPLICATION_SCHEDULE\n=\n'10 MINUTE'\n;\nCopy\nIf you specify the session policy database that references secondary roles in a different replication or failover group (\nrg2\n) than the\nreplication or failover group that contains account-level objects (\nmyrg\n) and you replicate or fail over\nrg2\nfirst, a\ndangling reference\noccurs. An error message tells you to place the session policy\ndatabase in the replication or failover group that contains the roles. This behavior occurs when the session policy is set on the account\nor users.\nIf the session policy and account level objects are in different replication groups, and the session policy is not set on the account or\nusers, you can replicate and refresh the target account. Be sure to refresh for the",
    " fail over\nrg2\nfirst, a\ndangling reference\noccurs. An error message tells you to place the session policy\ndatabase in the replication or failover group that contains the roles. This behavior occurs when the session policy is set on the account\nor users.\nIf the session policy and account level objects are in different replication groups, and the session policy is not set on the account or\nusers, you can replicate and refresh the target account. Be sure to refresh for the replication group that contains the account level\nobjects first.\nIf you refresh the target account after replicating or failing over the session policy with secondary roles and role objects, the target\naccount reflects the session policy and secondary roles behavior in the source account.\nAdditionally, when you refresh the database in the target account and the database contains a session policy that references secondary\nroles,\nALLOWED_SECONDARY_ROLES\nalways evaluates to\n[ALL]\n.\nReplication and secrets\n\u00b6\nYou can only replicate the secret using a replication or failover group. Specify the database that contains the secret, the database that\ncontains UDFs or procedures that reference the secret, and the integrations that reference the secret in a single replication or failover\ngroup.\nIf you have the database that contains the secret in one replication or failover group and the integration that references the secret in a\ndifferent replication or failover group, then:\nIf you replicate the integration first and then the secret, the operation is successful: all objects are replicated and there are no\ndangling references.\nIf you replicate the secret before the integration and the secret does not already exist in the target account, a \u201cplaceholder secret\u201d is\nadded in the target account to prevent a dangling reference. Snowflake maps the placeholder secret to the integration.\nAfter you replicate the group that contains the integration, on the next refresh operation for the group that contains the secret,\nSnowflake updates the target account to replace the placeholder secret with the secret that is referenced in the integration.\nIf you replicate the secret and do not replicate the integration from\naccount1\nto\naccount2\n, the integration doesn\u2019t work in the\ntarget account (\naccount2\n) because there is no integration to use the secret. Additionally, if you failover and the target account is\npromoted to source account, the integration will not work.\nWhen you decide to failover to make\naccount1\nas the source account, the secret and integration references",
    " that is referenced in the integration.\nIf you replicate the secret and do not replicate the integration from\naccount1\nto\naccount2\n, the integration doesn\u2019t work in the\ntarget account (\naccount2\n) because there is no integration to use the secret. Additionally, if you failover and the target account is\npromoted to source account, the integration will not work.\nWhen you decide to failover to make\naccount1\nas the source account, the secret and integration references match and the placeholder\nsecret is not used. This allows you to use the security integration and the secret that contains the credentials because the objects can\nreference each other.\nReplication and cloning\n\u00b6\nHistorically\nCloned objects\nwere replicated physically rather than logically to secondary databases. That is,\ncloned tables in a standard database don\u2019t contribute to the overall data storage unless or until DML operations on the clone\nadd to or modify existing data. However, when a cloned table is replicated to a secondary database, the physical data is also replicated,\nincreasing the data storage usage for your account.\nA logically replicated cloned table shares the micro-partitions of the original table it was cloned from,\nreducing the physical storage of the secondary table in the target account.\nIf the original table and cloned table are included in the same replication or failover group, the cloned table can be replicated\nlogically to the target account.\nLogical replication of clones\n\u00b6\nIf the original and cloned table are included in the same replication or failover group, the cloned table can be replicated\nlogically to the target account.\nFor example, if table\nt2\nin database\ndb2\nis a clone of table\nt1\nin database\ndb1\n, and both databases are included\nin replication group\nrg1\n, then table\nt2\nis created as a logical clone in the target account.\nA cloned object can be cloned to create additional clones of the original object. The original object and the cloned objects are part\nof the same\nclone group\n. For example, if table\nt3\nin database\ndb3\nis created as a clone of\nt2\n, it is in the same clone group\nas the original table\nt1\nand the cloned table\nt2\n.\nIf database\ndb3\nis later added to the replication group\nrg1\n, table\nt3\nis created in the",
    " the original object. The original object and the cloned objects are part\nof the same\nclone group\n. For example, if table\nt3\nin database\ndb3\nis created as a clone of\nt2\n, it is in the same clone group\nas the original table\nt1\nand the cloned table\nt2\n.\nIf database\ndb3\nis later added to the replication group\nrg1\n, table\nt3\nis created in the target account as a logical clone of\ntable\nt1\n.\nConsiderations\n\u00b6\nTables that are in the same clone group in the source account might not be in the same clone group in the target account.\nThe original table and its cloned table must be in the same replication or failover group.\nIn some cases, not all micro-partitions of the clone group can be shared with the cloned table. This can result in additional storage usage\nfor the cloned table in the target account.\nExample\n\u00b6\nTable\nt2\nin database\ndb2\nis a clone of table\nt1\nin database\ndb1\n. Include both databases in\nreplication group\nmyrg\nto logically replicate\nt2\nto the target account:\nCREATE\nREPLICATION\nGROUP\nmyrg\nOBJECT_TYPES\n=\nDATABASES\nALLOWED_DATABASES\n=\ndb1\n,\ndb2\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmyaccount2\nREPLICATION_SCHEDULE\n=\n'10 MINUTE'\n;\nCopy\nReplication and automatic clustering\n\u00b6\nIn a primary database, Snowflake monitors clustered tables using\nAutomatic Clustering\nand reclusters them as\nneeded. As part of a refresh operation, clustered tables are replicated to a secondary database with the current sorting of the table\nmicro-partitions. As such, reclustering is\nnot\nperformed again on the clustered tables in the secondary database, which would be\nredundant.\nIf a secondary database contains clustered tables and the database is promoted to become the primary database, Snowflake begins Automatic\nClustering of the tables in this database while simultaneously suspending the monitoring of clustered tables in the previous primary\ndatabase.\nSee\nReplication and Materialized Views\n(in this topic) for information about Automatic Clustering for materialized views.\nReplication and large, high-churn tables\n\u00b6\nWhen one or more rows of a table are updated or deleted, all of the",
    "Understanding replication cost\n\u00b6\nStandard & Business Critical Feature\nDatabase and share replication are available to all accounts.\nReplication of other account objects & failover/failback require Business Critical Edition (or higher).\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nCharges based on replication are divided into two categories: data transfer and compute resources. Both categories are billed on the\ntarget account (i.e. the account that stores the secondary database or secondary replication/failover group that is refreshed).\nData transfer\n:\nThe initial replication and subsequent synchronization operations transfer data between regions. Cloud providers charge for\ndata transferred from one region to another within their own network.\nThe data transfer rate is determined by the location of the source account (i.e. the account that stores the primary replication\nor failover group). For data transfer pricing, see the\nSnowflake Service Consumption Table\n.\nFor more information, see\nUnderstanding data transfer cost\n.\nCompute resources\n:\nReplication operations use Snowflake-provided compute resources for the following:\nTo determine the delta of both metadata and data to be copied during the refresh operation.\nTo copy the data between accounts across regions.\nThe service type for compute costs for replication in the\naccount usage\nand\norganization usage\nviews is REPLICATION.\nFor more information, see\nUnderstanding compute cost\n.\nNote\nThe target account also incurs standard storage costs for the data in each secondary database in the account.\nThe target account also incurs costs for the automatic background processes that service\nmaterialized views\nand\nsearch optimization\n. For details, see the \u201cServerless\nFeature Credit Table\u201d in the\nSnowflake Service Consumption Table\nfor the costs per compute hour.\nReplication charges are applied even if the initial replication or a refresh operation doesn\u2019t succeed. Any data that is copied\nbefore the initial replication or refresh operation fails can be reused by a subsequent refresh operation (if performed within 14 days)\nand doesn\u2019t need to be copied again.\nEstimating and controlling costs\n\u00b6\nIn general, monthly billing for replication is proportional to:\nAmount of table data in the primary database, or databases in a replication/failover group, that changes as a result of data loading\nor DML operations.\nFrequency of secondary database, or replication/failover group, refreshes from the primary database or replication/failover group.\nYou can control the cost of replication by carefully choosing which databases or objects to replicate and their",
    " to be copied again.\nEstimating and controlling costs\n\u00b6\nIn general, monthly billing for replication is proportional to:\nAmount of table data in the primary database, or databases in a replication/failover group, that changes as a result of data loading\nor DML operations.\nFrequency of secondary database, or replication/failover group, refreshes from the primary database or replication/failover group.\nYou can control the cost of replication by carefully choosing which databases or objects to replicate and their refresh frequency. You\ncan stop incurring replication costs by ceasing refresh operations.\nViewing actual costs\n\u00b6\nUsers with the ACCOUNTADMIN role can use SQL to view the amount of data transferred (in bytes) and the credit usage for\nreplication using replication or failover groups for your Snowflake account within a specified date range.\nUsers with the ACCOUNTADMIN role can use\nSnowsight\nor SQL to view the amount of replication data transferred\n(in bytes) for your Snowflake account within a specified date range.\nSnowsight\n:\nIn the navigation menu, select\nAdmin\n\u00bb\nCost management\n.\nTo view the data transfer amounts and credit usage for replication for your account:\nSQL\n:\nQuery either of the following:\nREPLICATION_GROUP_USAGE_HISTORY\ntable function (in the\nSnowflake Information Schema\n). This\nfunction returns replication usage activity within the last 14 days.\nREPLICATION_GROUP_USAGE_HISTORY view\n(in\nAccount Usage\n). This view returns\nreplication usage activity within the last 365 days (1 year).\nFor examples, see\nMonitor replication costs\n.\nTo view the cost of replication for individual databases replicated with Database Replication, see\nMonitoring database replication cost\n.\nOn this page\nEstimating and controlling costs\nViewing actual costs\nRelated content\nIntroduction to replication and failover across multiple accounts\nManaging cost in Snowflake",
    "Error notifications for replication and failover groups\n\u00b6\nStandard & Business Critical Feature\nDatabase and share replication are available to all accounts.\nReplication of other account objects & failover/failback require Business Critical Edition (or higher).\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nYou can receive error notifications for refresh operation failures by setting a notification integration for a primary replication\nor failover group.\nError notifications for refresh operation failures\n\u00b6\nWhen you enable error notifications for a replication or failover group, a notification is sent via the designated email, cloud messaging\nservice, or the webhook when a refresh operation fails.\nNotifications includes the following information:\nSource and target account names.\nSource and target regions (and region group, if applicable).\nPrimary and secondary replication or failover group name.\nTimestamp when the error occurred.\nError code and message.\nSource and target login URL.\nError notifications and failover\n\u00b6\nNotifications are enabled on the primary replication or failover group and sent using a notification integration. The\nnotification integration is not required to be replicated to the target account. In the case of failover, if the notification\nintegration has been replicated, or there is an existing notification integration with the same name, in the newly promoted\nsource account, error notifications continue to be sent.\nIf the notification integration is not available, error notifications are not sent for refresh operation failures.\nPrerequisite: Notification integration for error notifications\n\u00b6\nA notification integration is required to send error notifications. The notification integration must be one of the\nfollowing types to send email notifications on refresh operation failures:\nTYPE = EMAIL\n:\nThe email notification integration must have at least one verified email address in the DEFAULT_RECIPIENTS list.\nFor more information about creating an email notification with a default list of recipients, see\nSpecify a default list of recipients and a default subject line\n.\nTYPE = QUEUE\n:\nYou can use a notification integration that is configured to push notifications to a messaging service for any of the\ncloud providers supported by Snowflake. You must set the notification integration TYPE parameter to QUEUE and the DIRECTION\nparameter to OUTBOUND.\nFor more information, see\nSending notifications to cloud provider queues (Amazon SNS, Google Cloud PubSub, and Azure Event Grid)\n.\nTYPE = WEBHOOK\n:\nYou can use a notification integration that is configured to push notifications to a webhook for any of the external systems\nsupported by Snowflake. Set the notification integration TYPE parameter to",
    " for any of the\ncloud providers supported by Snowflake. You must set the notification integration TYPE parameter to QUEUE and the DIRECTION\nparameter to OUTBOUND.\nFor more information, see\nSending notifications to cloud provider queues (Amazon SNS, Google Cloud PubSub, and Azure Event Grid)\n.\nTYPE = WEBHOOK\n:\nYou can use a notification integration that is configured to push notifications to a webhook for any of the external systems\nsupported by Snowflake. Set the notification integration TYPE parameter to WEBHOOK. You may also need to create a secret\n(if required by the external system).\nFor more information, see\nSending webhook notifications\n.\nCreate a notification integration (TYPE = EMAIL)\n\u00b6\nTo create an email notification integration named\nmy_notification_int\nwith email address\nfirst.last@example.com\n, follow\nthese steps:\nEnsure that the email address\nfirst.last@example.com\nhas been verified\n.\nCreate the notification integration by executing the\nCREATE NOTIFICATION INTEGRATION\ncommand. For example:\nCREATE\nNOTIFICATION\nINTEGRATION\nmy_notification_int\nTYPE\n=\nEMAIL\nENABLED\n=\nTRUE\nDEFAULT_RECIPIENTS\n=\n(\n'first.last@example.com'\n);\nCopy\nCreate a notification integration (TYPE = QUEUE)\n\u00b6\nTo create a notification integration for pushing notifications to a cloud provider queue, follow the instructions provided for the\ncurrently supported cloud provider queues:\nCreating a notification integration to send notifications to an Amazon SNS topic\nCreating a notification integration to send notifications to a Microsoft Azure Event Grid topic\nCreating a notification integration to send notifications to a Google Cloud Pub/Sub topic\nCreate a notification integration (TYPE = WEBHOOK)\n\u00b6\nTo create a notification integration for pushing notifications to an external system webhook, follow the instructions provided for\nthe currently supported external system webhooks:\nCreating a\nSlack secret\nand\nSlack notification integration\nCreating a\nMicrosoft Teams secret\nand\nMicrosoft Teams notification integration\nCreating a\nPagerDuty secret\nand\nPagerDuty notification integration\nImportant\nThe webhook notification integration must specify the WEBHOOK_BODY_TEMPLATE parameter with\nSNOWFLAKE_WEBHOOK_MESSAGE\nas a placeholder value. When the notification is sent, the placeholder is replaced with the contents of the replication\nerror notification, as described in\nError notifications for refresh operation failures\n.\nThe format for specifying WEBHOOK_BODY_TEMPLATE depends on the external system:\nFor Slack or Microsoft Teams, WEBHOOK_BODY_TEMPLATE utilizes the following single",
    "uty secret\nand\nPagerDuty notification integration\nImportant\nThe webhook notification integration must specify the WEBHOOK_BODY_TEMPLATE parameter with\nSNOWFLAKE_WEBHOOK_MESSAGE\nas a placeholder value. When the notification is sent, the placeholder is replaced with the contents of the replication\nerror notification, as described in\nError notifications for refresh operation failures\n.\nThe format for specifying WEBHOOK_BODY_TEMPLATE depends on the external system:\nFor Slack or Microsoft Teams, WEBHOOK_BODY_TEMPLATE utilizes the following single-value JSON object\nformat as its value:\nWEBHOOK_BODY_TEMPLATE='\n{\n\"text\":\n\"SNOWFLAKE_WEBHOOK_MESSAGE\"\n}\n'\nCopy\nFor PagerDuty, WEBHOOK_BODY_TEMPLATE utilizes a multi-value JSON object as its value, but with the following differences\nfrom a standard PagerDuty notification integration:\nWithin the\npayload\nkey, the\nsummary\nkey is\nnot\nused to specify\nSNOWFLAKE_WEBHOOK_MESSAGE\n.\nInstead, an additional\ncustom_details\nkey is used to specify\nSNOWFLAKE_WEBHOOK_MESSAGE\n.\nFor example:\nWEBHOOK_BODY_TEMPLATE='\n{\n\"routing_key\":\n\"SNOWFLAKE_WEBHOOK_SECRET\"\n,\n\"event_action\":\n\"trigger\"\n,\n\"payload\":\n{\n\"summary\":\n\"Snowflake replication failure\"\n,\n\"source\":\n\"Snowflake monitoring\"\n,\n\"severity\":\n\"INFO\"\n,\n\"custom_details\":\n{\n\"message\":\n\"SNOWFLAKE_WEBHOOK_MESSAGE\"\n}\n}\n}\n'\nCopy\nAdd an error notification for a replication or failover group\n\u00b6\nTo enable error notifications for an existing replication/failover group, use the\nALTER REPLICATION GROUP\nor\nALTER FAILOVER GROUP\ncommand to set the ERROR_INTEGRATION parameter.\nFor example, add notification integration\nmy_notification_int\nto failover group\nmy_fg\n. The following statement must\nbe executed from the\nsource\naccount:\nALTER\nFAILOVER\nGROUP\nmy_fg\nSET\nERROR_INTEGRATION\n=\nmy_notification_int\n;\nCopy\nTo create a replication/failover group and enable error notifications, use the\nCREATE REPLICATION GROUP\nor\nCREATE FAILOVER GROUP\ncommand and set the ERROR_INTEGRATION parameter.\nFor example, to create failover group\nmy_fg\nto enable replication and failover of databases\ndb1\n,\ndb2\nto accounts\nmyaccount2\nand\nmyaccount2\nin organization",
    "_fg\nSET\nERROR_INTEGRATION\n=\nmy_notification_int\n;\nCopy\nTo create a replication/failover group and enable error notifications, use the\nCREATE REPLICATION GROUP\nor\nCREATE FAILOVER GROUP\ncommand and set the ERROR_INTEGRATION parameter.\nFor example, to create failover group\nmy_fg\nto enable replication and failover of databases\ndb1\n,\ndb2\nto accounts\nmyaccount2\nand\nmyaccount2\nin organization\nmyorg\n, execute the following statement in the\nsource\naccount to\ncreate a primary failover group:\nCREATE\nFAILOVER\nGROUP\nmy_fg\nOBJECT_TYPES\n=\nDATABASES\nALLOWED_DATABASES\n=\ndb1\n,\ndb2\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmyaccount2\n,\nmyorg\n.\nmyaccount3\nREPLICATION_SCHEDULE\n=\n'10 MINUTE'\nERROR_INTEGRATION\n=\nmy_notification_int\n;\nCopy\nNote\nIf the replication schedule for a replication or failover group is set to a high frequency, for example one minute,\nerror notifications for the same issue are sent for every scheduled refresh operation.\nOn this page\nError notifications for refresh operation failures\nError notifications and failover\nPrerequisite: Notification integration for error notifications\nAdd an error notification for a replication or failover group\nRelated content\nIntroduction to replication and failover across multiple accounts\nReplication considerations",
    "Failing over account objects\n\u00b6\nBusiness Critical Feature\nThis features requires Business Critical Edition (or higher). To inquire about upgrading, please\ncontact\nSnowflake Support\n.\nThis topic describes the steps necessary to fail over replicated account objects across multiple accounts in different\nregions\nfor disaster recovery.\nFor information about the purpose of the failover mechanism and when to use it, see\nIntroduction to business continuity & disaster recovery\n.\nPrerequisites\n\u00b6\nEnable replication in a set of accounts within the same organization, across multiple regions in one cloud service provider\nor across different cloud service providers.\nCreate a primary failover group that defines the kinds of objects to replicate, and specifies the target accounts\nto which to replicate. You can optionally divide the replicated objects across multiple failover groups, for example if\nsome databases should be replicated more frequently than others.\nCreate at least one secondary failover group (replica) of each primary failover group in one or more secondary accounts.\nRefresh (synchronize) each replica with the latest updates to the objects in the failover group. Perform an\ninitial refresh, and set up a schedule to regularly bring the latest changes to each secondary account.\nFor instructions, see\nReplicating account objects and databases\n.\nPromote a target account to serve as the source account\n\u00b6\nYou can promote a target account to serve as the source account (failover) using Snowsight or\nSQL\n.\nFor more information about the kinds of objects you can specify in a failover group,\nsee\nReplication groups and failover groups\n.\nPromote a target account to serve as the source account using Snowsight\n\u00b6\nNote\nOnly account administrators can edit a replication or failover group using Snowsight (see\nLimitations of using Snowsight for replication configuration\n).\nFor the most consistent and reliable failover experience, select all the applicable failover groups and\nconnections and promote them all at the same time. We refer to this operation as a\nbulk failover\n.\nTo promote a target account to serve as the source account using Snowsight, follow these steps:\nSign in to\nSnowsight\n.\nMake sure to sign in using the target account\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\n, then select\nInitiate failover\n. Doing so brings up a dialog where you make the remaining choices.\nSelect any failover groups",
    " We refer to this operation as a\nbulk failover\n.\nTo promote a target account to serve as the source account using Snowsight, follow these steps:\nSign in to\nSnowsight\n.\nMake sure to sign in using the target account\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\n, then select\nInitiate failover\n. Doing so brings up a dialog where you make the remaining choices.\nSelect any failover groups to promote.  After the failover, the objects specified in those\nfailover groups become writable on the newly promoted primary account. Those objects become\nread-only on the account that formerly was the primary and is now a secondary account.\nSelect\nNext\n.\nSelect any connections to promote. After the failover, those connections connect to the account\nthat you\u2019re promoting to be the new primary account.\nSelect\nNext\n.\nSelect\nFail over\nin the confirmation window.\nIf any refresh operations are in progress for the failover groups you selected, you can wait for those refreshes\nto complete, or choose an alternative approach if your failover is urgent and should take priority.\nThe default action is to wait for the refreshes to complete. That way, the primary and secondary systems are all\nin a consistent state when the bulk failover runs. Snowflake uses your currently selected warehouse to poll the\nstatus of the ongoing refreshes. If you don\u2019t have a selected warehouse, you select one now using the\nSelect warehouse\noption.\nOr, you can proceed with the failover immediately by selecting\nShow advanced options\n.\nTo fail over only the failover groups that aren\u2019t currently being refreshed, select\nExit with current progress\n.\nIn that case, you perform additional refreshes later for the groups that were skipped during the bulk failover.\nTo cancel the refresh operations and continue the failover, select\nCancel refreshes and force failover\n.\nIn that case, you might need to clean up any inconsistencies on the secondary system from the interrupted refreshes.\nIf the failover operation didn\u2019t complete for all failover groups, you can perform another bulk failover. Or you can fail over\nthe remaining failover groups one at a time, using the procedure in\nPromote a single failover group to serve as the primary using Snowsight\n.\nPromote a single failover group to serve as the primary using Snowsight\n\u00b6\n",
    " case, you might need to clean up any inconsistencies on the secondary system from the interrupted refreshes.\nIf the failover operation didn\u2019t complete for all failover groups, you can perform another bulk failover. Or you can fail over\nthe remaining failover groups one at a time, using the procedure in\nPromote a single failover group to serve as the primary using Snowsight\n.\nPromote a single failover group to serve as the primary using Snowsight\n\u00b6\nNote\nOnly account administrators can edit a replication or failover group using Snowsight (see\nLimitations of using Snowsight for replication configuration\n).\nTo promote a single failover group to be the primary using Snowsight, follow these steps:\nSign in to\nSnowsight\n.\nMake sure to sign in using the target account\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\n, then select\nGroups\n.\nLocate the failover group that you want to promote, and select the\nMore\nmenu (\n\u2026\n) in the last column of the row.\nSelect\nFail over\n, then select\nFail over\nin the confirmation window.\nTip\nYou typically use this procedure if you encounter a problem failing over one group, and you need to retry the\nfailover only for that group. To promote an entire account to be the primary, select multiple failover groups and connections and\nperform a bulk failover. For more information, see\nPromote a target account to serve as the source account using Snowsight\n.\nPromote a target account to serve as the source account using SQL\n\u00b6\nTo promote a target account to serve as the source account using SQL, you sign in to the target account\nand execute the\nALTER FAILOVER GROUP \u2026 PRIMARY\ncommand.\nPromote a secondary failover group to primary failover group\n\u00b6\nNote\nThe example in this section must be executed by a role with the FAILOVER privilege.\nThe following example promotes\nmyaccount2\nin the current\nmyorg\norganization to serve as the source account.\nSign in to target account\nmyaccount2\n.\nList failover groups in the account:\nSHOW FAILOVER GROUPS\n;\nCopy\nExecute the following statement for each secondary failover group you want to promote to serve as the primary failover group:\nALTER\nFAILOVER\nGROUP\nmy",
    " be executed by a role with the FAILOVER privilege.\nThe following example promotes\nmyaccount2\nin the current\nmyorg\norganization to serve as the source account.\nSign in to target account\nmyaccount2\n.\nList failover groups in the account:\nSHOW FAILOVER GROUPS\n;\nCopy\nExecute the following statement for each secondary failover group you want to promote to serve as the primary failover group:\nALTER\nFAILOVER\nGROUP\nmyfg\nPRIMARY\n;\nCopy\nNote\nDuring a partial outage in your source region, the replication service might continue to be available and might continue\nto refresh the secondary failover groups in target regions.\nTo ensure data integrity, Snowflake prevents failover if a refresh operation is in progress. This means you cannot\npromote a secondary failover group to serve as the primary if it is being refreshed by a replication operation.\nThe ALTER FAILOVER GROUP \u2026 PRIMARY command returns an error in this scenario.\nResolving failover statement failure due to an in-progress refresh operation\n\u00b6\nIf there is a refresh operation in progress for the secondary failover group you are trying to promote, the failover statement\nresults in the following error:\nReplication group \"<GROUP_NAME>\" cannot currently be set as primary because it is being\nrefreshed. Either wait for the refresh to finish or cancel the refresh and try again.\nTo successfully fail over, you must complete the following steps.\nSelect and complete one of the following options:\nImportant\nSuspending a refresh operation in the SECONDARY_DOWNLOADING_METADATA or SECONDARY_DOWNLOADING_DATA phase\nmight result in an inconsistent state on the target account. For more information, see\nView the current phase of an in-progress refresh operation\n.\nSuspend future refresh operations for the failover group. If there is an in-progress refresh operation, you must wait for\nit to complete before you can failover:\nALTER\nFAILOVER\nGROUP\nmyfg\nSUSPEND\n;\nCopy\nSuspend future refresh operations\nand\ncancel a scheduled refresh operation that is currently in progress (if there is one).\nIf the in-progress refresh operation was manually triggered, see\nCancel an in-progress refresh operation that wasn\u2019t automatically scheduled\n.\nALTER\nFAILOVER\nGROUP\nmyfg\nSUSPEND\nIMMEDIATE\n;\nCopy\nNote\nYou might experience a slight delay between the time that the statement returns and the",
    "\nmyfg\nSUSPEND\n;\nCopy\nSuspend future refresh operations\nand\ncancel a scheduled refresh operation that is currently in progress (if there is one).\nIf the in-progress refresh operation was manually triggered, see\nCancel an in-progress refresh operation that wasn\u2019t automatically scheduled\n.\nALTER\nFAILOVER\nGROUP\nmyfg\nSUSPEND\nIMMEDIATE\n;\nCopy\nNote\nYou might experience a slight delay between the time that the statement returns and the time that the cancellation\nof the refresh operation is finished.\nVerify no refresh operations are in progress for the failover group\nmyfg\n. The following query\nshould return no results:\nSELECT\nphase_name\n,\nstart_time\n,\njob_uuid\nFROM\nTABLE\n(\nINFORMATION_SCHEMA\n.\nREPLICATION_GROUP_REFRESH_HISTORY\n(\n'myfg'\n))\nWHERE\nphase_name\n<>\n'COMPLETED'\nand\nphase_name\n<>\n'CANCELED'\n;\nCopy\nTo see canceled refresh operations for failover group\nmyfg\n, you can execute the following statement:\nSELECT\nphase_name\n,\nstart_time\n,\njob_uuid\nFROM\nTABLE\n(\nINFORMATION_SCHEMA\n.\nREPLICATION_GROUP_REFRESH_HISTORY\n(\n'myfg'\n))\nWHERE\nphase_name\n=\n'CANCELED'\n;\nCopy\nNow you can promote the secondary failover group\nmyfg\nto primary failover group:\nALTER\nFAILOVER\nGROUP\nmyfg\nPRIMARY\n;\nCopy\nResume scheduled replication in target accounts\n\u00b6\nOn failover, scheduled refreshes on all secondary failover groups are suspended.\nALTER FAILOVER GROUP \u2026 RESUME\nmust be executed in each\ntarget account\nwith a\nsecondary failover group to resume automatic refreshes.\nALTER\nFAILOVER\nGROUP\nmyfg\nRESUME\n;\nCopy\nView the current phase of an in-progress refresh operation\n\u00b6\nA refresh operation can be safely canceled during most phases of the refresh operation. However, canceling a refresh operation\nin the SECONDARY_DOWNLOADING_METADATA or SECONDARY_DOWNLOADING_DATA phase might result in an inconsistent state on the target\naccount.  If the refresh operation has started one of these phases, it proceeds to completion regardless of the availability of\nthe source account. Allowing the phase to complete before you fail over ensures replicas are in a consistent state.\nAfter the replicas are in a consistent state, you can resume or replay your ingest and transformation pipelines to",
    " operation. However, canceling a refresh operation\nin the SECONDARY_DOWNLOADING_METADATA or SECONDARY_DOWNLOADING_DATA phase might result in an inconsistent state on the target\naccount.  If the refresh operation has started one of these phases, it proceeds to completion regardless of the availability of\nthe source account. Allowing the phase to complete before you fail over ensures replicas are in a consistent state.\nAfter the replicas are in a consistent state, you can resume or replay your ingest and transformation pipelines to update the\nreplicas to the current state.\nTo view the current phase of an in-progress refresh operation for a failover group, use the Information Schema\nREPLICATION_GROUP_REFRESH_PROGRESS, REPLICATION_GROUP_REFRESH_PROGRESS_BY_JOB, REPLICATION_GROUP_REFRESH_PROGRESS_ALL\ntable function.\nFor example, to view the current phase of an in-progress refresh operation for failover group\nmyfg\n, execute\nthe following statement:\nSELECT\nphase_name\n,\nstart_time\n,\nend_time\nFROM\nTABLE\n(\nINFORMATION_SCHEMA\n.\nREPLICATION_GROUP_REFRESH_PROGRESS\n(\n'myfg'\n)\n);\nCopy\nFor a list of refresh operations phases, see the\nusage notes\nfor the function.\nCancel an in-progress refresh operation that wasn\u2019t automatically scheduled\n\u00b6\nTo cancel an in-progress refresh operation that was not triggered automatically by a replication schedule, you must use the\nSYSTEM$CANCEL_QUERY\nfunction:\nFind the query ID or JOB_UUID for running refresh operations using one of the following options:\nFind the query IDs for all running refresh operations:\nSELECT\nquery_id\n,\nquery_text\nFROM\nTABLE\n(\nINFORMATION_SCHEMA\n.\nQUERY_HISTORY\n())\nWHERE\nquery_type\n=\n'REFRESH REPLICATION GROUP'\nAND\nexecution_status\n=\n'RUNNING'\nORDER\nBY\nstart_time\n;\nCopy\nUse the QUERY_TEXT column to identify the QUERY_ID for failover group refresh operations from the list.\nFind the JOB_UUID for an in-progress refresh operation for a specific failover group\nmyfg\n:\nSELECT\nphase_name\n,\nstart_time\n,\njob_uuid\nFROM\nTABLE\n(\nINFORMATION_SCHEMA\n.\nREPLICATION_GROUP_REFRESH_HISTORY\n(\n'myfg'\n))\nWHERE\nphase_name\n<>\n'COMPLETED'\nand\nphase_name\n<>\n'CANCELED'\n;\nCopy\nCancel the refresh operation using the SYSTEM$CANCEL_QUERY function and the QUERY_ID or JOB_UUID:\nSELECT\nSYSTEM$CANCEL_QUERY\n(\n'<QUERY_ID | JOB_UUID>'\n",
    " specific failover group\nmyfg\n:\nSELECT\nphase_name\n,\nstart_time\n,\njob_uuid\nFROM\nTABLE\n(\nINFORMATION_SCHEMA\n.\nREPLICATION_GROUP_REFRESH_HISTORY\n(\n'myfg'\n))\nWHERE\nphase_name\n<>\n'COMPLETED'\nand\nphase_name\n<>\n'CANCELED'\n;\nCopy\nCancel the refresh operation using the SYSTEM$CANCEL_QUERY function and the QUERY_ID or JOB_UUID:\nSELECT\nSYSTEM$CANCEL_QUERY\n(\n'<QUERY_ID | JOB_UUID>'\n);\nCopy\nReturns the following output:\nquery [<QUERY_ID>] terminated.\nAfter you cancel the in-progress refresh operation, continue to the\nnext steps\n.\nReopen active channels for Snowpipe Streaming in newly promoted source account\n\u00b6\nTables in a primary database that are populated by\nSnowpipe Streaming are replicated\nto secondary databases. After failover, reopen active Snowpipe Streaming channels for tables and re-insert any missing data rows\nfor the channels:\nReopen active channels for the table by calling the\nopenChannel\nAPI.\nFetch offset tokens:\nCall the\ngetLatestCommittedOffsetToken\nAPI\nor\nExecute the\nSHOW CHANNELS\ncommand to retrieve a list of the active channels of the table.\nRe-insert data rows for the channel from the fetched offset tokens.\nNote\nThese steps apply only to Snowpipe Streaming with the Snowflake Ingest SDK; it doesn\u2019t apply to Snowpipe Streaming with the Kafka connector. Follow\nthe steps below\nto restart the Kafka Connector after failover.\nSnowpipe Streaming and the Kafka connector\n\u00b6\nIf you are using the Kafka connector and Snowpipe Streaming, follow these steps after failover:\nUpdate the Kafka connector configuration to point to the newly promoted source account.\nExecute the SHOW CHANNELS command to retrieve the list of active channels and the offset tokens. Each channel belongs to a\nsingle partition in the Kafka topic.\nManually reset offsets in the Kafka Topic for each of those partitions (channels).\nRestart the Kafka Connector.\nFor more information, see:\nUsing Snowflake Connector for Kafka with Snowpipe Streaming Classic\n.\nReplication and Snowpipe Streaming\n.\nOn this page\nPrerequisites\nPromote a target account to serve as the source account\nPromote a target account to serve as the source account using Snowsight\nPromote a single failover group to serve as the primary using Snowsight\nPromote a target account to serve as the source account using SQL\nView the current phase of an in",
    "For more information, see:\nUsing Snowflake Connector for Kafka with Snowpipe Streaming Classic\n.\nReplication and Snowpipe Streaming\n.\nOn this page\nPrerequisites\nPromote a target account to serve as the source account\nPromote a target account to serve as the source account using Snowsight\nPromote a single failover group to serve as the primary using Snowsight\nPromote a target account to serve as the source account using SQL\nView the current phase of an in-progress refresh operation\nCancel an in-progress refresh operation that wasn\u2019t automatically scheduled\nReopen active channels for Snowpipe Streaming in newly promoted source account\nSnowpipe Streaming and the Kafka connector\nRelated content\nALTER FAILOVER GROUP",
    "Understanding and viewing Fail-safe\n\u00b6\nSeparate and distinct from Time Travel, Fail-safe ensures historical data is protected in the event of a system failure or other event (e.g. a\nsecurity breach).\nWhat is Fail-safe?\n\u00b6\nFail-safe provides a (non-configurable) 7-day period during which historical data may be recoverable by Snowflake. This period starts\nimmediately after the Time Travel retention period ends. Note, however, that a long-running Time Travel query will delay moving any data and\nobjects (tables, schemas, and databases) in the account into Fail-safe, until the query completes.\nAttention\nFail-safe is a data recovery service that is provided on a best effort basis and is intended only for use when all other recovery options have been attempted.\nFail-safe is\nnot\nprovided as a means for accessing historical data after the Time Travel retention period has ended. It is for use\nonly\nby\nSnowflake to recover data that may have been lost or damaged due to extreme operational failures.\nData recovery through Fail-safe may take from several hours to several days to complete.\nView Fail-safe storage for your account\n\u00b6\nWhen you review the total data storage usage for your account in Snowsight, you can view the\nhistorical data storage in Fail-safe.\nYou must use the ACCOUNTADMIN role to view the amount of data that is stored in Snowflake.\nIn Snowsight, follow these steps:\nIn the navigation menu, select\nAdmin\n\u00bb\nCost management\n, and then select\nConsumption\n.\nUse the\nUsage Type\nfilter to select\nStorage\n.\nReview the graph and table for Fail-safe storage. The\nStorage Breakdown\ncolumn in the table uses color-coded bars\nto represent the different kinds of storage, including Fail-safe storage. Hover the mouse pointer over\neach bar to see the size for each kind of storage.\nBilling for Fail-safe\n\u00b6\nData recovery through Fail-safe uses Snowflake-managed serverless compute. Standard serverless compute billing applies. For billing\ndetails, see \u201cTable 5: Serverless Feature Table\u201d in the\nSnowflake Service Consumption Table\n. To view the related credits that are consumed by data recovery\nthrough Fail-safe, use the following metering history views. Filter for the FAILSAFE_RECOVERY service type:\nMETERING_DAILY_HISTORY view\nMETERING_HISTORY view\nConsiderations\n\u00b6\nFor fail-safe and Snowpipe Streaming Classic",
    "-managed serverless compute. Standard serverless compute billing applies. For billing\ndetails, see \u201cTable 5: Serverless Feature Table\u201d in the\nSnowflake Service Consumption Table\n. To view the related credits that are consumed by data recovery\nthrough Fail-safe, use the following metering history views. Filter for the FAILSAFE_RECOVERY service type:\nMETERING_DAILY_HISTORY view\nMETERING_HISTORY view\nConsiderations\n\u00b6\nFor fail-safe and Snowpipe Streaming Classic, be aware of the following limitations:\nFail-safe doesn\u2019t support tables that contain data ingested by Snowpipe Streaming Classic. For such tables, you can\u2019t use fail-safe for recovery because fail-safe operations on that table will fail completely. For more information, see\nSnowpipe Streaming limitations\n.\nOn this page\nWhat is Fail-safe?\nView Fail-safe storage for your account\nBilling for Fail-safe\nConsiderations\nRelated content\nUnderstanding & using Time Travel\nData storage considerations",
    "Introduction to replication and failover across multiple accounts\n\u00b6\nStandard and Business Critical Feature\nDatabase and share replication are available to all accounts.\nReplication of other account objects & failover/failback require Business Critical Edition (or higher).\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nThis feature enables the replication of objects from a\nsource\naccount to one or more\ntarget\naccounts in the same organization.\nReplicated objects in each target account are referred to as\nsecondary\nobjects and are replicas of the\nprimary\nobjects in the source\naccount. Replication is supported across\nregions\nand across\ncloud platforms\n.\nRegion support for replication and failover/failback\n\u00b6\nAll Snowflake regions across Amazon Web Services, Google Cloud Platform, and Microsoft Azure support replication.\nCustomers can replicate across all regions within a\nregion group\n. To replicate between regions in\ndifferent region groups, (i.e. from a Snowflake commercial region to a Snowflake government or Virtual Private Snowflake region),\nplease contact\nSnowflake Support\n.\nReplication groups and failover groups\n\u00b6\nA\nreplication group\nis a defined collection of objects in a source account that are replicated as a unit to one or more target accounts. Replication groups provide read-only access for the replicated objects.\nA\nfailover group\nis a replication group that can also fail over. A secondary failover group in a target account provides read-only access for the replicated objects. When a secondary failover group is promoted to become the primary failover group, read-write access is available. You can promote any target account specified in the list of allowed accounts in a failover group to serve as the primary failover group.\nReplication and failover groups provide point-in-time consistency for the objects on the target account. The objects that can be included in a replication or failover group are listed below in\nReplicated objects\n.\nReplication feature / edition matrix\n\u00b6\nNote that some replication features are only available for Business Critical Edition (or higher).\nThe following table lists the availability of replication features for each Snowflake edition:\nFeature\nStandard\nEnterprise\nBusiness\u00a0Critical\nVPS\nDatabase replication\n\u2714\n\u2714\n\u2714\n\u2714\nShare replication\n\u2714\n\u2714\n\u2714\n\u2714\nReplication Group\n\u2714\n\u2714\n\u2714\n\u2714\nAccount object (other than database and share) replication\n\u2714\n\u2714\nFailover Group\n\u2714",
    "\u00b6\nNote that some replication features are only available for Business Critical Edition (or higher).\nThe following table lists the availability of replication features for each Snowflake edition:\nFeature\nStandard\nEnterprise\nBusiness\u00a0Critical\nVPS\nDatabase replication\n\u2714\n\u2714\n\u2714\n\u2714\nShare replication\n\u2714\n\u2714\n\u2714\n\u2714\nReplication Group\n\u2714\n\u2714\n\u2714\n\u2714\nAccount object (other than database and share) replication\n\u2714\n\u2714\nFailover Group\n\u2714\n\u2714\nData protected with\nTri-Secret Secure\n\u2714\n\u2714\nDataset replication\n\u2714\n\u2714\nCortex Search Service replication\n\u2714\n\u2714\nReplicated objects\n\u00b6\nThis feature supports replicating the objects listed below. Database replication and share replication are available on all editions.\nReplication of all other objects is only available for Business Critical Edition (or higher). For details on feature availability,\nsee the\nReplication feature / edition matrix\n.\nObject\nType or Feature\nReplicated\nNotes\nDatabases\n\u2714\nReplication of some databases is not supported or might fail the refresh operation. For more information, see\nCurrent limitations of replication\n.\nExternal volumes\n\u2714\nFailover group replication requires Business Critical Edition or higher later. Replication group replication is available to\nall accounts.\nIntegrations\nSecurity, API, Notification, Storage, External Access\n\u2714\nFor additional caveats and details on the supported types, see\nIntegration replication\n.\nRequires Business Critical Edition (or higher).\nNetwork policies\n\u2714\nRequires Business Critical Edition (or higher).\nParameters (account level)\n\u2714\nRequires Business Critical Edition (or higher).\nProgrammatic access tokens for\nusers\n\u2714\nIf users and roles are replicated, programmatic access tokens for users are replicated automatically.\nResource monitors\n\u2714\nResource monitor notifications for non-administrator users are replicated if you include\nusers\nin the group, however account\nadministrator notification settings are not replicated. For more information, see\nReplication of resource monitor email notification settings\n.\nRequires Business Critical Edition (or higher).\nRoles\n\u2714\nIncludes\naccount and database roles\n.\nIncludes privileges granted to roles, as well as roles granted to roles (i.e. hierarchies of roles).\nIf users and roles are replicated, roles granted to users are also replicated.\nThe REPLICATE and FAILOVER privileges are\nnot\nreplicated.\nRequires Business Critical Edition (or higher).\nShares\n\u2714\nRep",
    " are replicated in the suspended state to each target account\nand can be resumed in the target account.\nWorkspace replication\n\u00b6\nPreview Feature\n\u2014 Open\nAvailable to all accounts.\nRequires Business Critical Edition (or higher). To inquire about upgrading, please contact\nSnowflake Support\n.\nShared workspaces\nare replicated when they are included in a database that is part of a\nreplication or failover group. Private workspaces are replicated when their owning users are replicated. In secondary (target) accounts,\nreplicated content is read-only; Workspace files (including SQL files, Notebook files, and so on) are executable but cannot be edited.\nDataset replication\n\u00b6\nAccount replication supports replicating Datasets. Datasets are materialized data objects that you use with Snowflake ML.\nFor usage information, see\nSnowflake Datasets\n.\nReplication is supported for Datasets created starting with the General Availability of the Dataset replication feature.\nFor the release announcement, see\nMar 20, 2025: Snowflake Datasets (General availability)\n.\nCortex Search Service replication\n\u00b6\nBusiness Critical Feature\nRequires Business Critical Edition (or higher).\nThe feature supports replicating Cortex Search Services.\nFor more information, see\nReplicate a Cortex Search Service\n.\nReplication of roles and grants\n\u00b6\nBusiness Critical Feature\nRequires Business Critical Edition (or higher).\nIn order to replicate grants on objects to roles, roles must be replicated from the source account to the target account. To\nreplicate roles in a replication or failover group, you must include\nroles\nin the\nobject_types\nlist. Roles can be in a\nseparate replication or failover group from the data objects on which the privileges are granted.\nWhen\nroles\nare replicated, grants on objects are\nonly\nreplicated to a target account if:\nThe privilege was granted by the owner of the object or indirectly by a role that was granted the privilege with the\nWITH GRANT OPTION\nparameter by the owner of the object.\nBoth the grantee and grantor role for a privilege grant are located in the target account.\nThe object is replicated (i.e. the object type is included in the\nobject_types\nlist).\nOtherwise the grant on the object is not replicated.\nFor information about replicating secondary roles and session policies, see\nSession policies with secondary roles\n.\nNote\nIf a role is dropped that has the OWNERSHIP privilege on an active pipe in the target",
    "\nparameter by the owner of the object.\nBoth the grantee and grantor role for a privilege grant are located in the target account.\nThe object is replicated (i.e. the object type is included in the\nobject_types\nlist).\nOtherwise the grant on the object is not replicated.\nFor information about replicating secondary roles and session policies, see\nSession policies with secondary roles\n.\nNote\nIf a role is dropped that has the OWNERSHIP privilege on an active pipe in the target account, the refresh operation\nfails.\nPrivileges on replication groups and failover groups are\nnot\nreplicated. If the REPLICATE or FAILOVER privilege has been granted on replication groups or failover groups, these\nprivileges need to be granted in both the source\nand\ntarget accounts. Refer to\nReplication privileges\nfor details on these privileges.\nGrants for database objects\n\u00b6\nIf\nroles\nand\ndatabases\nare replicated to a target account (in the same or different replication or\nfailover group), refreshing a secondary database synchronizes the privilege grants on the database and the objects in the database\n(schemas, tables, views, etc.) to existing roles in the target account (i.e. roles that have been replicated to the target account).\nNote that only privilege grants on objects supported by database replication are synchronized. For the list of supported objects,\nsee\nReplicated database objects\n.\nExternal tables are not currently supported for replication. As a result, privilege grants on external tables are\nalso not replicated.\nFuture grants for objects\n\u00b6\nIf roles are replicated to the target account,\nfuture grants\nthat are granted at the\ndatabase or schema level are replicated to the target account. This also includes future grants on non-replication supported objects. For\nexample, external table replication is not yet supported, however future grants on external tables are replicated.\nWhen you create an external table in a target account, the privileges granted on future external tables materialize as intended.\nObject creation and ownership\n\u00b6\nIf new objects are created in a target account during a refresh from the source account, and roles are not replicated to the target\naccount, the OWNERSHIP privilege for the new objects is granted to the GLOBALORGADMIN role.\nIf roles are replicated to the target account, the OWNERSHIP privilege is granted to the same role on the target account as the\nrole with the OWNERSHIP privilege in the source account when roles",
    " external tables materialize as intended.\nObject creation and ownership\n\u00b6\nIf new objects are created in a target account during a refresh from the source account, and roles are not replicated to the target\naccount, the OWNERSHIP privilege for the new objects is granted to the GLOBALORGADMIN role.\nIf roles are replicated to the target account, the OWNERSHIP privilege is granted to the same role on the target account as the\nrole with the OWNERSHIP privilege in the source account when roles are next replicated. The roles may be replicated at the same\ntime the new objects are created in the target account if the objects and roles are in the same replication (or failover) group.\nGrants for shares\n\u00b6\nIn order to enable secure data sharing, grants on objects to shares are replicated even if\nroles\nare not\nreplicated to target accounts. This section provides information on how grants on objects to shares are replicated.\nIf\nroles\nare replicated from the source account to the target account, grants to objects on shares are replicated if:\nThe grantor role exists in the target account\nor\nThe grantor role in the source account has the OWNERSHIP privilege on the primary object.\nIf\nroles\nare not replicated from the source account to the target account, then:\nGrants on objects to shares are replicated.\nThe grantor role for grants on replicated objects to shares is the role with the OWNERSHIP privilege on the object.\nUser who refreshes objects in a target account\n\u00b6\nA user who executes the\nALTER FAILOVER GROUP \u2026 REFRESH\ncommand to refresh objects in a target account\nfrom the source account must use a role with the REPLICATE privilege on the failover group. Snowflake protects this user in the target account\nby failing in the following scenarios:\nIf the user does not exist in the source account, the refresh operation fails.\nIf the user exists in the source account, but a role with the REPLICATE privilege was not granted to the user, the refresh operation fails.\nReplication schedule\n\u00b6\nAs a best practice, Snowflake recommends scheduling automatic refreshes using the REPLICATION_SCHEDULE parameter. The schedule can be\ndefined when creating a new replication or failover group with CREATE\n<object>\nor later (using ALTER\n<object>\n).\nWhen you create a secondary replication or failover group, Snowflake automatically executes an initial refresh. The next refresh is\nscheduled based on when the prior refresh started",
    "PLICATE privilege was not granted to the user, the refresh operation fails.\nReplication schedule\n\u00b6\nAs a best practice, Snowflake recommends scheduling automatic refreshes using the REPLICATION_SCHEDULE parameter. The schedule can be\ndefined when creating a new replication or failover group with CREATE\n<object>\nor later (using ALTER\n<object>\n).\nWhen you create a secondary replication or failover group, Snowflake automatically executes an initial refresh. The next refresh is\nscheduled based on when the prior refresh started and the scheduling interval, or the next valid time based on the cron expression. For\nexample, if the refresh schedule interval is 10 minutes and the prior refresh operation (either a scheduled refresh or manually triggered\nrefresh) starts at 12:01, the next refresh is scheduled for 12:11.\nSnowflake ensures only one refresh is executed at any given time. If a refresh is still executing when the next refresh is scheduled, the\nnext refresh is delayed to start when the currently executing refresh completes. For example, if a refresh is scheduled to execute 15\nminutes after the hour, every hour, and the prior refresh completes at 12:16, the next refresh is scheduled to execute when the previously\nexecuting refresh is completed.\nNote\nAutomatically scheduled refresh operations are executed using the role with the OWNERSHIP privilege on the replication\nor failover group. If a scheduled refresh operation fails due to insufficient privileges, grant the required privileges\nto the role with the OWNERSHIP privilege on the group.\nSuspend and resume scheduled replication\n\u00b6\nA secondary failover group cannot be promoted to the primary group while a refresh is executing. To fail over gracefully, suspend scheduled\nreplication in the target account. After the failover is completed, resume the scheduled replication. For more information,\nsee\nALTER FAILOVER GROUP\n.\nReplication to accounts on lower editions\n\u00b6\nIf either of the following conditions is true, Snowflake displays an error message:\nA primary replication group with only database and/or share objects is in a Business Critical (or higher) account but one\nor more of the accounts approved for replication are on lower editions. Business Critical Edition is intended for Snowflake\naccounts with extremely sensitive data.\nA primary replication or failover group with any\nobject types\nis in a\nBusiness Critical (or higher) account and a signed business associate agreement is in place to store PHI data in the account per HIPAA\nand",
    " error message:\nA primary replication group with only database and/or share objects is in a Business Critical (or higher) account but one\nor more of the accounts approved for replication are on lower editions. Business Critical Edition is intended for Snowflake\naccounts with extremely sensitive data.\nA primary replication or failover group with any\nobject types\nis in a\nBusiness Critical (or higher) account and a signed business associate agreement is in place to store PHI data in the account per HIPAA\nand\nHITRUST CSF\nregulations. However, no such agreement is in place for one or more of the accounts\nenabled for replication, regardless if they are Business Critical (or higher) accounts.\nThis behavior is implemented in an effort to help prevent account administrators for Business Critical (or higher) accounts from\ninadvertently replicating sensitive data to accounts on lower editions.\nAn account administrator (a user with the ACCOUNTADMIN role) or a user with a role with the\nCREATE REPLICATION GROUP/CREATE FAILOVER GROUP or OWNERSHIP privilege can override this default behavior by including the\nIGNORE EDITION CHECK clause when executing the CREATE\n<object>\nor ALTER\n<object>\nstatement. If IGNORE EDITION CHECK is set, the primary replication or failover group may be replicated to the specified accounts on\nlower Snowflake editions in these specific scenarios.\nNote\nFailover groups can only be created in a Business Critical Edition (or higher) account. Therefore failover groups can\nonly\nbe\nreplicated to an account that is a Business Critical Edition (or higher) account.\nCurrent limitations of replication\n\u00b6\nDatabases created from shares cannot be replicated.\nRefresh operations fail if the primary database includes a stream with an unsupported source object.\nThe operation also fails if the source object for any stream has been dropped.\nAppend-only streams are not supported on replicated source objects.\nNote\nDatabase replication does not work for task graphs if the graph is owned by a different role than the role that performs replication.\nOn this page\nRegion support for replication and failover/failback\nReplication groups and failover groups\nReplication feature / edition matrix\nReplicated objects\nDatabase replication\nExternal volume replication\nIntegration replication\nNetwork policy replication\nParameter replication\nResource monitor replication\nRole replication\nShare replication\nBackup replication for database, schema, and table backups\nUser replication\nWarehouse replication\nWorkspace replication\nDataset replication\nCortex Search Service replication\nReplication of",
    " than the role that performs replication.\nOn this page\nRegion support for replication and failover/failback\nReplication groups and failover groups\nReplication feature / edition matrix\nReplicated objects\nDatabase replication\nExternal volume replication\nIntegration replication\nNetwork policy replication\nParameter replication\nResource monitor replication\nRole replication\nShare replication\nBackup replication for database, schema, and table backups\nUser replication\nWarehouse replication\nWorkspace replication\nDataset replication\nCortex Search Service replication\nReplication of roles and grants\nUser who refreshes objects in a target account\nReplication schedule\nSuspend and resume scheduled replication\nReplication to accounts on lower editions\nCurrent limitations of replication\nRelated content\nReplication considerations\nReplicating databases and account objects across multiple accounts\nFailing over account objects",
    ", see\nReplication of resource monitor email notification settings\n.\nRequires Business Critical Edition (or higher).\nRoles\n\u2714\nIncludes\naccount and database roles\n.\nIncludes privileges granted to roles, as well as roles granted to roles (i.e. hierarchies of roles).\nIf users and roles are replicated, roles granted to users are also replicated.\nThe REPLICATE and FAILOVER privileges are\nnot\nreplicated.\nRequires Business Critical Edition (or higher).\nShares\n\u2714\nReplication of\ninbound shares\n(shares from providers) is\nnot\nsupported.\nUsers\n\u2714\nRequires Business Critical Edition (or higher).\nWarehouses\n\u2714\nRequires Business Critical Edition (or higher). Currently,\ninteractive warehouses\nare\nnot\nreplicated.\nWorkspaces\n\u2714\nRequires Business Critical Edition (or higher).\nDatabase replication\n\u00b6\nSnowflake account replication supports replicating databases. Replication for a database includes the objects contained in that\ndatabase. The refresh operation for a database includes changes to the objects and data since the previous refresh for that database.\nIf\nroles\nare replicated (in the same or different replication or failover group), the database refresh also synchronizes the\nprivilege grants on the secondary database and the objects in the database (schemas, tables, views, etc.) to roles in the account.\nRefer to\nGrants for database objects\nfor more details.\nReplication of some databases is not supported or might fail the refresh operation. For more information, see\nCurrent limitations of replication\n.\nReplicated database objects\n\u00b6\nWhen a primary database is replicated, a snapshot of its database objects and data is transferred to the secondary database. However,\nsome database objects are not replicated. The following table indicates which database objects are replicated to a secondary database.\nFor specific usage information about these objects, see\nReplication considerations\n.\nNote\nObjects that are\nnot\nsupported for replication are skipped during replication and won\u2019t be available in the target account post failover.\nObject\nType or Feature\nReplicated\nNotes\nSchemas\n\u2714\nBy default, all schemas in replicated databases are replicated. If you use failover groups, you can choose\nwhich schemas within a database are replicated. For more information, see\nSchema-level replication for failover groups\n.\nTables\nPermanent tables\n\u2714\nTransient tables\n\u2714\nTemporary tables\nAutomatic Clustering of clustered tables\n\u2714\nDynamic tables\n\u2714\nFor more information,",
    " target account post failover.\nObject\nType or Feature\nReplicated\nNotes\nSchemas\n\u2714\nBy default, all schemas in replicated databases are replicated. If you use failover groups, you can choose\nwhich schemas within a database are replicated. For more information, see\nSchema-level replication for failover groups\n.\nTables\nPermanent tables\n\u2714\nTransient tables\n\u2714\nTemporary tables\nAutomatic Clustering of clustered tables\n\u2714\nDynamic tables\n\u2714\nFor more information, see\nReplication and dynamic tables\n.\nExternal tables\nHybrid tables\nApache Iceberg\u2122 tables\n\u2714\nOnly Snowflake-managed Iceberg tables are supported. Replication for Iceberg tables requires external volume replication.\nFor more information, see\nConfigure replication for Snowflake-managed Apache Iceberg\u2122 tables\n.\nInteractive tables\nCurrently,\ninteractive tables\naren\u2019t replicated.\nTable constraints\n\u2714\nExcept if a foreign key in the database references a primary/unique key in another database.\n.\nEvent tables\nSequences\n\u2714\nViews\nViews\n\u2714\nIf a view references any object in another database (e.g. table columns, other views, UDFs, or stages),\n.\nboth databases must be replicated.\nMaterialized views\n\u2714\nSecure views\n\u2714\nSemantic views\nFile formats\n\u2714\nStages\nStages\n\u2714\nSupported for replication and failover groups only. Not supported for database replication.\n.\nFor more information, see\nStage, pipe, and load history replication\n.\nTemporary stages\nPipes\n\u2714\nSupported for replication and failover groups only. Not supported for database replication.\n.\nFor more information, see\nStage, pipe, and load history replication\n.\nStored procedures\n\u2714\nFor more information, see\nReplication of stored procedures and user-defined functions (UDFs)\n.\nStreams\n\u2714\nFor more information, see\nReplication and streams\n.\nTasks\n\u2714\nFor more information, see\nReplication and tasks\n.\nData metric functions (DMFs)\nData Quality\n\u2714\nFor more information, see\nReplication of data metric functions (DMFs)\n.\nUDFs\n\u2714\nFor more information, see\nReplication of stored procedures and user-defined functions (UDFs)\n.\nPolicies\nAggregation policies\n\u2714\nAuthentication policies\n\u2714\nColumn-level Security (masking)\n\u2714\nFor masking, row access, and tag-based masking policies, see\npolicy replication considerations\n",
    "lication and tasks\n.\nData metric functions (DMFs)\nData Quality\n\u2714\nFor more information, see\nReplication of data metric functions (DMFs)\n.\nUDFs\n\u2714\nFor more information, see\nReplication of stored procedures and user-defined functions (UDFs)\n.\nPolicies\nAggregation policies\n\u2714\nAuthentication policies\n\u2714\nColumn-level Security (masking)\n\u2714\nFor masking, row access, and tag-based masking policies, see\npolicy replication considerations\n.\nJoin policies\n\u2714\nPassword policies\n\u2714\nPrivacy policies\n\u2714\nFor more information, see\nPrivacy policies\n.\nProjection policies\n\u2714\nRow access policies\n\u2714\nSession policies\n\u2714\nFor session, password, and authentication policies, see\nreplication and security policies\n.\nTag-based masking policies\n\u2714\nBackup policies\n\u2714\nBackups\nare available for all Snowflake editions.\nBackups with retention lock and backups with legal holds are available for Business Critical Edition (or higher).\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nStorage lifecycle policies\n\u2714\nFor information about replication of policies and archived data, see\nReplication and storage lifecycle policies\n.\nTags\nObject Tagging\n\u2714\nFor tags, see\nReplication and tags\n.\nAlerts\n\u2714\nSecrets\nSecrets for External API Authentication\n\u2714\nYou can replicate secrets by using a replication group and failover group. For additional details, see\nReplication and secrets\n.\nNetwork rules\n\u2714\nFor replication of network policies that use network rules, see\nReplicating network policies\n.\nBackups\nBackup sets\n\u2714\nBackups\nare available for all Snowflake editions.\nBackups with retention lock and backups with legal holds are available for Business Critical Edition (or higher).\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nClass instances\nCUSTOM_CLASSIFIER\n\u2714\nReplication is supported for instances of the\nCUSTOM_CLASSIFIER\nclass. Instances\nof all other Snowflake\nclasses\nare\nnot\nreplicated. For the full list of Snowflake\nclasses, see\nAvailable classes\n.\nPackages policies\nPython UDF, UDTF, stored procedures\n\u2714\nIf there is a\npackages policy\nset on the source account, in order\nto successfully replicate account objects, the database containing the packages policy\nmust\nbe replicated to the target account\nin the same or different replication or fail",
    "IFIER\nclass. Instances\nof all other Snowflake\nclasses\nare\nnot\nreplicated. For the full list of Snowflake\nclasses, see\nAvailable classes\n.\nPackages policies\nPython UDF, UDTF, stored procedures\n\u2714\nIf there is a\npackages policy\nset on the source account, in order\nto successfully replicate account objects, the database containing the packages policy\nmust\nbe replicated to the target account\nin the same or different replication or failover group. Otherwise, the refresh operation fails with a\ndangling references error\n.\nObjects for machine learning workflows\nModels\n\u2714\nFor usage information, see\nSnowflake Model Registry\n.\nDatasets\n\u2714\nFor information about how replication works for Datasets, see\nDataset replication\n.\nOnline feature tables\nOnline feature tables do not support replication or cloning.\nGit repository clones\n\u2714\nFor information about how replication works for Git repository clones, see\nGit repository replication\n.\nFor usage information for Git repository clones, see\nUsing a Git repository in Snowflake\n.\nSnowflake Notebooks\n\u2714\nFor information about how replication works for Snowflake Notebooks, see\nNotebook replication\n.\nDatabase replication and encryption\n\u00b6\nSnowflake protects metadata and data sets at rest and in transit between the source and target accounts. The account\nmaster key\n(AMK) encrypts the key hierarchy within the account as shown in the\nhierarchical key model\n. Snowflake encrypts replicated data in the target account using the\naccount master key and the key hierarchy in the target account, regardless of whether you enable Tri-Secret Secure in the target account.\nWhen you enable Tri-Secret Secure in the target account, Snowflake uses the composite master key and the corresponding key hierarchy in\nthe target account to encrypt the data. Note that target accounts do not have Tri-Secret Secure enabled by default; you must enable this\nfeature.\nFor more information about data encryption in Snowflake, see\nUnderstanding end-to-end encryption in Snowflake\n.\nExternal volume replication\n\u00b6\nPreview Feature\n\u2014 Open\nAvailable to all accounts.\nStandard & Business Critical Feature\nReplication of external volumes to a replication group is available to all accounts.\nReplication of external volumes to a failover group requires Business Critical Edition or later.\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nIceberg tables rely on external volumes, which are\naccount-level objects that require",
    " in Snowflake, see\nUnderstanding end-to-end encryption in Snowflake\n.\nExternal volume replication\n\u00b6\nPreview Feature\n\u2014 Open\nAvailable to all accounts.\nStandard & Business Critical Feature\nReplication of external volumes to a replication group is available to all accounts.\nReplication of external volumes to a failover group requires Business Critical Edition or later.\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nIceberg tables rely on external volumes, which are\naccount-level objects that require extra configuration to connect to your external cloud storage. Before you can replicate an Iceberg table,\nyou must configure replication for external volumes. Account replication supports the replication of external volumes. For more\ninformation about replicating external volumes and Snowflake-managed Iceberg tables, see\nConfigure replication for Snowflake-managed Apache Iceberg\u2122 tables\n.\nFor more information about external volumes, see\nExternal volume\n.\nIntegration replication\n\u00b6\nBusiness Critical Feature\nRequires Business Critical Edition (or higher). To inquire about upgrading, please contact\nSnowflake Support\n.\nAccount replication supports the replication of integrations for the following features:\nSecurity integrations of the following types:\nFederated Authentication & SSO (i.e. SAML2)\nSCIM\nSnowflake OAuth\nExternal OAuth\nFor more information about security integrations, see\nReplication of security integrations & network policies across multiple accounts\n.\nAPI integrations.\nAfter replicating API integrations to a target account, you must grant access to the remote service to the replicated\nexternal functions. For more information, see\nUpdating the remote service for API integrations\n.\nNotification integrations of the following types:\nTYPE = EMAIL\nTYPE = QUEUE with DIRECTION = OUTBOUND\nTYPE = WEBHOOK\nStorage integrations.\nWhen you replicate a storage integration, you must establish a new trust relationship for your cloud storage in the target\naccounts. To learn more, see\nConfigure cloud storage access for secondary storage integrations\n.\nExternal access integrations.\nFor more information about external access integrations, see\nExternal network access overview\n.\nNetwork policy replication\n\u00b6\nBusiness Critical Feature\nRequires Business Critical Edition (or higher).\nThe feature supports replicating network policies.\nFor more information, see\nReplication of security integrations & network policies across multiple accounts\n.\nParameter replication\n\u00b6\nBusiness Critical Feature\nRequires Business Critical Edition (or higher).\nThis feature supports replicating account-level parameters and object parameters. Object parameters",
    ".\nExternal access integrations.\nFor more information about external access integrations, see\nExternal network access overview\n.\nNetwork policy replication\n\u00b6\nBusiness Critical Feature\nRequires Business Critical Edition (or higher).\nThe feature supports replicating network policies.\nFor more information, see\nReplication of security integrations & network policies across multiple accounts\n.\nParameter replication\n\u00b6\nBusiness Critical Feature\nRequires Business Critical Edition (or higher).\nThis feature supports replicating account-level parameters and object parameters. Object parameters are replicated when the object is\nincluded in the replication group. For example, if\nWAREHOUSES\nare replicated, warehouse-specific parameters\n(e.g.\nSTATEMENT_TIMEOUT_IN_SECONDS\n) are replicated. For a full list, see\nObject parameters\n.\nAccount-level parameter replication includes all\nAccount parameters\nand\nparameters set on the account\n.\nAccount-level parameters (e.g.\nDATA_RETENTION_TIME_IN_DAYS\n) are replicated when\nACCOUNT\nPARAMETERS\nis included in\nthe list of object types for a replication group.\nResource monitor replication\n\u00b6\nBusiness Critical Feature\nRequires Business Critical Edition (or higher).\nThis feature supports replicating resource monitors and privileges granted on resource monitors to roles. A secondary resource monitor\nfollows the same quota reset schedule as its primary. For example, if the quota on the primary resource monitor resets on the first of the\nmonth, and the secondary is first replicated on the 15th of the month, its quota will reset on the first of the next month along with the\nprimary.\nReplication of resource monitor email notification settings\n\u00b6\nEmail notification settings for resource monitors are not included with resource monitor replication. Email notifications for\nnon-administrator users can be replicated with resource monitors. However, account administrator notification settings are\ncurrently not replicated:\nIf\nusers\nand\nresource\nmonitors\nare included in the\nobject_types\nlist for the replication or failover group,\nnotification settings for non-administrator users are replicated:\nThe\nnotify_users\nlist for a warehouse-level resource monitor is replicated to target accounts.\nEmail notifications for non-administrator users\nare sent\non the target account.\nIf\nresource\nmonitors\nis included in the\nobject_types\nlist for the replication or failover group, but\nusers\nis not included, the\nnotify_users\nlist for a secondary warehouse-level resource monitor is empty.\nAccount administrator notification settings are\nnot\nreplicated:\nAn account administrator must\n",
    " replicated:\nThe\nnotify_users\nlist for a warehouse-level resource monitor is replicated to target accounts.\nEmail notifications for non-administrator users\nare sent\non the target account.\nIf\nresource\nmonitors\nis included in the\nobject_types\nlist for the replication or failover group, but\nusers\nis not included, the\nnotify_users\nlist for a secondary warehouse-level resource monitor is empty.\nAccount administrator notification settings are\nnot\nreplicated:\nAn account administrator must\nenable email notifications\nin each account using the web interface.\nResource monitor notifications are sent to account administrators if they have enabled email notifications in the source and/or\ntarget accounts.\nRole replication\n\u00b6\nBusiness Critical Feature\nRequires Business Critical Edition (or higher).\nThis feature supports replicating roles, including role hierarchies. Role objects must be replicated to replicate access privileges.\nReplicated access privileges are listed in\nReplication of roles and grants\nbelow.\nNote\nAll roles are replicated.\nShare replication\n\u00b6\nThis feature supports replication of share objects as well as access privileges granted to shares on database objects.\nReplication of\ninbound shares\n(shares from providers) is\nnot\nsupported.\nBackup replication for database, schema, and table backups\n\u00b6\nThe Snowflake\nbackups\nfeature lets you encapsulate a series of backups for a specific database,\nschema, or table inside an object known as a backup set. You can optionally control the schedule of automatic backups and\nautomatic deletion of backups after an expiry period by applying a backup policy to the backup set. Backup sets and\nbackup policies are database-level objects. Snowflake replicates those objects along with the databases and schemas that contain them.\nFor information about how Snowflake replicates backup sets and backup policies, see\nReplicate backup-related objects\n.\nUser replication\n\u00b6\nBusiness Critical Feature\nRequires Business Critical Edition (or higher).\nThis feature supports replicating users and their properties to target accounts, the following user authentication methods, and provisioning\nusers and groups with SCIM:\nAuthentication Method\nWorks in Target Accounts\nNotes\nPassword\n\u2714\nPassword with MFA (multi-factor authentication)\n\u2714\nUsers who are enrolled in MFA in the source account must separately enroll in MFA when they log in to each target account.\nMulti-factor authentication (MFA)\n\u2714\nUsers who are enrolled in MFA in the source account must separately enroll in MFA when they log in to each target account.\n",
    " user authentication methods, and provisioning\nusers and groups with SCIM:\nAuthentication Method\nWorks in Target Accounts\nNotes\nPassword\n\u2714\nPassword with MFA (multi-factor authentication)\n\u2714\nUsers who are enrolled in MFA in the source account must separately enroll in MFA when they log in to each target account.\nMulti-factor authentication (MFA)\n\u2714\nUsers who are enrolled in MFA in the source account must separately enroll in MFA when they log in to each target account.\nKey-pair authentication\n\u2714\nProgrammatic access tokens\n\u2714\nProgrammatic access tokens are replicated to the target account only if users and roles are replicated.\nFederated Authentication\n\u2714\nRefer to\nReplication of security integrations & network policies across multiple accounts\nfor details on replicating federated SSO (i.e. SAML2) security integrations.\nSnowflake OAuth\n\u2714\nRefer to\nReplication of security integrations & network policies across multiple accounts\nfor details on replicating OAuth security integrations.\nExternal OAuth\n\u2714\nRefer to\nReplication of security integrations & network policies across multiple accounts\nfor details on replicating OAuth security integrations.\nSCIM\n\u2714\nRefer to\nReplication of security integrations & network policies across multiple accounts\nfor details on replicating SCIM security integrations.\nNote\nIf\nUSERS\nand\nROLES\nobjects are replicated to a target account, these object types are read-only in the target account\nand cannot be modified. Users and roles must be created in the source account, then replicated to each target account. Refer to\nReplication and read-only secondary objects\n.\nWarehouse replication\n\u00b6\nBusiness Critical Feature\nRequires Business Critical Edition (or higher). To inquire about upgrading, please contact\nSnowflake Support\n.\nThis feature supports replicating warehouses and privileges granted on warehouses to roles (if\nroles\nare replicated).\nThe state of the primary warehouse is not replicated. Warehouses are replicated in the suspended state to each target account\nand can be resumed in the target account.\nWorkspace replication\n\u00b6\nPreview Feature\n\u2014 Open\nAvailable to all accounts.\nRequires Business Critical Edition (or higher). To inquire about upgrading, please contact\nSnowflake Support\n.\nShared workspaces\nare replicated when they are included in a database that is part of a\nreplication or failover group. Private workspaces are replicated when their owning users are replicated. In secondary (target)",
    "Monitoring replication and failover\n\u00b6\nStandard & Business Critical Feature\nDatabase and share replication are available to all accounts.\nReplication of other account objects & failover/failback require Business Critical Edition (or higher).\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nThis topic provides information on how to monitor account replication progress, history, and costs.\nUse Snowsight to monitor replication\n\u00b6\nTo monitor the replication progress and status for\nreplication and failover groups\nin an\norganization, use the\nReplication\npage in Snowsight.\nYou can view the status and details of refresh operations, including:\nCurrent status of the most recent refresh operation.\nReplica lag time (time since the last refresh operation).\nDistribution of replica lag times across groups.\nDate and time of the next scheduled refresh operation.\nNote\nSnowsight lists the replication and failover groups for which your role has the MONITOR, OWNERSHIP, or REPLICATE privilege on.\nRefresh operation details are only available to users with the ACCOUNTADMIN role or the OWNERSHIP privilege on the group.\nYou must be signed in to the source or target account to view refresh operation details. If you are not, you will be prompted to sign in.\nBoth the source account and the target account must use the same connection type (public internet). Otherwise, signing in to\nthe target account fails.\nCurrently, if your account uses private connectivity, you can\u2019t use Snowsight to create or modify groups or connection\nobjects. However, you can use Snowsight to monitor groups that were created using SQL.\nTo view the replication status of each replication or failover group, complete the following steps:\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\nand then select\nGroups\n.\nThe\nGroups\npage displays\nrefresh operation details\nfor all the groups for which your\nrole has a privilege to view. You can use the tiles to filter the view.\nFor example, if the\nStatus\ntile indicates there are failed refresh operations, you can select the tile to investigate the group(s)\nwith failures.\nThe lag time in the\nLongest Replication lag\ntile refers to the duration of time since the last refresh operation. This is the\nlength of time that the secondary replication or failover group\nlags\nbehind the primary group. The longest lag time is the\n",
    " privilege to view. You can use the tiles to filter the view.\nFor example, if the\nStatus\ntile indicates there are failed refresh operations, you can select the tile to investigate the group(s)\nwith failures.\nThe lag time in the\nLongest Replication lag\ntile refers to the duration of time since the last refresh operation. This is the\nlength of time that the secondary replication or failover group\nlags\nbehind the primary group. The longest lag time is the\nlength of time since the oldest secondary replication group was last refreshed.\nFor example, if you have three failover groups,\nfg_1\n,\nfg_2\n,\nfg_3\n, with independent replication schedules of\n10 minutes, 2 hours, and 12 hours respectively, the longest lag time could be as long as 12 hours. If\nfg_3\n, however, was\nrecently refreshed in the target account, its lag time resets to 0 and a different failover group could have a longer lag time.\nYou can select an individual bar in the\nGroup Lag Distribution\ntile to filter the results to an individual group.\nYou can also filter groups by using the search field or the dropdown menus:\nYou can search by replication or failover group name using the\n(search) box.\nChoose\nType\nto filter the results by replication or failover group.\nChoose\nReplicating\nto filter by primary (select\nTo\n) or secondary groups (select\nFrom\n).\nChoose the\n(accounts) menu to filter the results by account name.\nChoose\nStatus\nto filter results by refresh operation status:\nRefresh Cancelled\nRefresh Failed\nRefresh In Progress\nRefresh Successful\nYou can see the following details about your replication and failover groups:\nColumn\nDescription\nName\nName of the replication or failover group.\nIs Replicating\nIndicates if the group is being replicated\nto\na target account or\nfrom\na source account.\nIf this column contains\ndestinations available\n, there are no secondary replication or failover groups.\nThe number of destinations available indicates the number of target accounts the primary group can be replicated to.\nStatus\nDisplays the status of the latest refresh operation.\nYou must be signed in to the source or target account in order to access replication details.\nIf you are not signed in, select\nSign in\nto view refresh operation status for the secondary group.\nBoth the source account and the target account must",
    ".\nIf this column contains\ndestinations available\n, there are no secondary replication or failover groups.\nThe number of destinations available indicates the number of target accounts the primary group can be replicated to.\nStatus\nDisplays the status of the latest refresh operation.\nYou must be signed in to the source or target account in order to access replication details.\nIf you are not signed in, select\nSign in\nto view refresh operation status for the secondary group.\nBoth the source account and the target account must use the same connection type (public internet). Otherwise, signing in\nto the target account fails.\nReplication Lag\nThe length of time since the last refresh operation. This is the length of time that the secondary replication group \u201clags\u201d behind\nthe primary replication group.\nNext Refresh\nThe date and time of the next scheduled refresh operation.\nYou can select a replication or failover group to view detailed information about each refresh operation. For more information, see\nthe section on replication history in Snowsight\n.\nMonitor the progress of refresh operations\n\u00b6\nThis section provides information on how to monitor replication progress for a specific replication or failover group using either\nSnowsight or SQL.\nUse Snowsight to monitor the progress of refresh operations\n\u00b6\nYou can view the status of a refresh operation in progress and the details of historical refresh operations using Snowsight.\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\n, select\nGroups\n.\nSelect the name of a replication or failover group.\nTip\nIf your account uses private connectivity, you can still use Snowsight to monitor groups.\nAlthough creating or modifying groups or connection objects through Snowsight isn\u2019t currently available\nwith private connectivity, Snowsight can monitor the groups that you create using SQL.\nFor more information about the detailed view, see\nthe section on replication history in Snowsight\n.\nUse SQL to monitor the progress of refresh operations\n\u00b6\nTo monitor the progress of a replication or failover group refresh, query the\nREPLICATION_GROUP_REFRESH_PROGRESS, REPLICATION_GROUP_REFRESH_PROGRESS_BY_JOB, REPLICATION_GROUP_REFRESH_PROGRESS_ALL\ntable function (in the\nSnowflake Information Schema\n).\nExample\n\u00b6\nView the progress of the most recent refresh operation for the failover group\nmyfg\n:\nSELECT\nphase_name\n,\nstart_time\n,\nend_time\n,\nprogress\n,\ndetails\n",
    " monitor the progress of refresh operations\n\u00b6\nTo monitor the progress of a replication or failover group refresh, query the\nREPLICATION_GROUP_REFRESH_PROGRESS, REPLICATION_GROUP_REFRESH_PROGRESS_BY_JOB, REPLICATION_GROUP_REFRESH_PROGRESS_ALL\ntable function (in the\nSnowflake Information Schema\n).\nExample\n\u00b6\nView the progress of the most recent refresh operation for the failover group\nmyfg\n:\nSELECT\nphase_name\n,\nstart_time\n,\nend_time\n,\nprogress\n,\ndetails\nFROM\nTABLE\n(\nINFORMATION_SCHEMA\n.\nREPLICATION_GROUP_REFRESH_PROGRESS\n(\n'myfg'\n));\nCopy\nView replication history\n\u00b6\nYou can view replication history using Snowsight or using\nSQL\n.\nNote\nYou can view the replication history for the replication and failover groups for which your role has the MONITOR, OWNERSHIP, or\nREPLICATE privilege on.\nUse Snowsight to view replication history\n\u00b6\nYou can view the replication history and details for each refresh operation for a specific replication or failover group in the details\npage for the group.\nSign in to\nSnowsight\n.\nIn the navigation menu, select\nAdmin\n\u00bb\nAccounts\n.\nSelect\nReplication\n, select\nGroups\n.\nSelect the name of a replication or failover group.\nYou can review the following information about the group:\nGroup type (replication group or failover group).\nReplication schedule (for example, every 10 minutes).\nDuration of each refresh operation.\nReplica lag time (length of time since last refresh operation).\nDate and time of the next scheduled refresh operation.\nTip\nIf your account uses private connectivity, you can still use Snowsight to monitor groups.\nAlthough creating or modifying groups or connection objects through Snowsight isn\u2019t currently available\nwith private connectivity, Snowsight can monitor the groups that you create using SQL.\nYou can filter the data on the page by status and time period:\nChoose\nStatus\nto filter results by refresh operation status:\nRefresh Cancelled\nRefresh Failed\nRefresh In Progress\nRefresh Successful\nChoose\nDuration\nto show refresh operation details for:\nLast hour\nLast 24 hours\nLast 7 days\nAll\nSelecting\nAll\ndisplays the last 14 days of refresh operations.\nThe details for each refresh operation include the following columns:\nColumn\nDescription\nQuery ID\nQuery ID of the refresh operation.\nStatus\nDisplays the status of the refresh operation. Valid values",
    " filter results by refresh operation status:\nRefresh Cancelled\nRefresh Failed\nRefresh In Progress\nRefresh Successful\nChoose\nDuration\nto show refresh operation details for:\nLast hour\nLast 24 hours\nLast 7 days\nAll\nSelecting\nAll\ndisplays the last 14 days of refresh operations.\nThe details for each refresh operation include the following columns:\nColumn\nDescription\nQuery ID\nQuery ID of the refresh operation.\nStatus\nDisplays the status of the refresh operation. Valid values include\nSuccessful\n,\nFailed\n,\nIn\nProgress\n.\nEnded\nDate and time the refresh operation ended.\nDuration\nThe length of time the refresh operation took to complete.\nThe duration period is broken down and color coded by\nreplication phase\n. The width\nof each colored segment indicates the portion of the time spent in that phase.\nThe image below is for reference only. This graph is available when you\nselect the refresh operation\nfor additional details.\nTransferred\nThe number of bytes replicated.\nObjects\nThe number of objects replicated.\nSelect a row to view additional details about a specific refresh operation including:\nDuration of each replication phase.\nError message (for failed refresh operations).\nList of database objects replicated by type and number.\nNumber of databases replicated and database names.\nUse SQL to view replication history\n\u00b6\nTo view the replication history of a specific replication or failover group within a specified date range, query one of the following:\nREPLICATION_GROUP_REFRESH_HISTORY, REPLICATION_GROUP_REFRESH_HISTORY_ALL\ntable function (in the\nSnowflake Information Schema\n).\nREPLICATION_GROUP_REFRESH_HISTORY view\n(in\nAccount Usage\n).\nExamples\n\u00b6\nQuery the Information Schema REPLICATION_GROUP_REFRESH_HISTORY table function to view the account replication history of failover\ngroup\nmyfg\nin the last 7 days:\nSELECT\nPHASE_NAME\n,\nSTART_TIME\n,\nEND_TIME\n,\nTOTAL_BYTES\n,\nOBJECT_COUNT\nFROM\nTABLE\n(\ninformation_schema\n.\nreplication_group_refresh_history\n(\n'myfg'\n))\nWHERE\nSTART_TIME\n>=\nCURRENT_DATE\n()\n-\nINTERVAL\n'7 days'\n;\nCopy\nQuery the Account Usage REPLICATION_GROUP_REFRESH_HISTORY view to view the account replication history in the current month:\nSELECT\nREPLICATION_GROUP_NAME\n,\nPHASE_NAME\n,\nSTART_TIME\n,\nEND_TIME\n,\nTOTAL_BYTES\n,\nOBJECT_COUNT\nFROM\nsnowflake\n.\naccount_usage\n.\nreplication_group_refresh_history\nWHERE\nSTART_TIME",
    "_refresh_history\n(\n'myfg'\n))\nWHERE\nSTART_TIME\n>=\nCURRENT_DATE\n()\n-\nINTERVAL\n'7 days'\n;\nCopy\nQuery the Account Usage REPLICATION_GROUP_REFRESH_HISTORY view to view the account replication history in the current month:\nSELECT\nREPLICATION_GROUP_NAME\n,\nPHASE_NAME\n,\nSTART_TIME\n,\nEND_TIME\n,\nTOTAL_BYTES\n,\nOBJECT_COUNT\nFROM\nsnowflake\n.\naccount_usage\n.\nreplication_group_refresh_history\nWHERE\nSTART_TIME\n>=\nDATE_TRUNC\n(\n'month'\n,\nCURRENT_DATE\n());\nCopy\nMonitor replication costs\n\u00b6\nTo monitor credit usage for replication, query one of the following:\nREPLICATION_GROUP_USAGE_HISTORY view\n(in\nAccount Usage\n).\nREPLICATION_GROUP_USAGE_HISTORY\ntable function (in the\nSnowflake Information Schema\n).\nExamples\n\u00b6\nQuery the REPLICATION_GROUP_USAGE_HISTORY table function to view credits used for account replication in the last 7 days:\nSELECT\nstart_time\n,\nend_time\n,\nreplication_group_name\n,\ncredits_used\n,\nbytes_transferred\nFROM\nTABLE\n(\ninformation_schema\n.\nreplication_group_usage_history\n(\ndate_range_start\n=>\nDATEADD\n(\n'day'\n,\n-\n7\n,\nCURRENT_DATE\n())));\nCopy\nQuery the Account Usage REPLICATION_GROUP_USAGE_HISTORY view to view the credits used by replication or failover group for account\nreplication history in the current month:\nSELECT\nstart_time\n,\nend_time\n,\nreplication_group_name\n,\ncredits_used\n,\nbytes_transferred\nFROM\nsnowflake\n.\naccount_usage\n.\nreplication_group_usage_history\nWHERE\nstart_time\n>=\nDATE_TRUNC\n(\n'month'\n,\nCURRENT_DATE\n());\nCopy\nMonitor replication costs for databases\n\u00b6\nThe cost for replication for an individual database included in a replication or failover group can be calculated by retrieving the\nnumber of copied bytes for the database and associating it with the credits used.\nExamples\n\u00b6\nQuery Account Usage views\n\u00b6\nThe following examples calculate the costs for database replication in one replication group for the past 30 days.\nQuery the REPLICATION_GROUP_REFRESH_HISTORY Account Usage view and calculate the sum of the number of bytes replicated per database.\nFor example, to calculate the sum of the number of bytes replicated for databases in the replication group\nmyrg\nin the last\n30 days:\nSELECT\nSUM\n(\nvalue\n:totalBytesToReplicate\n)\nas\nsum",
    "\u00b6\nQuery Account Usage views\n\u00b6\nThe following examples calculate the costs for database replication in one replication group for the past 30 days.\nQuery the REPLICATION_GROUP_REFRESH_HISTORY Account Usage view and calculate the sum of the number of bytes replicated per database.\nFor example, to calculate the sum of the number of bytes replicated for databases in the replication group\nmyrg\nin the last\n30 days:\nSELECT\nSUM\n(\nvalue\n:totalBytesToReplicate\n)\nas\nsum_database_bytes\nFROM\nsnowflake\n.\naccount_usage\n.\nreplication_group_refresh_history\nrh\n,\nLATERAL\nFLATTEN\n(\ninput\n=>\nrh\n.\ntotal_bytes\n:databases\n)\nWHERE\nrh\n.\nreplication_group_name\n=\n'MYRG'\nAND\nrh\n.\nstart_time\n>=\nCURRENT_DATE\n()\n-\nINTERVAL\n'30 days'\n;\nCopy\nNote the output of the sum of database bytes:\n+--------------------+\n| SUM_DATABASE_BYTES |\n|--------------------|\n|              22016 |\n+--------------------+\nQuery the REPLICATION_GROUP_USAGE_HISTORY Account Usage view and calculate the sum of the number of credits used and the sum\nof the bytes transferred for replication.\nFor example, to calculate the sum of the number of credits used and the sum of the bytes transferred for replication of the\nreplication group\nmyrg\nin the last 30 days:\nSELECT\nSUM\n(\ncredits_used\n)\nAS\ncredits_used\n,\nSUM\n(\nbytes_transferred\n)\nAS\nbytes_transferred\nFROM\nsnowflake\n.\naccount_usage\n.\nreplication_group_usage_history\nWHERE\nreplication_group_name\n=\n'MYRG'\nAND\nstart_time\n>=\nCURRENT_DATE\n()\n-\nINTERVAL\n'30 days'\n;\nCopy\nNote the output of the sum of the credits used and the sum of bytes transferred:\n+--------------+-------------------+\n| CREDITS_USED | BYTES_TRANSFERRED |\n|--------------+-------------------|\n|  1.357923604 |             22013 |\n+--------------+-------------------+\nCalculate the replication costs for databases using the values of the bytes transferred for databases, sum of the credits used, and\nthe sum of all bytes transferred for replication from the previous two steps:\n(<database_bytes_transferred>\n/\n<bytes_transferred>)\n*\n<credits_used>\nFor example:\n(22016\n/\n22013)\n*\n1.357923604\n=\n1.",
    "+-------------------|\n|  1.357923604 |             22013 |\n+--------------+-------------------+\nCalculate the replication costs for databases using the values of the bytes transferred for databases, sum of the credits used, and\nthe sum of all bytes transferred for replication from the previous two steps:\n(<database_bytes_transferred>\n/\n<bytes_transferred>)\n*\n<credits_used>\nFor example:\n(22016\n/\n22013)\n*\n1.357923604\n=\n1.35810866)\nQuery Information Schema table functions\n\u00b6\nFor refresh operations within the past 14 days, query the associated Information Schema table functions.\nREPLICATION_GROUP_REFRESH_HISTORY, REPLICATION_GROUP_REFRESH_HISTORY_ALL\nREPLICATION_GROUP_USAGE_HISTORY\nQuery the REPLICATION_GROUP_REFRESH_HISTORY table function to view the sum of the number of bytes copied for database replication\nfor the replication group\nmyrg\n:\nSELECT\nSUM\n(\nvalue\n:totalBytesToReplicate\n)\nFROM\nTABLE\n(\ninformation_schema\n.\nreplication_group_refresh_history\n(\n'myrg'\n))\nAS\nrh\n,\nLATERAL\nFLATTEN\n(\ninput\n=>\ntotal_bytes\n:databases\n)\nWHERE\nrh\n.\nphase_name\n=\n'COMPLETED'\nAND\nrh\n.\nstart_time\n>=\nCURRENT_DATE\n()\n-\nINTERVAL\n'14 days'\n;\nCopy\nQuery the REPLICATION_GROUP_USAGE_HISTORY table function to view sum of the number of credits used and the sum of the bytes\ntransferred for replication for the replication group\nmyrg\n:\nSELECT\nSUM\n(\ncredits_used\n),\nSUM\n(\nbytes_transferred\n)\nFROM\nTABLE\n(\ninformation_schema\n.\nreplication_group_usage_history\n(\ndate_range_start\n=>\nDATEADD\n(\n'day'\n,\n-\n14\n,\nCURRENT_DATE\n()),\nreplication_group_name\n=>\n'myrg'\n));\nCopy\nOn this page\nUse Snowsight to monitor replication\nMonitor the progress of refresh operations\nView replication history\nMonitor replication costs\nMonitor replication costs for databases\nRelated content\nIntroduction to replication and failover across multiple accounts\nReplication considerations",
    " the progress of refresh operations\nView replication history\nMonitor replication costs\nMonitor replication costs for databases\nRelated content\nIntroduction to replication and failover across multiple accounts\nReplication considerations",
    "Storage costs for Time Travel and Fail-safe\n\u00b6\nStorage fees are incurred for maintaining historical data during both the Time Travel and Fail-safe periods.\nStorage usage and fees\n\u00b6\nThe fees are calculated for each 24-hour period (that is, 1 day) from the time that the data changed. The number of days that Snowflake maintains\nhistorical data is based on the table type and the Time Travel retention period for the table.\nAlso, Snowflake minimizes the amount of storage required for historical data by maintaining only the information required to restore the individual table rows that were updated or deleted. As a result,\nstorage usage is calculated as a percentage of the table that changed. Snowflake only maintains full copies of tables when tables are dropped or truncated.\nTemporary and transient tables\n\u00b6\nTo help manage the storage costs associated with Time Travel and Fail-safe, Snowflake provides two table types, temporary and transient, which do not incur the same fees as standard (that is, permanent) tables:\nTransient tables can have a Time Travel retention period of either 0 or 1 day.\nTemporary tables can also have a Time Travel retention period of 0 or 1 day; however, this retention period ends as soon as the table is dropped or the session in which the table was created ends.\nTransient and temporary tables have no Fail-safe period.\nAs a result, the maximum additional fees incurred for Time Travel and Fail-safe by these types of tables is limited to 1 day. The following table illustrates the different scenarios, based on\ntable type:\nTable Type\nTime Travel Retention Period (Days)\nFail-safe Period (Days)\nMin , Max Historical Data Maintained (Days)\nPermanent\n0 or 1 (for Snowflake Standard Edition)\n7\n7 , 8\n0 to 90 (for Snowflake Enterprise Edition)\n7\n7 , 97\nTransient\n0 or 1\n0\n0 , 1\nTemporary\n0 or 1\n0\n0 , 1\nConsiderations for using temporary and transient tables to manage storage costs\n\u00b6\nWhen you choose whether to store data in permanent, temporary, or transient tables, consider the following details:\nTemporary tables are dropped when the session in which they were created ends. Data stored in temporary tables is\nnot\nrecoverable after the table is dropped.\nHistorical data in transient tables\ncan\u2019t\nbe recovered by Snowflake after the Time Travel retention period ends. Use transient tables",
    "0\n0 , 1\nConsiderations for using temporary and transient tables to manage storage costs\n\u00b6\nWhen you choose whether to store data in permanent, temporary, or transient tables, consider the following details:\nTemporary tables are dropped when the session in which they were created ends. Data stored in temporary tables is\nnot\nrecoverable after the table is dropped.\nHistorical data in transient tables\ncan\u2019t\nbe recovered by Snowflake after the Time Travel retention period ends. Use transient tables\nonly\nfor data you can replicate or reproduce\nindependently from Snowflake.\nLong-lived tables, such as fact tables, should\nalways\nbe defined as permanent to ensure they are fully protected by Fail-safe.\nYou can define short-lived tables as transient to eliminate Fail-safe costs. For example, you might use transient tables for data with a lifetime of less than 1 day, such as ETL work tables.\nIf downtime and the time required to reload lost data are factors, permanent tables, even with their added Fail-safe costs, might offer a better overall solution than transient tables.\nNote\nThe default type for tables is permanent. To define a table as temporary or transient, you must explicitly specify the type during table creation:\nCREATE\n[\nOR\nREPLACE\n]\n[\nTEMPORARY\n|\nTRANSIENT\n]\nTABLE\n<name>\n...\nFor more information, see\nCREATE TABLE\n.\nMigrating data from permanent tables to transient tables\n\u00b6\nMigrating data from permanent tables to transient tables involves performing the following tasks:\nUse\nCREATE TABLE \u2026 AS SELECT\nto create and populate the transient tables with the data from the original, permanent tables.\nApply all access control privileges granted on the original tables to the new tables. For more information about access control, see\nOverview of Access Control\n.\nUse\nDROP TABLE\nto delete the original tables.\nOptionally, use\nALTER TABLE\nto rename the new tables to match the original tables.\nCost for backups\n\u00b6\nThe following table describes charges for backups.\nFor information about credit consumption, see the\nSnowflake Service Consumption Table\n.\nCost component\nDescription\nBilled\nBackup compute\nSnowflake-managed compute service generates scheduled backup creation and expiration.\nYes\nRestore compute\nSnowflake-managed warehouses are used to restore objects from backups.\nYes\nBackup storage\nSnowflake-managed cloud object storage to store backup data.\nBilled for bytes retained for backups, similar to bytes retained for",
    " backups\n\u00b6\nThe following table describes charges for backups.\nFor information about credit consumption, see the\nSnowflake Service Consumption Table\n.\nCost component\nDescription\nBilled\nBackup compute\nSnowflake-managed compute service generates scheduled backup creation and expiration.\nYes\nRestore compute\nSnowflake-managed warehouses are used to restore objects from backups.\nYes\nBackup storage\nSnowflake-managed cloud object storage to store backup data.\nBilled for bytes retained for backups, similar to bytes retained for clones.\nYou can monitor costs for backup storage in the\nTABLE_STORAGE_METRICS\nview using the\nRETAINED_FOR_CLONE_BYTES\ncolumn, and in the\nBACKUP_STORAGE_USAGE\nview.\nOn this page\nStorage usage and fees\nTemporary and transient tables\nConsiderations for using temporary and transient tables to manage storage costs\nMigrating data from permanent tables to transient tables\nCost for backups\nRelated content\nUnderstanding & using Time Travel\nUnderstanding and viewing Fail-safe\nWorking with Temporary and Transient Tables\nData storage considerations",
    "Understanding & using Time Travel\n\u00b6\nSnowflake Time Travel enables accessing historical data (that is, data that has been changed or deleted) at any point within a defined period.\nIt serves as a powerful tool for performing the following tasks:\nRestoring objects that might have been accidentally or intentionally deleted. You can restore individual objects,\nsuch as tables, or restore all the objects inside a container object by restoring an entire schema or database.\nDuplicating and backing up data from key points in the past.\nAnalyzing data usage/manipulation over specified periods of time.\nIntroduction to Time Travel\n\u00b6\nUsing Time Travel, you can perform the following actions within a defined period of time:\nQuery data in the past that has since been updated or deleted.\nCreate clones of entire tables, schemas, and databases at or before specific points in the past.\nRestore tables, schemas, databases, and some other kinds of objects that have been dropped.\nNote\nWhen querying historical data in a table or non-materialized view, the current table or view schema is used. For more\ninformation, see\nUsage notes\nfor AT | BEFORE.\nAfter the defined period of time has elapsed, the data is moved into\nSnowflake Fail-safe\nand these actions\ncan no longer be performed.\nNote\nA long-running Time Travel query will delay moving any data and objects (tables, schemas, and databases) in the account into Fail-safe,\nuntil the query completes.\nTime Travel SQL extensions\n\u00b6\nTo support Time Travel, the following SQL extensions have been implemented:\nAT | BEFORE\nclause which can be specified in SELECT statements and CREATE \u2026 CLONE commands (immediately\nafter the object name). The clause uses one of the following parameters to pinpoint the exact historical data you want to access:\nTIMESTAMP\nOFFSET (time difference in seconds from the present time)\nSTATEMENT (query ID for statement)\nUNDROP <object>\ncommand for tables, schemas, databases, accounts, external volumes, and tags.\nData retention period\n\u00b6\nA key component of Snowflake Time Travel is the data retention period.\nWhen data in a table is modified, including deletion of data or dropping an object containing data, Snowflake preserves the state of the data\nbefore the update. The data retention period specifies the number of days for which this historical data is preserved and, therefore,\nTime Travel operations (SELECT, CREATE \u2026 CLONE, UNDROP) can be performed on the data.\nThe standard retention period is ",
    ".\nData retention period\n\u00b6\nA key component of Snowflake Time Travel is the data retention period.\nWhen data in a table is modified, including deletion of data or dropping an object containing data, Snowflake preserves the state of the data\nbefore the update. The data retention period specifies the number of days for which this historical data is preserved and, therefore,\nTime Travel operations (SELECT, CREATE \u2026 CLONE, UNDROP) can be performed on the data.\nThe standard retention period is 1 day (24 hours) and is automatically enabled for all Snowflake accounts:\nFor Snowflake Standard Edition, the retention period can be set to 0 (or unset back to the default of 1 day) at the account and object\nlevel (that is, databases, schemas, and tables).\nFor Snowflake Enterprise Edition (and higher):\nFor transient databases, schemas, and tables, the retention period can be set to 0 (or unset back to the default of 1 day). The same\nis also true for temporary tables.\nFor permanent databases, schemas, and tables, the retention period can be set to any value from 0 up to 90 days.\nNote\nA retention period of 0 days for an object effectively deactivates Time Travel for the object.\nWhen the retention period ends for an object, the historical data is moved into\nSnowflake Fail-safe\n:\nHistorical data is no longer available for querying.\nPast objects can no longer be cloned.\nPast objects that were dropped can no longer be restored.\nTo specify the data retention period for Time Travel:\nThe\nDATA_RETENTION_TIME_IN_DAYS\nobject parameter can be used by users with the ACCOUNTADMIN role to set the default\nretention period for your account.\nThe same parameter can be used to explicitly override the default when creating a database, schema, and individual table.\nThe data retention period for a database, schema, or table can be changed at any time.\nThe\nMIN_DATA_RETENTION_TIME_IN_DAYS\naccount parameter can be set by users with the ACCOUNTADMIN role to set a minimum\nretention period for the account. This parameter does not alter or replace the DATA_RETENTION_TIME_IN_DAYS parameter value. However it\nmay change the effective data retention time. When this parameter is set at the account level, the effective minimum data retention\nperiod for an object is determined by MAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\nLimitations\n\u00b6\n",
    ";\nCopy\nDropping and restoring objects\n\u00b6\nThe following sections explain the Time Travel considerations for the DROP, SHOW, and UNDROP commands.\nDropping objects\n\u00b6\nWhen a table, schema, or database is dropped, it is not immediately overwritten or removed from the system. Instead, it is retained for the\ndata retention period for the object, during which time the object can be restored. Once dropped objects are moved to\nFail-safe\n, you cannot restore them.\nTo drop a notebook, table, schema, or database, use the following commands:\nDROP NOTEBOOK\nDROP TABLE\nDROP SCHEMA\nDROP DATABASE\nNote\nAfter dropping an object, creating an object with the same name does not restore the object. Instead, it creates a new version of the\nobject. The original, dropped version is still available and can be restored.\nRestoring a dropped object restores the object in place (that is, it does not create a new object).\nListing dropped objects\n\u00b6\nDropped objects can be listed using the following commands with the HISTORY keyword specified:\nSHOW TABLES\nSHOW SCHEMAS\nSHOW DATABASES\nSHOW ACCOUNTS\nFor example:\nSHOW\nTABLES\nHISTORY\nLIKE\n'load%'\nIN\nmytestdb\n.\nmyschema\n;\nSHOW\nSCHEMAS\nHISTORY\nIN\nmytestdb\n;\nSHOW DATABASES\nHISTORY\n;\nCopy\nThe output includes all dropped objects and an additional DROPPED_ON column, which displays the date and time when the object was dropped.\nIf an object has been dropped more than once, each version of the object is included as a separate row in the output.\nNote\nAfter the retention period for an object has passed and the object has been purged, it is no longer displayed in the\nSHOW\n<object_type>\nHISTORY output.\nRestoring objects\n\u00b6\nA dropped object that has not been purged from the system (that is, the object is displayed in the SHOW\n<object_type>\nHISTORY output) can be\nrestored using the following commands:\nUNDROP NOTEBOOK\nUNDROP TABLE\nUNDROP SCHEMA\nUNDROP DATABASE\nUNDROP ICEBERG TABLE\nUNDROP DYNAMIC TABLE\nUNDROP EXTERNAL VOLUME\nUNDROP TAG\nUNDROP ACCOUNT\nCalling UNDROP restores the object to its most recent state before the DROP command was issued.\nFor example:\nUNDROP\nTABLE\nmytable\n;\n",
    " object is displayed in the SHOW\n<object_type>\nHISTORY output) can be\nrestored using the following commands:\nUNDROP NOTEBOOK\nUNDROP TABLE\nUNDROP SCHEMA\nUNDROP DATABASE\nUNDROP ICEBERG TABLE\nUNDROP DYNAMIC TABLE\nUNDROP EXTERNAL VOLUME\nUNDROP TAG\nUNDROP ACCOUNT\nCalling UNDROP restores the object to its most recent state before the DROP command was issued.\nFor example:\nUNDROP\nTABLE\nmytable\n;\nUNDROP\nSCHEMA\nmyschema\n;\nUNDROP\nDATABASE\nmydatabase\n;\nUNDROP\nNOTEBOOK\nmynotebook\n;\nCopy\nNote\nIf an object with the same name already exists, UNDROP fails. You must rename the existing object, which then enables you to restore\nthe previous version of the object.\nAccess control requirements and name resolution\n\u00b6\nSimilar to dropping an object, a user must have OWNERSHIP privileges for an object to restore it. In addition, the user must have CREATE\nprivileges on the object type for the database or schema where the dropped object will be restored.\nRestoring tables and schemas is only supported in the current schema or current database, even if a fully-qualified object name is specified.\nExample: Dropping and restoring a table multiple times\n\u00b6\nIn the following example, the\nmytestdb.public\nschema contains two tables:\nloaddata1\nand\nproddata1\n. The\nloaddata1\ntable is\ndropped and recreated twice, creating three versions of the table:\nCurrent version\nSecond (most recent) dropped version\nFirst dropped version\nThe example then illustrates how to restore the two dropped versions of the table:\nFirst, the current table with the same name is renamed to\nloaddata3\n. This enables restoring the most recent version of the dropped\ntable, based on the timestamp.\nThen, the most recent dropped version of the table is restored.\nThe restored table is renamed to\nloaddata2\nto enable restoring the first version of the dropped table.\nLastly, the first version of the dropped table is restored.\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped",
    " table is renamed to\nloaddata2\nto enable restoring the first version of the dropped table.\nLastly, the first version of the dropped table is restored.\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nDROP\nTABLE\nloaddata1\n;\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | Fri, 13 May 2016 19:04:46",
    " MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | Fri, 13 May 2016 19:04:46 -0700 |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nCREATE\nTABLE\nloaddata1\n(\nc1\nnumber\n);\nINSERT\nINTO\nloaddata1\nVALUES\n(\n1111\n),\n(\n2222\n),\n(\n3333\n),\n(\n4444\n);\nDROP\nTABLE\nloaddata1\n;\nCREATE\nTABLE\nloaddata1\n(\nc1\nvarchar\n);\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 0    | 0     | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 4    | 4096  | PUBLIC | 1              | Fri, 13 May 2016 19:05:51 -0700 |\n| Tue, 17 Mar 2016 17:41:55 -070",
    " | PUBLIC | 1              | [NULL]                          |\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 4    | 4096  | PUBLIC | 1              | Fri, 13 May 2016 19:05:51 -0700 |\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | Fri, 13 May 2016 19:04:46 -0700 |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nALTER\nTABLE\nloaddata1\nRENAME\nTO\nloaddata3\n;\nUNDROP\nTABLE\nloaddata1\n;\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 4    | 4096  | PUBLIC | 1              | [NULL]                          |\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA3 | MYTESTDB      | PUBLIC      | TABLE |         |            | 0    | 0     | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1",
    " 0    | 0     | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | Fri, 13 May 2016 19:04:46 -0700 |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nALTER\nTABLE\nloaddata1\nRENAME\nTO\nloaddata2\n;\nUNDROP\nTABLE\nloaddata1\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | [NULL]                          |\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA2 | MYTESTDB      | PUBLIC      | TABLE |         |            | 4    | 4096  | PUBLIC | 1              | [NULL]                          |\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA3 | MYTESTDB      | PUBLIC      | TABLE |         |            | 0    | 0     | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | ",
    "NULL]                          |\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA3 | MYTESTDB      | PUBLIC      | TABLE |         |            | 0    | 0     | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nCopy\nOn this page\nIntroduction to Time Travel\nEnabling and deactivating Time Travel\nSpecifying the data retention period for an object\nChecking the data retention period for an object\nChanging the data retention period for an object\nQuerying historical data\nCloning historical objects\nDropping and restoring objects\nRelated content\nOverview of the data lifecycle\nData storage considerations\nRelated info\nFor a tutorial on using Time Travel, see the following page:\nGetting Started with Time Travel\n(Snowflake Quickstarts)",
    "_TIME_IN_DAYS\naccount parameter can be set by users with the ACCOUNTADMIN role to set a minimum\nretention period for the account. This parameter does not alter or replace the DATA_RETENTION_TIME_IN_DAYS parameter value. However it\nmay change the effective data retention time. When this parameter is set at the account level, the effective minimum data retention\nperiod for an object is determined by MAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\nLimitations\n\u00b6\nWhen using Time Travel, the following object types are\nnot\ncloned:\nExternal tables\nInternal (Snowflake) stages\nHybrid tables can be cloned for databases but not for schemas.\nUser tasks in a database or schema are\nnot\ncloned when using CREATE SCHEMA \u2026 TIMESTAMP. In the following example, tasks in the source schema (S1) are not cloned to the schema with a timestamp (S2) but are cloned to the schema without a timestamp (S3).\nCREATE\nSCHEMA\nS1\n;\nUSE\nSCHEMA\nS1\n;\nCREATE\nTASK\nT1\nAS\nSELECT\n1\n;\nCREATE\nSCHEMA\nS2\nCLONE\nS1\nAT\n(\nTIMESTAMP\n=>\n'2025-04-01 12:00:00'\n);\n-- T1 is not cloned into S2\nCREATE\nSCHEMA\nS3\nCLONE\nS1\n;\n-- T1 is cloned into S3\nCopy\nEnabling and deactivating Time Travel\n\u00b6\nNo tasks are required to enable Time Travel. It is automatically enabled with the standard, 1-day retention period.\nHowever, you may want to upgrade to Snowflake Enterprise Edition to enable configuring longer data retention periods of up to 90 days\nfor databases, schemas, and tables. Note that extended data retention requires additional storage which will be reflected in your monthly\nstorage charges. For more information about storage charges, see\nStorage costs for Time Travel and Fail-safe\n.\nTime Travel cannot be deactivated for an account. A user with the ACCOUNTADMIN role can set\nDATA_RETENTION_TIME_IN_DAYS\nto 0 at\nthe account level, which means that all databases (and subsequently all schemas and tables) created in the account have no retention period\nby default; however, this default can be overridden at any time for any database, schema, or table.\nA user with the ACCOUNTADMIN role can also set",
    "\nStorage costs for Time Travel and Fail-safe\n.\nTime Travel cannot be deactivated for an account. A user with the ACCOUNTADMIN role can set\nDATA_RETENTION_TIME_IN_DAYS\nto 0 at\nthe account level, which means that all databases (and subsequently all schemas and tables) created in the account have no retention period\nby default; however, this default can be overridden at any time for any database, schema, or table.\nA user with the ACCOUNTADMIN role can also set the\nMIN_DATA_RETENTION_TIME_IN_DAYS\nat the account level. This parameter\nsetting enforces a minimum data retention period for databases, schemas, and tables. Setting MIN_DATA_RETENTION_TIME_IN_DAYS does not\nalter or replace the DATA_RETENTION_TIME_IN_DAYS parameter value. It may, however, change the effective data retention period for objects.\nWhen MIN_DATA_RETENTION_TIME_IN_DAYS is set at the account level, the data retention period for an object is determined by\nMAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\nTime Travel can be deactivated for individual databases, schemas, and tables by specifying\nDATA_RETENTION_TIME_IN_DAYS\nwith a\nvalue of 0 for the object. However, if DATA_RETENTION_TIME_IN_DAYS is set to a value of 0, and MIN_DATA_RETENTION_TIME_IN_DAYS is set\nat the account level and is greater than 0, the higher value setting takes precedence.\nAttention\nBefore setting\nDATA_RETENTION_TIME_IN_DAYS\nto 0 for any object, consider whether you want to deactivate Time Travel for the object,\nparticularly as it pertains to recovering the object if it is dropped. When an object with no retention period is dropped, you will not\nbe able to restore the object.\nAs a general rule, we recommend maintaining a value of (at least) 1 day for any given object.\nIf the Time Travel retention period is set to 0, any modified or deleted data is moved into Fail-safe (for permanent tables)\nor deleted (for transient tables) by a background process. This may take a short time to complete. During that time, the\nTIME_TRAVEL_BYTES in table storage metrics might contain a non-zero value even when the Time Travel retention period is 0 days.\nSpecifying the data retention period for an object\n\u00b6\nEnterprise Edition Feature\nSpecifying a retention period greater than 1 day requires Enterprise Edition (or higher). To inquire about",
    " data is moved into Fail-safe (for permanent tables)\nor deleted (for transient tables) by a background process. This may take a short time to complete. During that time, the\nTIME_TRAVEL_BYTES in table storage metrics might contain a non-zero value even when the Time Travel retention period is 0 days.\nSpecifying the data retention period for an object\n\u00b6\nEnterprise Edition Feature\nSpecifying a retention period greater than 1 day requires Enterprise Edition (or higher). To inquire about upgrading, please contact\nSnowflake Support\n.\nBy default, the maximum retention period is 1 day (one 24-hour period). With Snowflake Enterprise Edition (and higher), the default\nfor your account can be set to any value up to 90 days:\nWhen creating a table, schema, or database, the account default can be overridden using the\nDATA_RETENTION_TIME_IN_DAYS\nparameter in the command.\nIf a retention period is specified for a database or schema, the period is inherited by default for all objects created in the\ndatabase/schema.\nA minimum retention period can be set on the account using the\nMIN_DATA_RETENTION_TIME_IN_DAYS\nparameter. If this parameter is\nset at the account level, the data retention period for an object is determined by\nMAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\nChecking the data retention period for an object\n\u00b6\nTo check the current retention period for a table, schema, or database, you can check the value of the\nretention_time\ncolumn in the output of the corresponding SHOW command, such as\nSHOW TABLES\n,\nSHOW SCHEMAS\n, or\nSHOW DATABASES\n.\nFor objects that are derived from tables, schemas, or databases, such as materialized views, you can examine the\nretention periods of the parent objects.\nFor streams, you can check the value of the\nstale_after\ncolumn in the output from the\nSHOW STREAMS\ncommand.\nTo include information about objects that have already been dropped, include the HISTORY clause with the\nSHOW command.\nThe following example shows how you might check the retention periods of certain objects by filtering the output of\nthe SHOW commands.\nThe following example checks the retention period for specific named tables:\nSHOW\nTABLES\n->>\nSELECT\n\"name\"\n,\n\"retention_time\"\nFROM\n$\n1\nWHERE\n\"name\"\nIN\n(\n'MY_TABLE1'\n,\n'MY_TABLE2",
    "S\ncommand.\nTo include information about objects that have already been dropped, include the HISTORY clause with the\nSHOW command.\nThe following example shows how you might check the retention periods of certain objects by filtering the output of\nthe SHOW commands.\nThe following example checks the retention period for specific named tables:\nSHOW\nTABLES\n->>\nSELECT\n\"name\"\n,\n\"retention_time\"\nFROM\n$\n1\nWHERE\n\"name\"\nIN\n(\n'MY_TABLE1'\n,\n'MY_TABLE2'\n);\nCopy\nThe following example checks for schemas where Time Travel is turned off:\nSHOW\nSCHEMAS\n->>\nSELECT\n\"name\"\n,\n\"retention_time\"\nFROM\n$\n1\nWHERE\n\"retention_time\"\n=\n0\n;\nCopy\nThe following example checks for databases where retention time is larger than the default.\nThe results include databases that have already been dropped.\nSHOW DATABASES\nHISTORY\n->>\nSELECT\n\"name\"\n,\n\"retention_time\"\n,\n\"dropped_on\"\nFROM\n$\n1\nWHERE\n\"retention_time\"\n>\n1\n;\nCopy\nChanging the data retention period for an object\n\u00b6\nIf you change the data retention period for a table, the new retention period impacts all data that is active, as well as any data currently\nin Time Travel. The impact depends on whether you increase or decrease the period:\nIncreasing Retention\n:\nCauses the data currently in Time Travel to be retained for the longer time period.\nFor example, if you have a table with a 10-day retention period and increase the period to 20 days, data that would have been removed\nafter 10 days is now retained for an additional 10 days before moving into Fail-safe.\nNote that this doesn\u2019t apply to any data that is older than 10 days and has already moved into Fail-safe.\nDecreasing Retention\n:\nReduces the amount of time data is retained in Time Travel:\nFor active data modified after the retention period is reduced, the new shorter period applies.\nFor data that is currently in Time Travel:\nIf the data is still within the new shorter period, it remains in Time Travel.\nIf the data is outside the new period, it moves into Fail-safe.\nFor example, if you have a table with a 10-day retention period and you decrease the period to 1-day, data from days 2 to 10 will be moved\ninto Fail-safe, leaving only the data from day 1 accessible",
    " is reduced, the new shorter period applies.\nFor data that is currently in Time Travel:\nIf the data is still within the new shorter period, it remains in Time Travel.\nIf the data is outside the new period, it moves into Fail-safe.\nFor example, if you have a table with a 10-day retention period and you decrease the period to 1-day, data from days 2 to 10 will be moved\ninto Fail-safe, leaving only the data from day 1 accessible through Time Travel.\nHowever, the process of moving the data from Time Travel into Fail-safe is performed by a background process, so the change is not immediately\nvisible. Snowflake guarantees that the data will be moved, but does not specify when the process will complete; until the background process\ncompletes, the data is still accessible through Time Travel.\nNote\nIf you change the data retention period for a database or schema, the change only affects active objects contained within\nthe database or schema. Any objects that have been dropped (for example, tables) remain unaffected.\nFor example, if you have a schema\ns1\nwith a 90-day retention period and table\nt1\nis in schema\ns1\n,\ntable\nt1\ninherits the 90-day retention period. If you drop table\ns1.t1\n,\nt1\nis retained in Time Travel\nfor 90 days. Later, if you change the schema\u2019s data retention period to 1 day, the retention\nperiod for the dropped table\nt1\nis unchanged. Table\nt1\nwill still be retained in Time Travel for 90 days.\nTo alter the retention period of a dropped object, you must undrop the object, then alter its retention period.\nTo change the retention period for an object, use the appropriate\nALTER <object>\ncommand. For example, to change the\nretention period for a table:\nCREATE\nTABLE\nmytable\n(\ncol1\nNUMBER\n,\ncol2\nDATE\n)\nDATA_RETENTION_TIME_IN_DAYS\n=\n90\n;\nALTER\nTABLE\nmytable\nSET\nDATA_RETENTION_TIME_IN_DAYS\n=\n30\n;\nCopy\nAttention\nChanging the retention period for your account or individual objects changes the value for all lower-level objects that do not have a\nretention period explicitly set. For example:\nIf you change the retention period at the account level, all databases, schemas, and tables that do not",
    "1\nNUMBER\n,\ncol2\nDATE\n)\nDATA_RETENTION_TIME_IN_DAYS\n=\n90\n;\nALTER\nTABLE\nmytable\nSET\nDATA_RETENTION_TIME_IN_DAYS\n=\n30\n;\nCopy\nAttention\nChanging the retention period for your account or individual objects changes the value for all lower-level objects that do not have a\nretention period explicitly set. For example:\nIf you change the retention period at the account level, all databases, schemas, and tables that do not have an explicit retention period\nautomatically inherit the new retention period.\nIf you change the retention period at the schema level, all tables in the schema that do not have an explicit retention period inherit the\nnew retention period.\nKeep this in mind when changing the retention period for your account or any objects in your account because the change might have\nTime Travel consequences that you did not anticipate or intend. In particular, we do\nnot\nrecommend changing the retention period to 0\nat the account level.\nDropped containers and object retention inheritance\n\u00b6\nCurrently, when a database is dropped, the data retention period for child schemas or tables, if explicitly set to be different from the\nretention of the database, is not honored. The child schemas or tables are retained for the same period of time as the database.\nSimilarly, when a schema is dropped, the data retention period for child tables, if explicitly set to be different from the retention of\nthe schema, is not honored. The child tables are retained for the same period of time as the schema.\nTo honor the data retention period for these child objects (schemas or tables), drop them explicitly\nbefore\nyou drop the database\nor schema.\nQuerying historical data\n\u00b6\nWhen any DML operations are performed on a table, Snowflake retains previous versions of the table data for a defined period of time. This\nenables querying earlier versions of the data using the\nAT | BEFORE\nclause.\nThis clause supports querying data either exactly at or immediately preceding a specified point in the table\u2019s history within the\nretention period. The specified point can be time-based (for example, a timestamp or time offset from the present) or it can be the ID for a\ncompleted statement (for example, SELECT or INSERT).\nFor example:\nThe following query selects historical data from a table as of the date and time represented by the specified\ntimestamp\n:\nSELECT\n*\nFROM\nmy_table\nAT\n(\nTIM",
    " querying data either exactly at or immediately preceding a specified point in the table\u2019s history within the\nretention period. The specified point can be time-based (for example, a timestamp or time offset from the present) or it can be the ID for a\ncompleted statement (for example, SELECT or INSERT).\nFor example:\nThe following query selects historical data from a table as of the date and time represented by the specified\ntimestamp\n:\nSELECT\n*\nFROM\nmy_table\nAT\n(\nTIMESTAMP\n=>\n'Wed, 26 Jun 2024 09:20:00 -0700'\n::timestamp_tz\n);\nCopy\nThe following query selects historical data from a table as of 5 minutes ago:\nSELECT\n*\nFROM\nmy_table\nAT\n(\nOFFSET\n=>\n-\n60\n*\n5\n);\nCopy\nThe following query selects historical data from a table up to, but not including any changes made by the specified statement:\nSELECT\n*\nFROM\nmy_table\nBEFORE\n(\nSTATEMENT\n=>\n'8e5d0ca9-005e-44e6-b858-a8f5b37c5726'\n);\nCopy\nNote\nIf the TIMESTAMP, OFFSET, or STATEMENT specified in the\nAT | BEFORE\nclause falls outside the data\nretention period for the table, the query fails and returns an error.\nCloning historical objects\n\u00b6\nIn addition to queries, the\nAT | BEFORE\nclause can be used with the CLONE keyword in the CREATE command\nfor a table, schema, or database to create a logical duplicate of the object at a specified point in the object\u2019s history. If you don\u2019t\nspecify a point in time, the clone defaults to the state of the object as of now\n(the\nCURRENT_TIMESTAMP\nvalue).\nFor example:\nThe following\nCREATE TABLE\nstatement creates a clone of a table as of the date and time represented by the\nspecified timestamp:\nCREATE\nTABLE\nrestored_table\nCLONE\nmy_table\nAT\n(\nTIMESTAMP\n=>\n'Wed, 26 Jun 2024 01:01:00 +0300'\n::timestamp_tz\n);\nCopy\nThe following\nCREATE SCHEMA\nstatement creates a clone of a schema and all its objects as they existed 1 hour\nbefore the current time:\nCREATE\nSCHEMA\nrestored_schema\nCLONE\nmy_schema\nAT\n(\nOFFSET\n=",
    "CREATE\nTABLE\nrestored_table\nCLONE\nmy_table\nAT\n(\nTIMESTAMP\n=>\n'Wed, 26 Jun 2024 01:01:00 +0300'\n::timestamp_tz\n);\nCopy\nThe following\nCREATE SCHEMA\nstatement creates a clone of a schema and all its objects as they existed 1 hour\nbefore the current time:\nCREATE\nSCHEMA\nrestored_schema\nCLONE\nmy_schema\nAT\n(\nOFFSET\n=>\n-\n3600\n);\nCopy\nThe following\nCREATE DATABASE\nstatement creates a clone of a database and all its objects as they existed prior\nto the completion of the specified statement:\nCREATE\nDATABASE\nrestored_db\nCLONE\nmy_db\nBEFORE\n(\nSTATEMENT\n=>\n'8e5d0ca9-005e-44e6-b858-a8f5b37c5726'\n);\nCopy\nNote\nThe cloning operation for a database or schema fails:\nIf the specified Time Travel time is beyond the retention time of\nany current child\n(for example, a table) of the entity.\nAs a workaround for child objects that have been purged from Time Travel, use the\nIGNORE TABLES WITH INSUFFICIENT DATA RETENTION\nparameter of the\nCREATE <object> \u2026 CLONE command. For more information, see\nChild objects and data retention time\n.\nIf the specified Time Travel time is at or before the point in time when the object was created.\nThe following\nCREATE DATABASE\nstatement creates a clone of a database and all its objects as they existed\nfour days ago, skipping any tables that have a data retention period of less than four days:\nCREATE\nDATABASE\nrestored_db\nCLONE\nmy_db\nAT\n(\nTIMESTAMP\n=>\nDATEADD\n(\ndays\n,\n-\n4\n,\ncurrent_timestamp\n)\n::timestamp_tz\n)\nIGNORE TABLES WITH INSUFFICIENT DATA RETENTION\n;\nCopy\nDropping and restoring objects\n\u00b6\nThe following sections explain the Time Travel considerations for the DROP, SHOW, and UNDROP commands.\nDropping objects\n\u00b6\nWhen a table, schema, or database is dropped, it is not immediately overwritten or removed from the system. Instead, it is retained for the\ndata retention period for the object, during which time the object can be restored. Once dropped objects are moved to\nFail-safe\n, you cannot restore them.\nTo drop",
    "Replication of security integrations & network policies across multiple accounts\n\u00b6\nBusiness Critical Feature\nAccount object replication and failover/failback require Business Critical Edition (or higher). To inquire about upgrading, please\ncontact\nSnowflake Support\n.\nThis topic provides information on how to replicate security integrations, along with using failover/failback with\neach of these objects, and assumes familiarity with replication and failover/failback with other account-level objects\n(e.g. users, roles, warehouses).\nFor details, see\nIntroduction to replication and failover across multiple accounts\n.\nThese objects and services are supported across\nregions\nand across\ncloud platforms\n.\nOverview\n\u00b6\nSnowflake supports replicating network policies and security integrations for federated SSO (i.e. SAML2), OAuth, and SCIM along with\nenabling failover/failback for each network policy and integration.\nThe general approach to test replication and failover/failback with each network policy and security integration is as follows:\nIdentify the source account and target account for replication, and identify the connection URL.\nComplete steps in the source account.\nComplete steps in the target account.\nTest failover/failback.\nNote that because network policies and security integrations have different use cases, the exact steps for the source account and target\naccount with respect to each object differ slightly.\nFor details, see:\nReplicating SAML2 security integrations\nReplicating SCIM security integrations\nReplicating OAuth security integrations\nReplicating network policies\nReplicating integrations and objects for the Snowflake Connector for ServiceNow\nReplicating SAML2 security integrations\n\u00b6\nReplicating a SAML2 security integration links the source account and the target account to the identity provider by specifying the\nconnection URL\nin the SAML2 security integration definition.\nIt is important to update the identity provider to specify the connection URL and that users exist in the source account. Without these\nupdates, user verification cannot occur, which will result in the inability of the user to access the target account.\nCurrent Limitation\n:\nFor SAML SSO to Snowflake, replicating a SAML2 security integration that specifies the connection URL is only supported on the current\nprimary connection and not supported on the secondary connection. Note that for failover, the result is promoting a secondary connection\nto serve as the primary connection. After failover, S",
    " in the source account. Without these\nupdates, user verification cannot occur, which will result in the inability of the user to access the target account.\nCurrent Limitation\n:\nFor SAML SSO to Snowflake, replicating a SAML2 security integration that specifies the connection URL is only supported on the current\nprimary connection and not supported on the secondary connection. Note that for failover, the result is promoting a secondary connection\nto serve as the primary connection. After failover, SAML SSO works on the new primary connection.\nIf SAML SSO is needed for both primary and secondary connections, then create and manage SAML2 security integrations independently on\nboth Snowflake accounts.\nFor this procedure, assume the following:\nSource account:\nhttps://example-northamericawest.snowflakecomputing.com/\nTarget account:\nhttps://example-northamericaeast.snowflakecomputing.com/\nConnection URL:\nhttps://example-global.snowflakecomputing.com\nA secondary connection does not exist in the target account.\nThis procedure is a representative example to do the following:\nReplicate a SAML2 security integration from the source account to the target account.\nTest failover.\nPromote the secondary connection in the source account to serve as the primary connection.\nSource account steps (includes IdP steps):\nIf the source account is already configured for\nDatabase Failover/Failback and Client Redirect\n,\nskip to the next step.\nOtherwise, enable failover using an\nALTER CONNECTION\ncommand:\nALTER\nCONNECTION\nglobal\nENABLE\nFAILOVER\nTO\nACCOUNTS\nexample\n.\nnorthamericaeast\n;\nCopy\nUsing Okta as a representative example for the identity provider, create a\nSnowflake application in Okta\nthat specifies the connection URL. Update\nthe Okta fields as follows:\nLabel\n:\nSnowflake\nSubdomain\n:\nexample-global\nBrowser plugin auto-submit\n: Check the box to enable automatic login when a user lands on the login page.\nIn the source account, update the SAML2 security integration to specify the connection URL for the\nsaml2_snowflake_issuer_url\nand\nsaml2_snowflake_acs_url\nsecurity integration properties.\nCREATE\nOR\nREPLACE\nSECURITY INTEGRATION\nmy_idp\nTYPE\n=\nsaml2\nENABLED\n=\ntrue\nSAML2_ISSUER\n=\n'http://www.okta.com",
    "\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmyaccount2\n;\nCopy\nExample\n\u00b6\nFor this example, assume the following:\nSource account:\nhttps://example-northamericawest.snowflakecomputing.com/\nTarget account:\nhttps://example-northamericaeast.snowflakecomputing.com/\nConnection URL:\nhttps://example-global.snowflakecomputing.com\nA secondary connection exists in the target account (i.e. only refresh operations are needed).\nNetwork policies exist in the source account.\nThe Snowflake OAuth and/or SCIM security integration already exists in the source account and the integration specifies a network policy.\nThis procedural example does the following:\nReplicates network policies along with the network rules that is uses to restrict network traffic.\nReplicates a security integration to which the network policy is assigned.\nRefreshes the failover group.\nVerifies the network policy activation.\nPromotes the secondary connection in the source account to serve as the primary connection.\nSource account steps:\nVerify that network policies exist in the source Snowflake account by executing a\nSHOW NETWORK POLICIES\ncommand.\nVerify the Snowflake OAuth and/or SCIM security integrations include a network policy by executing a\nSHOW INTEGRATIONS\ncommand to identify the security integration and then execute a\nDESCRIBE INTEGRATION\ncommand on the Snowflake OAuth security integration.\nUpdate the failover group to include\nnetwork\npolicies\nand\naccount\nparameters\nusing an ALTER FAILOVER GROUP command.\nALTER\nFAILOVER\nGROUP\nfg\nSET\nOBJECT_TYPES\n=\nusers\n,\nroles\n,\nwarehouses\n,\nresource monitors\n,\nintegrations\n,\nnetwork policies\n,\naccount\nparameters\nALLOWED_INTEGRATION_TYPES\n=\nsecurity\nintegrations\n;\nCopy\nTarget account steps:\nRefresh the secondary failover group to update the target account to include the network policy objects and the Snowflake OAuth\nsecurity integration that specifies the network policy.\nALTER\nFAILOVER\nGROUP\nfg\nREFRESH\n;\nCopy\nVerify the network policy object exists by executing a SHOW NETWORK POLICIES command, and verify the Snowflake OAuth security\nintegration specifies the replicated network policy by executing a DESCRIBE SECURITY INTEGRATION command on the security integration.\nVerify the network policy activation as shown in\nIdentify an activated network policy\n.\nVerify connecting to each Snowflake account using the Snowflake OAuth client of your choice.\nOption",
    " policy.\nALTER\nFAILOVER\nGROUP\nfg\nREFRESH\n;\nCopy\nVerify the network policy object exists by executing a SHOW NETWORK POLICIES command, and verify the Snowflake OAuth security\nintegration specifies the replicated network policy by executing a DESCRIBE SECURITY INTEGRATION command on the security integration.\nVerify the network policy activation as shown in\nIdentify an activated network policy\n.\nVerify connecting to each Snowflake account using the Snowflake OAuth client of your choice.\nOptionally promote the secondary failover group and the secondary connection in the target account to primary. This will promote the\ntarget account to serve as the new source account.\nFailover group:\nALTER\nFAILOVER\nGROUP\nfg\nPRIMARY\n;\nCopy\nConnection:\nALTER\nCONNECTION\nglobal\nPRIMARY\n;\nCopy\nIf you completed the previous step, reverify that you can connect to each Snowflake account using the Snowflake OAuth client of your\nchoice.\nReplicating integrations and objects for the Snowflake Connector for ServiceNow\n\u00b6\nThe\nSnowflake Connector for ServiceNow\nallows Snowflake to ingest data from ServiceNow. The connector requires the following objects in\nyour Snowflake account:\nSecret.\nSecurity integration of\ntype\n=\napi_authentication\n.\nAPI integration.\nDatabase to store the ingested data.\nWarehouse for the connector to use.\nAccount roles to manage the access to these objects.\nYou create these objects prior to installing the connector and you can replicate these objects to the target account. After replicating\nthese objects, you can install the connector in the target account. The connector must be installed in the target account because the\ninstallation depends on a share that Snowflake provides. You need to create a database from the share during the connector installation and\nyou cannot replicate a database that is created from a share.\nDepending on how you want to manage the replication of account objects, you can have one or more replication or failover groups. A single\nreplication group centralizes the replication management of the objects and avoids scenarios where some objects are replicated and other\nobjects are not replicated. Otherwise, you must coordinate the replication operation carefully to ensure that all objects are replicated to\nthe target account.\nFor example, you can have a replication group for databases. This replication group (e.g.\nrg1\n) specifies the database that contains the\nsecret and the database to store the ServiceNow data. The other replication group (e.g.\nrg",
    "\nreplication group centralizes the replication management of the objects and avoids scenarios where some objects are replicated and other\nobjects are not replicated. Otherwise, you must coordinate the replication operation carefully to ensure that all objects are replicated to\nthe target account.\nFor example, you can have a replication group for databases. This replication group (e.g.\nrg1\n) specifies the database that contains the\nsecret and the database to store the ServiceNow data. The other replication group (e.g.\nrg2\n) specifies the user, role, and integration\nobjects and the grants of these roles to users. In this scenario, if you replicate the integrations first and then decide to refresh the\ntarget account to include the secret database, users, and roles, the replication refresh operation is successful.\nHowever, if you replicate the users and roles and the database that contains the secret in a group before you replicate the integration,\nthen a placeholder secret is used until the security integration is replicated; the placeholder secret prevents a dangling reference. Once\nthe security integration is replicated, the placeholder secret is replaced with the real secret.\nThis procedure is a representative example to do the following:\nReplicate the integrations and the databases containing the secret and ingested data.\nRefresh the failover group.\nPromote the secondary connection in the source account to serve as the primary connection.\nInstall and use the connector after replication.\nFor this procedure, assume the following:\nSource account:\nhttps://example-northamericawest.snowflakecomputing.com/\nTarget account:\nhttps://example-northamericaeast.snowflakecomputing.com/\nConnection URL:\nhttps://example-global.snowflakecomputing.com\nA secondary connection exists in the target account (i.e. only refresh operations are needed).\nOther security integrations for authentication and network policies to restrict access are already replicated.\nSource account steps:\nVerify that the objects for the connector exist in the source Snowflake account by executing SHOW commands on each of these object types.\nshow\nsecrets\nin\ndatabase\nsecretsdb\n;\nshow security integrations\n;\nshow api integrations\n;\nshow\ntables\nin\ndatabase\ndestdb\n;\nshow warehouses\n;\nshow roles\n;\nCopy\nNote that\nsecretsdb\nis the name of the database that contains the secret and\ndestdb\nis the name of the database that contains\nthe ingested data from ServiceNow.\nUpdate the failover group to include API",
    " object types.\nshow\nsecrets\nin\ndatabase\nsecretsdb\n;\nshow security integrations\n;\nshow api integrations\n;\nshow\ntables\nin\ndatabase\ndestdb\n;\nshow warehouses\n;\nshow roles\n;\nCopy\nNote that\nsecretsdb\nis the name of the database that contains the secret and\ndestdb\nis the name of the database that contains\nthe ingested data from ServiceNow.\nUpdate the failover group to include API integrations and the databases containing the secret and ingested data using an ALTER FAILOVER\nGROUP command.\nALTER\nFAILOVER\nGROUP\nfg\nSET\nOBJECT_TYPES\n=\ndatabases\n,\nusers\n,\nroles\n,\nwarehouses\n,\nresource monitors\n,\nintegrations\n,\nnetwork policies\n,\naccount\nparameters\nALLOWED_DATABASES\n=\nsecretsdb\n,\ndestdb\nALLOWED_INTEGRATION_TYPES\n=\nsecurity\nintegrations\n,\napi integrations\n;\nCopy\nTarget account steps:\nRefresh the secondary failover group to replicate the integrations and databases to the target account.\nALTER\nFAILOVER\nGROUP\nfg\nREFRESH\n;\nCopy\nVerify the replicated objects exist using the following SHOW commands.\nshow\nsecrets\n;\nshow security integrations\n;\nshow api integrations\n;\nshow\ndatabase\n;\nshow\ntables\nin\ndatabase\ndestdb\n;\nshow roles\n;\nCopy\nVerify connecting to each Snowflake account using the method of your choice (such as Snowflake CLI, a browser, or SnowSQL).\nOptionally promote the secondary failover group and the secondary connection in the target account to primary. This will promote the\ntarget account to serve as the new source account.\nFailover group:\nALTER\nFAILOVER\nGROUP\nfg\nPRIMARY\n;\nCopy\nConnection:\nALTER\nCONNECTION\nglobal\nPRIMARY\n;\nCopy\nIf you completed the previous step, reverify that you can connect to each Snowflake account.\nAt this point, the target account contains the replicated objects and users can login. However, there are additional steps in the target\naccount to use the connector.\nUpdate the remote service associated with the API integration in the cloud platform that hosts your Snowflake account.\nFor details, refer to\nUpdating the remote service for API integrations\n.\nInstall the connector manually or with Snowsight. For details, refer to:\nInstall the connector manually",
    " reverify that you can connect to each Snowflake account.\nAt this point, the target account contains the replicated objects and users can login. However, there are additional steps in the target\naccount to use the connector.\nUpdate the remote service associated with the API integration in the cloud platform that hosts your Snowflake account.\nFor details, refer to\nUpdating the remote service for API integrations\n.\nInstall the connector manually or with Snowsight. For details, refer to:\nInstall the connector manually\nInstall the connector with Snowsight\nAccess the ServiceNow Data in Snowflake\n.\nOn this page\nOverview\nReplicating SAML2 security integrations\nReplicating SCIM security integrations\nReplicating OAuth security integrations\nReplicating network policies\nReplicating integrations and objects for the Snowflake Connector for ServiceNow\nRelated content\nIntroduction to replication and failover across multiple accounts\nReplicating databases across multiple accounts\nControlling network traffic with network policies",
    " login when a user lands on the login page.\nIn the source account, update the SAML2 security integration to specify the connection URL for the\nsaml2_snowflake_issuer_url\nand\nsaml2_snowflake_acs_url\nsecurity integration properties.\nCREATE\nOR\nREPLACE\nSECURITY INTEGRATION\nmy_idp\nTYPE\n=\nsaml2\nENABLED\n=\ntrue\nSAML2_ISSUER\n=\n'http://www.okta.com/exk6e8mmrgJPj68PH4x7'\nSAML2_SSO_URL\n=\n'https://example.okta.com/app/snowflake/exk6e8mmrgJPj68PH4x7/sso/saml'\nSAML2_PROVIDER\n=\n'OKTA'\nSAML2_X509_CERT\n=\n'MIIDp...'\nSAML2_SP_INITIATED_LOGIN_PAGE_LABEL\n=\n'OKTA'\nSAML2_ENABLE_SP_INITIATED\n=\ntrue\nSAML2_SNOWFLAKE_ISSUER_URL\n=\n'https://example-global.snowflakecomputing.com'\nSAML2_SNOWFLAKE_ACS_URL\n=\n'https://example-global.snowflakecomputing.com/fed/login'\n;\nCopy\nIn Okta, assign the Snowflake application to users. For details, see\nAssign an app integration to a user\n.\nVerify that SSO to the source account works for users that are specified in the Snowflake application in Okta and users in the source\naccount.\nNote that SSO should work for both IdP-initiated and Snowflake-initiated SSO flows. For details, see\nSupported SSO workflows\n.\nIn the source account, if a failover group does not already exist,\ncreate\na failover group to\ninclude security integrations. Note that this example is representative and includes other account objects that might or might not be\nnecessary to replicate.\nIf a failover group already exists,\nalter\nthe failover group to include integrations.\nCREATE\nFAILOVER\nGROUP\nFG\nOBJECT_TYPES\n=\nusers\n,\nroles\n,\nwarehouses\n,\nresource monitors\n,\nintegrations\nALLOWED_INTEGRATION_TYPES\n=\nsecurity\nintegrations\nALLOWED_ACCOUNTS\n=\nexample\n.\nnorthamericaeast\nREPLICATION_SCHEDULE\n=\n'10 MINUTE'\n;\nCopy\nTarget Account Steps:\nPrior to replication, verify the number of users and security integr",
    ",\nalter\nthe failover group to include integrations.\nCREATE\nFAILOVER\nGROUP\nFG\nOBJECT_TYPES\n=\nusers\n,\nroles\n,\nwarehouses\n,\nresource monitors\n,\nintegrations\nALLOWED_INTEGRATION_TYPES\n=\nsecurity\nintegrations\nALLOWED_ACCOUNTS\n=\nexample\n.\nnorthamericaeast\nREPLICATION_SCHEDULE\n=\n'10 MINUTE'\n;\nCopy\nTarget Account Steps:\nPrior to replication, verify the number of users and security integrations that are present in the target\naccount by executing the\nSHOW USERS\nand\nSHOW INTEGRATIONS\ncommands, respectively.\nCreate a secondary connection. For details, see\nCREATE CONNECTION\n.\nCREATE\nCONNECTION\nglobal\nAS\nREPLICA\nOF\nexample\n.\nnorthamericawest\n.\nglobal\n;\nCopy\nCreate a secondary failover group in the target account. For details, see\nCREATE FAILOVER GROUP\n.\nCREATE\nFAILOVER\nGROUP\nfg\nAS\nREPLICA\nOF\nexample\n.\nnorthamericawest\n.\nfg\n;\nCopy\nWhen creating a secondary failover group, an initial refresh is automatically executed.\nTo manually refresh a secondary failover group in the target account, execute the following statement. For details, see\nALTER FAILOVER GROUP\ncommand.\nALTER\nFAILOVER\nGROUP\nfg\nREFRESH\n;\nCopy\nIf the refresh operation was successful, the target account should include new users that were added to the source account and not\npreviously present in the target account. Similarly, the target account should include the SAML2 security integration that specifies\nthe connection URL.\nVerify the refresh operation was successful by executing the following commands:\nSHOW INTEGRATIONS\n(should include 1 new integration)\nSHOW USERS\n(should include the number of new users added)\nDESCRIBE INTEGRATION\n(for the integration\nmyidp\n)\nPromote the secondary connection in the target account to serve as the primary connection. After executing the following command, users\ncan use SAML SSO to authenticate to the new target account.\nALTER\nCONNECTION\nglobal\nPRIMARY\n;\nCopy\nReplicating SCIM security integrations\n\u00b6\nReplicating a SCIM security integration allows the target account to incorporate SCIM updates that are made to the source account\n(e.g. adding new users, adding new roles) after refreshing the target",
    "ote the secondary connection in the target account to serve as the primary connection. After executing the following command, users\ncan use SAML SSO to authenticate to the new target account.\nALTER\nCONNECTION\nglobal\nPRIMARY\n;\nCopy\nReplicating SCIM security integrations\n\u00b6\nReplicating a SCIM security integration allows the target account to incorporate SCIM updates that are made to the source account\n(e.g. adding new users, adding new roles) after refreshing the target account.\nAfter replicating the SCIM security integration, both Snowflake accounts have the ability to receive SCIM updates from the identity\nprovider. However, Snowflake allows specifying only one account as the\nprimary\n(i.e. source) account and it is the primary account that\nreceives SCIM updates from the identity provider.\nYou can optionally designate a different account as the primary account to receive SCIM updates after replicating the SCIM integration.\nNote that the target account can receive SCIM updates from the source account only after refreshing the target account.\nFor this procedure, assume the following:\nSource account:\nhttps://example-northamericawest.snowflakecomputing.com/\nTarget account:\nhttps://example-northamericaeast.snowflakecomputing.com/\nConnection URL:\nhttps://example-global.snowflakecomputing.com\nA secondary connection exists in the target account (i.e. only refresh operations are needed).\nThis procedure is a representative example to do the following:\nReplicate a SCIM security integration from the source account to the target account.\nAdd a new user in Okta, push the new user to the source account, and replicate the new user to the target account.\nRefresh the failover group.\nPromote the secondary connection in the target account to serve as the primary connection.\nSource account steps:\nExecute\nSHOW CONNECTIONS\nto verify that the connection in the source account is the primary connection. If it\nis not the primary connection, use the\nALTER CONNECTION\ncommand to promote the connection in the source\naccount to serve as the primary connection.\nIf an Okta SCIM security integration is already configured in the source account, skip to the next step.\nOtherwise, configure an\nOkta SCIM\nsecurity integration in the source account.\nCREATE\nROLE\nIF\nNOT\nEXISTS\nokta_provisioner\n;\nGRANT\nCREATE\nUSER\nON\nACCOUNT\nTO\nROLE\nokta_provisioner\n;\n",
    "\nALTER CONNECTION\ncommand to promote the connection in the source\naccount to serve as the primary connection.\nIf an Okta SCIM security integration is already configured in the source account, skip to the next step.\nOtherwise, configure an\nOkta SCIM\nsecurity integration in the source account.\nCREATE\nROLE\nIF\nNOT\nEXISTS\nokta_provisioner\n;\nGRANT\nCREATE\nUSER\nON\nACCOUNT\nTO\nROLE\nokta_provisioner\n;\nGRANT\nCREATE\nROLE\nON\nACCOUNT\nTO\nROLE\nokta_provisioner\n;\nGRANT\nROLE\nokta_provisioner\nTO\nROLE\nACCOUNTADMIN\n;\nCREATE\nOR\nREPLACE\nSECURITY INTEGRATION\nokta_provisioning\nTYPE\n=\nscim\nSCIM_CLIENT\n=\n'okta'\nRUN_AS_ROLE\n=\n'OKTA_PROVISIONER'\n;\nselect\nsystem$generate_scim_access_token\n(\n'OKTA_PROVISIONING'\n);\nCopy\nBe sure to update the Okta SCIM application for Snowflake. For details, see\nOkta configuration\n.\nIn Okta,\ncreate a new user\nin the Okta application for Snowflake.\nVerify the user is pushed to Snowflake by executing a\nSHOW USERS\ncommand in Snowflake.\nIf the failover group already specifies\nsecurity\nintegrations\n, skip to the next step. This would be true if you have already\nconfigured the failover group for the purposes of\nSAML SSO in the target account\n(in this topic).\nOtherwise, modify the existing failover group using an ALTER FAILOVER GROUP command to specify\nsecurity\nintegrations\n.\nALTER\nFAILOVER\nGROUP\nfg\nSET\nOBJECT_TYPES\n=\nusers\n,\nroles\n,\nwarehouses\n,\nresource monitors\n,\nintegrations\nALLOWED_INTEGRATION_TYPES\n=\nsecurity\nintegrations\n;\nCopy\nAt this point, you can optionally refresh the secondary failover group as shown in the\ntarget account steps for SCIM\nto ensure the new user in the source account is in the target\naccount.\nChoosing to refresh the secondary failover group now allows for an easy check to make sure that the change to the source account, adding\na new user in this sequence, is visible in the target account.\nHowever, if you need or prefer to do additional work in the identity provider, such as modifying",
    " this point, you can optionally refresh the secondary failover group as shown in the\ntarget account steps for SCIM\nto ensure the new user in the source account is in the target\naccount.\nChoosing to refresh the secondary failover group now allows for an easy check to make sure that the change to the source account, adding\na new user in this sequence, is visible in the target account.\nHowever, if you need or prefer to do additional work in the identity provider, such as modifying other users or updating role\nassignments, you can continue doing that work now and then refresh the secondary failover group in one operation later.\nTarget account steps:\nPrior to replication, verify the number of users and security integrations that are present in the target\naccount by executing the\nSHOW USERS\nand\nSHOW INTEGRATIONS\ncommands, respectively.\nRefresh the secondary failover group to update the target account to include the new user\n(and any other changes that were made in Okta and the source account).\nALTER\nFAILOVER\nGROUP\nfg\nREFRESH\n;\nCopy\nVerify that the new user is added to the target account by executing a\nSHOW USERS\ncommand.\nOptionally, promote the secondary failover group and the secondary connection in the target account to primary. This will promote the\ntarget account to serve as the new source account.\nFailover group:\nALTER\nFAILOVER\nGROUP\nfg\nPRIMARY\n;\nCopy\nConnection:\nALTER\nCONNECTION\nglobal\nPRIMARY\n;\nCopy\nReplicating OAuth security integrations\n\u00b6\nReplicating OAuth security integrations includes both Snowflake OAuth security integrations and External OAuth security integrations.\nNote the following:\nSnowflake OAuth\n:\nAfter replication and configuring failover/failback, a user connecting to either the source account or target account via an OAuth client\ndoes not need to re-authenticate to the target account.\nExternal OAuth\n:\nAfter replication and configuring failover/failback, a user connecting to either the source account or target account via an OAuth client\nmight\nneed to re-authenticate to the target account.\nRe-authentication is likely to be necessary if the OAuth authorization server is not configured to issue a refresh token. Therefore,\nensure that the OAuth authorization server issues refresh tokens so that the OAuth client can connect to the source and target Snowflake\naccounts.\nFor this procedure, assume the following:\nSource account:\nhttps://example-northamericawest",
    "back, a user connecting to either the source account or target account via an OAuth client\nmight\nneed to re-authenticate to the target account.\nRe-authentication is likely to be necessary if the OAuth authorization server is not configured to issue a refresh token. Therefore,\nensure that the OAuth authorization server issues refresh tokens so that the OAuth client can connect to the source and target Snowflake\naccounts.\nFor this procedure, assume the following:\nSource account:\nhttps://example-northamericawest.snowflakecomputing.com/\nTarget account:\nhttps://example-northamericaeast.snowflakecomputing.com/\nConnection URL:\nhttps://example-global.snowflakecomputing.com\nA secondary connection exists in the target account (i.e. only refresh operations are needed).\nThe Snowflake OAuth or External OAuth security integrations already exist in the source account.\nThis procedure is a representative example to do the following:\nReplicate an OAuth security integration.\nRefresh the failover group.\nPromote the secondary connection in the target account to serve as the primary connection.\nSource account steps:\nIf the failover group already specifies\nsecurity\nintegrations\n, skip to the next step. This would be true if you have already\nconfigured the failover group for the purposes of\nSAML SSO in the target account\n(in this topic) or\nSCIM\n(also in this topic).\nOtherwise, modify the existing failover group using an ALTER FAILOVER GROUP command to specify\nsecurity\nintegrations\n.\nALTER\nFAILOVER\nGROUP\nfg\nSET\nOBJECT_TYPES\n=\nusers\n,\nroles\n,\nwarehouses\n,\nresource monitors\n,\nintegrations\nALLOWED_INTEGRATION_TYPES\n=\nsecurity\nintegrations\n;\nCopy\nTarget account steps:\nRefresh the secondary failover group to update the target account to include the OAuth security integration objects.\nALTER\nFAILOVER\nGROUP\nfg\nREFRESH\n;\nCopy\nVerify connecting to each Snowflake account using the OAuth client of your choice.\nOptionally, promote the secondary failover group and the secondary connection in the target account to primary. This will promote the\ntarget account to serve as the new source account.\nFailover group:\nALTER\nFAILOVER\nGROUP\nfg\nPRIMARY\n;\nCopy\nConnection:\nALTER\nCONNECTION\nglobal\nPRIMARY\n;\nCopy\nIf you completed the previous step, reverify that you can connect to each Snowflake account using",
    " each Snowflake account using the OAuth client of your choice.\nOptionally, promote the secondary failover group and the secondary connection in the target account to primary. This will promote the\ntarget account to serve as the new source account.\nFailover group:\nALTER\nFAILOVER\nGROUP\nfg\nPRIMARY\n;\nCopy\nConnection:\nALTER\nCONNECTION\nglobal\nPRIMARY\n;\nCopy\nIf you completed the previous step, reverify that you can connect to each Snowflake account using the OAuth client of your choice.\nReplicating network policies\n\u00b6\nReplicating a network policy from the source account to the target account allows administrators to restrict access to the target account\nbased on the network identifier of the origin of an incoming request.\nReplicating network policy references and assignments\n\u00b6\nReplicating a network policy replicates the network policy object\nand\nany network policy references/assignments. For example, if a\nnetwork policy references a network rule in the source account, and both objects exist in the target account, then the network policy uses\nthe same network rule in the target account. Similarly, if a network policy is assigned to a user and the user exists in both the source and\ntarget accounts, replicating the network policy assigns the network policy to the user in the target account.\nReplicating network policy references and assignments assumes referenced objects and objects to which the network policy is assigned are\nalso replicated. If you do not replicate the supporting object types properly, Snowflake fails the refresh operation in the target account.\nIf a referenced object or object to which the network policy is assigned does not already exist in the target account, include its object\ntype in the same replication or failover group as the network policy. The following examples demonstrate the required settings if the\nsupporting objects do not already exist in the target account.\nNetwork policies that use network rules\nThe replication or failover group must include\nnetwork\npolicies\nand\ndatabases\n. Network rules are schema-level objects\nand are replicated with the database in which they are contained. For example:\nCREATE\nFAILOVER\nGROUP\nfg\nOBJECT_TYPES\n=\nnetwork policies\n,\ndatabases\nALLOWED_DATABASES\n=\ntestdb2\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmyaccount2\n;\nCopy\nNetwork policies assigned to an account\nThe replication or failover group must include\nnetwork\npolicies\nand\naccount\nparameters",
    "atabases\n. Network rules are schema-level objects\nand are replicated with the database in which they are contained. For example:\nCREATE\nFAILOVER\nGROUP\nfg\nOBJECT_TYPES\n=\nnetwork policies\n,\ndatabases\nALLOWED_DATABASES\n=\ntestdb2\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmyaccount2\n;\nCopy\nNetwork policies assigned to an account\nThe replication or failover group must include\nnetwork\npolicies\nand\naccount\nparameters\n. If the network policy uses\nnetwork rules, you must also include\ndatabases\n. For example:\nCREATE\nFAILOVER\nGROUP\nfg\nOBJECT_TYPES\n=\nnetwork policies\n,\naccount\nparameters\n,\ndatabases\nALLOWED_DATABASES\n=\ntestdb2\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmyaccount2\n;\nCopy\nNetwork policies assigned to a user\nThe replication or failover group must include\nnetwork\npolicies\nand\nusers\n. If the network policy uses network rules, you\nmust also include\ndatabases\n. For example:\nCREATE\nFAILOVER\nGROUP\nfg\nOBJECT_TYPES\n=\nnetwork policies\n,\nusers\n,\ndatabases\nALLOWED_DATABASES\n=\ntestdb2\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmyaccount2\n;\nCopy\nNetwork policies assigned to a security integration\nNetwork policy replication applies to network policies that are specified in Snowflake OAuth and SCIM\nsecurity integrations\n, provided that the replication or failover group includes\nintegrations\n,\nsecurity\nintegrations\nand\nnetwork\npolicies\n. If the network policy uses network rules, you must also\ninclude\ndatabases\n.\nCREATE\nFAILOVER\nGROUP\nfg\nOBJECT_TYPES\n=\nnetwork policies\n,\nintegrations\n,\ndatabases\nALLOWED_DATABASES\n=\ntestdb2\nALLOWED_INTEGRATION_TYPES\n=\nsecurity\nintegrations\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmyaccount2\n;\nCopy\nExample\n\u00b6\nFor this example, assume the following:\nSource account:\nhttps://example-northamericawest.snowflakecomputing.com/\nTarget account:\nhttps://example-northamericaeast.snowflakecomputing.com/\nConnection URL:\nhttps://example-global.snowflakecomputing.com\nA secondary connection exists in the target account (i.e. only refresh operations are needed).\nNetwork",
    "Stage, pipe, and load history replication\n\u00b6\nStandard & Business Critical Feature\nDatabase and share replication are available to all accounts.\nReplication of other account objects & failover/failback require Business Critical Edition (or higher).\nTo inquire about upgrading, please contact\nSnowflake Support\n.\nThis topic provides information about replication support for data pipeline objects and related metadata,\nincluding stages, storage integrations, pipes, and load history. You can replicate these objects to configure failover for ingest and ETL\npipelines across\nregions\nand across\ncloud platforms\n.\nBefore you get started, we recommend that you have familiarity with Snowflake support for replication and failover/failback.\nFor more information, see\nIntroduction to replication and failover across multiple accounts\n.\nRequirements\n\u00b6\nImportant\nIf a database in a target account that you plan to use\nalready\ncontains stages and pipes, we recommend that you contact support\nbefore enabling replication. When a replication or failover group in your source account includes that database, any pre-existing stages\nand pipes are dropped from the database.\nTo replicate any external stages that use a storage integration, you must configure your replication or failover group to replicate\nSTORAGE\nINTEGRATIONS\n. Otherwise, external stages are replicated without the associated storage integration.\nYou can use an\nALTER REPLICATION GROUP\nor\nALTER FAILOVER GROUP\nstatement to modify these properties for an existing group.\nIf you add\nINTEGRATIONS\nto the\nOBJECT_TYPES\nlist in your ALTER statement,\ninclude any other existing objects in the list to avoid dropping those objects in the target account.\nThe same applies if you add\nSTORAGE\nINTEGRATIONS\nto the\nALLOWED_INTEGRATION_TYPES\nlist.\nFor example:\nALTER\nFAILOVER\nGROUP\nmy_failover_group\nSET\nOBJECT_TYPES\n=\nROLES\n,\nINTEGRATIONS\nALLOWED_INTEGRATION_TYPES\n=\nAPI INTEGRATIONS\n,\nSTORAGE INTEGRATIONS\n;\nCopy\nNote\nYour cloud storage provider might limit replication of data pipeline objects between commercial and government cloud regions. To avoid\ngovernment cloud data replication limitations, configure your failover resources in any region accessible to your government cloud region.\nFor more information about government cloud limitations, review your cloud storage provider\u2019s documentation.\nReplication and stages\n\u00b6\nThis section describes the current level of replication functionality that Snowflake supports for different types of stages.\nReplication",
    "ATIONS\n,\nSTORAGE INTEGRATIONS\n;\nCopy\nNote\nYour cloud storage provider might limit replication of data pipeline objects between commercial and government cloud regions. To avoid\ngovernment cloud data replication limitations, configure your failover resources in any region accessible to your government cloud region.\nFor more information about government cloud limitations, review your cloud storage provider\u2019s documentation.\nReplication and stages\n\u00b6\nThis section describes the current level of replication functionality that Snowflake supports for different types of stages.\nReplication of internal stages\n\u00b6\nThe following table describes how replication works for each type of internal stage.\nType\nDescription of Replication Support\nTable stage\nEmpty table stages are created for tables in a replicated database. Files on table stages are not replicated.\nUser stage\nUser and user stage replication requires Business Critical Edition (or higher).\nEmpty user stages are created for replicated users. Files on user stages are not replicated.\nNamed stage\nNamed internal stages are replicated when you replicate a database.\nThe stage must have a directory table enabled on it in order to replicate the files on the stage.\nReplication of external stages\n\u00b6\nNote\nSnowflake does not replicate files on an external stage.\nThe cloud storage URL points to the same location for external stages in primary and secondary databases.\nThe following table describes how replication works for each type of external stage.\nType\nDescription of Replication Support\nNamed stage with no credentials (public storage location)\nNamed external stages are replicated when you replicate a database. The files on an external stage are not replicated.\nNamed stage with credentials (private storage location)\nReplicated stages include the cloud provider credentials, such as secret keys or access tokens.\nNamed stage with storage integration (private storage location)\nStorage integration replication requires Business Critical Edition (or higher).\nThe replication or failover group must include\nSTORAGE\nINTEGRATIONS\nin the\nALLOWED_INTEGRATION_TYPES\nlist.\nFor more information, see\nCREATE FAILOVER GROUP\n.\nYou must also take action to configure the trust relationships for your cloud storage in the target accounts.\nFor more information, see\nConfigure cloud storage access for secondary storage integrations\n.\nNote\nTo associate a secondary stage or pipe with a different cloud storage location than the one associated with the primary object,\ncontact the support team. For example, you might choose a location in another region.\nConsiderations\n\u00b6\nThe following constraints apply to stage objects:\nSnowflake currently supports stage replication as part of group-based",
    ", you reduce the risk of losing notifications and data after failover.\nNote\nIf you already use SNS, migration is not necessary.\nInstead, follow the usual steps to configure automation with SNS for secondary directory tables or auto-ingest pipes before failover.\nConfigure automated refresh for directory tables on secondary stages\nConfigure notifications for secondary auto-ingest pipes\nPrerequisites\n\u00b6\nTo migrate, you must meet the following conditions:\nYou have already set up one or more event notifications for your S3 bucket. For instructions, see the topic for your use case:\nRefreshing Directory Tables Automatically for Amazon S3: Creating a New S3 Event Notification\nCreating a New S3 Event Notification to Automate Snowpipe\nYou have already created a replication or failover group in a target account that includes a stage with a directory table or a pipe.\nMigrate to an SNS Topic\n\u00b6\nCreate an SNS topic in your AWS account.\nFor instructions, see\nCreating an Amazon SNS topic\nin the AWS SNS documentation.\nSubscribe your target destinations (for example, other SQS queues or AWS Lambda workloads) for your S3 event notification(s)\nto your SNS topic. SNS publishes event notifications for your bucket to all subscribers to the topic.\nFor instructions, see the\nAWS SNS documentation\n.\nUpdate the access policy for your topic with the following permissions:\nAllow the Snowflake IAM user to subscribe the SQS queue that is in your\ntarget\naccount\nto your topic.\nAllow Amazon S3 to publish event notifications from your bucket to the SNS topic.\nFor instructions, see\nStep 1: Subscribe the Snowflake SQS Queue to the SNS Topic\n.\nIn your target Snowflake account, call the\nSYSTEM$CONVERT_PIPES_SQS_TO_SNS\nfunction.\nThe function subscribes the SQS queue in your\ntarget\naccount to your SNS topic without interrupting metadata\nsynchronization or ingestion work.\nSpecify your S3 bucket name and SNS topic ARN.\nSELECT\nSYSTEM$CONVERT_PIPES_SQS_TO_SNS\n(\n's3_mybucket'\n,\n'arn:aws:sns:us-west-2:001234567890:MySNSTopic'\n)\nCopy\nUpdate your S3 event notifications to use your SNS topic as a destination. For instructions, see the\nAmazon S3 User Guide\n.\nAfter you complete these steps, the SQS queue automatically",
    "Specify your S3 bucket name and SNS topic ARN.\nSELECT\nSYSTEM$CONVERT_PIPES_SQS_TO_SNS\n(\n's3_mybucket'\n,\n'arn:aws:sns:us-west-2:001234567890:MySNSTopic'\n)\nCopy\nUpdate your S3 event notifications to use your SNS topic as a destination. For instructions, see the\nAmazon S3 User Guide\n.\nAfter you complete these steps, the SQS queue automatically unbinds from your S3 event notification(s).\nAll of the directory tables and pipes that use the specified S3 bucket will start using SNS as the source of notifications.\nOn this page\nRequirements\nReplication and stages\nReplication and pipes\nExample 1: Replicate a named internal stage\nExample 2: Replicate an external stage and storage integration\nExample 3: Replicate an auto-ingest pipe\nMigrate to Amazon Simple Notification Service (SNS)",
    " must also take action to configure the trust relationships for your cloud storage in the target accounts.\nFor more information, see\nConfigure cloud storage access for secondary storage integrations\n.\nNote\nTo associate a secondary stage or pipe with a different cloud storage location than the one associated with the primary object,\ncontact the support team. For example, you might choose a location in another region.\nConsiderations\n\u00b6\nThe following constraints apply to stage objects:\nSnowflake currently supports stage replication as part of group-based replication (replication and failover groups).\nStage replication is not supported for database replication.\nYou can replicate an external stage. However, the files on an external stage are not replicated.\nYou can replicate an internal stage. To replicate the files on an internal stage, you must enable a directory table on the stage.\nSnowflake replicates only the files that are mapped by the directory table.\nWhen you replicate an internal stage with a directory table, you cannot disable the directory table on the primary or secondary stage.\nThe directory table contains critical information about replicated files and files loaded using a COPY statement.\nA refresh operation will fail if the directory table on an internal stage contains a file that is larger than 5GB. To work around this\nlimitation, move any files larger than 5GB to a different stage.\nYou cannot disable the directory table on a primary or secondary stage, or any stage that has previously been replicated. Follow\nthese steps\nbefore\nyou add the database that contains the stage to a replication or failover group.\nDisable the directory table\non the primary stage.\nMove the files that are larger than 5GB to another stage that does not have a directory table enabled.\nAfter you move the files to another stage, re-enable the directory table on the primary stage.\nFiles on user stages and table stages are not replicated.\nFor named external stages that use a storage integration, you must configure the trust relationship for secondary storage integrations\nin your target accounts prior to failover. For more information, see\nConfigure cloud storage access for secondary storage integrations\n.\nIf you replicate an external stage with a directory table, and you have configured\nautomated refresh\nfor the source\ndirectory table, you must configure automated refresh for the\nsecondary\ndirectory table before failover. For more information,\nsee\nConfigure automated refresh for directory tables on secondary stages\n.\nA copy command might take longer than expected if the directory table on a replicated stage is not consistent with the",
    "over. For more information, see\nConfigure cloud storage access for secondary storage integrations\n.\nIf you replicate an external stage with a directory table, and you have configured\nautomated refresh\nfor the source\ndirectory table, you must configure automated refresh for the\nsecondary\ndirectory table before failover. For more information,\nsee\nConfigure automated refresh for directory tables on secondary stages\n.\nA copy command might take longer than expected if the directory table on a replicated stage is not consistent with the\nreplicated files on the stage. To make a directory table consistent, refresh it with an\nALTER STAGE \u2026 REFRESH\nstatement.\nTo check the consistency status of a directory table, use the\nSYSTEM$GET_DIRECTORY_TABLE_STATUS\nfunction.\nReplication and pipes\n\u00b6\nThis section describes the current level of replication functionality supported for different types of pipes.\nSnowflake supports replication for the following:\nPipe objects, including auto-ingest and REST endpoint pipes that load data from external stages.\nPipe-level parameters.\nPrivilege grants on pipe objects.\nNote\nTo associate a secondary stage or pipe with a different cloud storage location than the one associated with the primary object,\ncontact the support team. For example, you might choose a location in another region.\nPipes in secondary databases\n\u00b6\nPipes in a secondary database are in a\nREAD_ONLY\nexecution state and receive notifications\nbut do not load data until you promote the secondary database to serve as the primary.\nAfter you promote a secondary database, the pipes will transition to a\nFAILING_OVER\nexecution state.\nOnce failover is complete, the pipes should be in the\nRUNNING\nexecution state\nand begin to load any data that is available since the last refresh time (that is, the last time that the former primary database was updated).\nReplication of auto-ingest pipes\n\u00b6\nIn the event of a failover, a replicated auto-ingest pipe becomes the new primary pipe and can do the following:\nLoad any data that has not yet been loaded.\nThis includes any data that is new since the newly promoted primary database was last refreshed.\nContinue to receive notifications when the stage has new files to load, and loads data from those files.\nNote\nTo receive notifications, you must configure a secondary auto-ingest pipe in a target account\nprior to failover\n.\nFor more information, see\nConfigure notifications for secondary auto-ingest pipes\n.\nReplication of REST endpoint pipes\n\u00b6\nFor pipes",
    "Load any data that has not yet been loaded.\nThis includes any data that is new since the newly promoted primary database was last refreshed.\nContinue to receive notifications when the stage has new files to load, and loads data from those files.\nNote\nTo receive notifications, you must configure a secondary auto-ingest pipe in a target account\nprior to failover\n.\nFor more information, see\nConfigure notifications for secondary auto-ingest pipes\n.\nReplication of REST endpoint pipes\n\u00b6\nFor pipes that use the\nSnowpipe REST API\nto load data,\nSnowflake replicates the pipes and their load history metadata to each target account that you specify.\nThere are no additional configuration steps you need to take on the target accounts.\nFor a detailed list of load history metadata, see\nLoad metadata\n.\nTo continue data loading in the event of a failover, call the REST API from the newly-promoted source account.\nConsiderations\n\u00b6\nThe following constraints apply to pipe objects:\nSnowflake currently supports pipe replication as part of group-based replication (replication and failover groups).\nPipe replication is not supported for database replication.\nSnowflake replicates the copy history of a pipe only when the pipe belongs to the same replication group as its target table.\nReplication of notification integrations is not supported.\nSnowflake only replicates load history after the latest table truncate.\nTo receive notifications, you must configure a secondary auto-ingest pipe in a target account\nprior to failover\n.\nFor more information, see\nConfigure notifications for secondary auto-ingest pipes\n.\nUse the\nSYSTEM$PIPE_STATUS\nfunction to resolve any pipes not in their expected execution state after failover.\nSnowflake doesn\u2019t support replication and failover for Snowpipe with the Kafka connector, but Snowflake does support replication and failover for Snowpipe Streaming with the Kafka connector. For more information, see\nSnowpipe Streaming and the Kafka connector\n.\nExample 1: Replicate a named internal stage\n\u00b6\nThis example demonstrates how replication works for internal stages. In particular, the example shows how the directory table\nis the single source of truth for stage metadata before and after replication.\nThe first part of the example completes the following tasks in a\nsource\naccount.\nCreate an internal stage named\nmy_int_stage\nwith a directory table enabled to replicate the files on the stage. Then copy data\nfrom a table named\nmy_table\ninto files on the stage.\nNote\nThe example refresh",
    "\u00b6\nThis example demonstrates how replication works for internal stages. In particular, the example shows how the directory table\nis the single source of truth for stage metadata before and after replication.\nThe first part of the example completes the following tasks in a\nsource\naccount.\nCreate an internal stage named\nmy_int_stage\nwith a directory table enabled to replicate the files on the stage. Then copy data\nfrom a table named\nmy_table\ninto files on the stage.\nNote\nThe example refreshes the directory table after loading\nfile1\nand\nfile2\nonto the stage to synchronize\nthe table metadata with the latest set of files in the stage definition for the directory tables.\nHowever, no refresh operation occurs after loading\nfile3\n.\nCREATE\nOR\nREPLACE\nSTAGE\nmy_stage\nDIRECTORY\n=\n(\nENABLE\n=\nTRUE\n);\nCOPY\nINTO\n@\nmy_stage\n/\nfolder1\n/\nfile1\nfrom\nmy_table\n;\nCOPY\nINTO\n@\nmy_stage\n/\nfolder2\n/\nfile2\nfrom\nmy_table\n;\nALTER\nSTAGE\nmy_stage\nREFRESH\n;\nCOPY\nINTO\n@\nmy_stage\n/\nfolder3\n/\nfile3\nfrom\nmy_table\n;\nCopy\nCreate a failover group:\nCREATE\nFAILOVER\nGROUP\nmy_stage_failover_group\nOBJECT_TYPES\n=\nDATABASES\nALLOWED_DATABASES\n=\nmy_database_1\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmy_account_2\n;\nCopy\nThe second part of the example completes the replication and failover process in a\ntarget\naccount:\nCreate a failover group as a replica of the failover group in the source account, refresh the objects in the new failover group,\nand promote the target account to serve as the source account.\nCREATE\nFAILOVER\nGROUP\nmy_stage_failover_group\nAS\nREPLICA\nOF\nmyorg\n.\nmy_account_1\n.\nmy_stage_failover_group\n;\nALTER\nFAILOVER\nGROUP\nmy_stage_failover_group\nREFRESH\n;\nALTER\nFAILOVER\nGROUP\nmy_stage_failover_group\nPRIMARY\n;\nCopy\nNext, refresh the directory table on the replicated stage and copy all of the\nfiles tracked by the directory table on\nmy_stage\ninto a table named\nmy_table\n.\nNote\nThe COPY INTO statement loads\nfile1",
    "\n.\nmy_account_1\n.\nmy_stage_failover_group\n;\nALTER\nFAILOVER\nGROUP\nmy_stage_failover_group\nREFRESH\n;\nALTER\nFAILOVER\nGROUP\nmy_stage_failover_group\nPRIMARY\n;\nCopy\nNext, refresh the directory table on the replicated stage and copy all of the\nfiles tracked by the directory table on\nmy_stage\ninto a table named\nmy_table\n.\nNote\nThe COPY INTO statement loads\nfile1\nand\nfile2\ninto the table, but not\nfile3\n.\nThis is because the directory table was not refreshed after adding\nfile3\nin the source account.\nALTER\nSTAGE\nmy_stage\nREFRESH\n;\nCOPY\nINTO\nmy_table\nFROM\n@\nmy_stage\n;\nCopy\nExample 2: Replicate an external stage and storage integration\n\u00b6\nThis example provides a sample workflow for replicating an external stage and storage integration to a target account.\nThe example assumes that you have already completed the following:\nConfigured secure access to your Amazon S3 bucket\n.\nThe first part of the example completes the following tasks in a\nsource\naccount.\nCreate a storage integration for an Amazon S3 bucket in database\nmy_database_2\n.\nCREATE\nSTORAGE\nINTEGRATION\nmy_storage_int\nTYPE\n=\nexternal_stage\nSTORAGE_PROVIDER\n=\n's3'\nSTORAGE_ALLOWED_LOCATIONS\n=\n(\n's3://mybucket/path'\n)\nSTORAGE_BLOCKED_LOCATIONS\n=\n(\n's3://mybucket/blockedpath'\n)\nENABLED\n=\ntrue\n;\nCopy\nCreate an external stage in database\nmy_database_2\nusing storage integration\nmy_storage_int\n.\nCREATE\nSTAGE\nmy_ext_stage\nURL\n=\n's3://mybucket/path'\nSTORAGE_INTEGRATION\n=\nmy_storage_int\nCopy\nCreate a failover group and include database\nmy_database_2\nand storage integration objects.\nCREATE\nFAILOVER\nGROUP\nmy_external_stage_fg\nOBJECT_TYPES\n=\ndatabases\n,\nintegrations\nALLOWED_INTEGRATION_TYPES\n=\nstorage integrations\nALLOWED_DATABASES\n=\nmy_database_2\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmy_account_2\n;\nCopy\nThe second part of the example completes the replication and failover process in a\ntarget\naccount:\nCreate a failover group as a replica of the fail",
    " integration objects.\nCREATE\nFAILOVER\nGROUP\nmy_external_stage_fg\nOBJECT_TYPES\n=\ndatabases\n,\nintegrations\nALLOWED_INTEGRATION_TYPES\n=\nstorage integrations\nALLOWED_DATABASES\n=\nmy_database_2\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmy_account_2\n;\nCopy\nThe second part of the example completes the replication and failover process in a\ntarget\naccount:\nCreate a failover group as a replica of the failover group in the source account and refresh.\nCREATE\nFAILOVER\nGROUP\nmy_external_stage_fg\nAS\nREPLICA\nOF\nmyorg\n.\nmy_account_1\n.\nmy_external_stage_fg\n;\nALTER\nFAILOVER\nGROUP\nmy_external_stage_fg\nREFRESH\n;\nCopy\nAfter you replicate the storage integration to the target account, you must take additional steps to update your cloud\nprovider permissions to grant the replication integration access to your cloud storage. For more information, see\nConfigure cloud storage access for secondary storage integrations\n.\nExample 3: Replicate an auto-ingest pipe\n\u00b6\nThis example provides a sample workflow for replicating a pipe that uses\nan\nAmazon Simple Notification Service (SNS) topic with Amazon Simple Queue Service (SQS) to automate Snowpipe\n.\nThe example assumes that you have already completed the following tasks:\nCreated and configured a storage integration for Amazon S3\n. For example\npurposes, we use a storage integration named\nmy_s3_storage_int\n.\nCreated an Amazon SNS topic and subscription, and subscribed the Snowflake SQS queue to your SNS topic\n.\nCreated an external stage that references your storage integration. For example\npurposes, we use a stage named\nmy_s3_stage\n. For instructions, see\nCREATE STAGE\n.\nStart with the following tasks in a\nsource\naccount.\nUse the\nCREATE PIPE\ncommand to create a pipe with auto-ingest enabled that loads data from the external stage into a table named\nmytable\n.\nCREATE\nPIPE\nsnowpipe_db\n.\npublic\n.\nmypipe\nAUTO_INGEST\n=\nTRUE\nAWS_SNS_TOPIC\n=\n'<topic_arn>'\nAS\nCOPY\nINTO\nsnowpipe_db\n.\npublic\n.\nmytable\nFROM\n@\nsnowpipe_db\n.\npublic\n.\nmy_s3_stage\nFILE_FORMAT\n=\n(\nTYPE\n=\n'JSON'\n",
    " pipe with auto-ingest enabled that loads data from the external stage into a table named\nmytable\n.\nCREATE\nPIPE\nsnowpipe_db\n.\npublic\n.\nmypipe\nAUTO_INGEST\n=\nTRUE\nAWS_SNS_TOPIC\n=\n'<topic_arn>'\nAS\nCOPY\nINTO\nsnowpipe_db\n.\npublic\n.\nmytable\nFROM\n@\nsnowpipe_db\n.\npublic\n.\nmy_s3_stage\nFILE_FORMAT\n=\n(\nTYPE\n=\n'JSON'\n);\nCopy\nRefresh the pipe with an\nALTER PIPE\nstatement to load data from the stage from the last 7 days.\nALTER\nPIPE\nmypipe\nREFRESH\n;\nCopy\nFinally, use\nCREATE FAILOVER GROUP\nto create a failover group\nthat allows replication of storage integrations.\nCREATE\nFAILOVER\nGROUP\nmy_pipe_failover_group\nOBJECT_TYPES\n=\nDATABASES\n,\nINTEGRATIONS\nALLOWED_INTEGRATION_TYPES\n=\nSTORAGE INTEGRATIONS\nALLOWED_DATABASES\n=\nsnowpipe_db\nALLOWED_ACCOUNTS\n=\nmyorg\n.\nmy_account_2\n;\nCopy\nThe second part of the example completes the replication and failover process in a\ntarget\naccount:\nCreate a failover group as a replica of the failover group in the source account.\nCREATE\nFAILOVER\nGROUP\nmy_pipe_failover_group\nAS\nREPLICA\nOF\nmyorg\n.\nmy_account_1\n.\nmy_pipe_failover_group\n;\nCopy\nExecute a\nDESCRIBE INTEGRATION\nstatement to retrieve the ARN for the\nAWS IAM User for your Snowflake account on the secondary deployment.\nUse the ARN to grant the IAM user permissions to access your S3 bucket.\nSee Step 5: Grant the IAM User Permissions to Access Bucket Objects\n.\nDESC\nINTEGRATION\nmy_s3_storage_int\n;\nCopy\nCall the\nSYSTEM$GET_AWS_SNS_IAM_POLICY\nsystem function to generate an IAM policy that grants the new SQS queue permission\nto subscribe to your SNS topic. Snowflake created the new SQS queue in your target account when you replicated the failover group from your\nsource account.\nSELECT\nSYSTEM$GET_AWS_SNS_IAM_POLICY\n(\n'<topic_arn>'\n);\nCopy\ntopic_arn\nis the Amazon Resource Name (ARN) of the SNS topic that you created for",
    "_AWS_SNS_IAM_POLICY\nsystem function to generate an IAM policy that grants the new SQS queue permission\nto subscribe to your SNS topic. Snowflake created the new SQS queue in your target account when you replicated the failover group from your\nsource account.\nSELECT\nSYSTEM$GET_AWS_SNS_IAM_POLICY\n(\n'<topic_arn>'\n);\nCopy\ntopic_arn\nis the Amazon Resource Name (ARN) of the SNS topic that you created for the original pipe in your source account.\nThen,\nSubscribe the new Amazon SQS queue to your SNS topic\n.\nRefresh the objects in your new failover group.\nALTER\nFAILOVER\nGROUP\nmy_pipe_failover_group\nREFRESH\n;\nCopy\nFinally, promote the target account to serve as the source account with the\nALTER FAILOVER GROUP\ncommand.\nALTER\nFAILOVER\nGROUP\nmy_pipe_failover_group\nPRIMARY\n;\nCopy\nThe\nmypipe\npipe will begin to load any data that was made available since the\nlast time the failover group was refreshed in the source account.\nTo verify that the replicated pipe is working, query the table from the pipe\u2019s COPY statement.\nSELECT\n*\nFROM\nmytable\n;\nCopy\nMigrate to Amazon Simple Notification Service (SNS)\n\u00b6\nThis section covers how to migrate from sending Amazon S3 event notifications directly to an Amazon Simple Queue Service (SQS)\nqueue to using an Amazon Simple Notification Service (SNS) topic for the following scenarios:\nRefreshing directory tables automatically for Amazon S3\nAutomating Snowpipe for Amazon S3\nWhen you replicate a directory table or pipe,\nSnowflake creates a new SQS queue in your target account to handle automation. You can configure a single SNS topic to\ndeliver event notifications from your S3 bucket to all SQS queues across multiple accounts.\nBy broadcasting your S3 event notification(s) to every SQS queue, you reduce the risk of losing notifications and data after failover.\nNote\nIf you already use SNS, migration is not necessary.\nInstead, follow the usual steps to configure automation with SNS for secondary directory tables or auto-ingest pipes before failover.\nConfigure automated refresh for directory tables on secondary stages\nConfigure notifications for secondary auto-ingest pipes\nPrerequisites\n\u00b6\nTo migrate, you must meet the following conditions:\nYou have already set up one or more event notifications for your S",
    "Understanding & using Time Travel\n\u00b6\nSnowflake Time Travel enables accessing historical data (that is, data that has been changed or deleted) at any point within a defined period.\nIt serves as a powerful tool for performing the following tasks:\nRestoring objects that might have been accidentally or intentionally deleted. You can restore individual objects,\nsuch as tables, or restore all the objects inside a container object by restoring an entire schema or database.\nDuplicating and backing up data from key points in the past.\nAnalyzing data usage/manipulation over specified periods of time.\nIntroduction to Time Travel\n\u00b6\nUsing Time Travel, you can perform the following actions within a defined period of time:\nQuery data in the past that has since been updated or deleted.\nCreate clones of entire tables, schemas, and databases at or before specific points in the past.\nRestore tables, schemas, databases, and some other kinds of objects that have been dropped.\nNote\nWhen querying historical data in a table or non-materialized view, the current table or view schema is used. For more\ninformation, see\nUsage notes\nfor AT | BEFORE.\nAfter the defined period of time has elapsed, the data is moved into\nSnowflake Fail-safe\nand these actions\ncan no longer be performed.\nNote\nA long-running Time Travel query will delay moving any data and objects (tables, schemas, and databases) in the account into Fail-safe,\nuntil the query completes.\nTime Travel SQL extensions\n\u00b6\nTo support Time Travel, the following SQL extensions have been implemented:\nAT | BEFORE\nclause which can be specified in SELECT statements and CREATE \u2026 CLONE commands (immediately\nafter the object name). The clause uses one of the following parameters to pinpoint the exact historical data you want to access:\nTIMESTAMP\nOFFSET (time difference in seconds from the present time)\nSTATEMENT (query ID for statement)\nUNDROP <object>\ncommand for tables, schemas, databases, accounts, external volumes, and tags.\nData retention period\n\u00b6\nA key component of Snowflake Time Travel is the data retention period.\nWhen data in a table is modified, including deletion of data or dropping an object containing data, Snowflake preserves the state of the data\nbefore the update. The data retention period specifies the number of days for which this historical data is preserved and, therefore,\nTime Travel operations (SELECT, CREATE \u2026 CLONE, UNDROP) can be performed on the data.\nThe standard retention period is ",
    ".\nData retention period\n\u00b6\nA key component of Snowflake Time Travel is the data retention period.\nWhen data in a table is modified, including deletion of data or dropping an object containing data, Snowflake preserves the state of the data\nbefore the update. The data retention period specifies the number of days for which this historical data is preserved and, therefore,\nTime Travel operations (SELECT, CREATE \u2026 CLONE, UNDROP) can be performed on the data.\nThe standard retention period is 1 day (24 hours) and is automatically enabled for all Snowflake accounts:\nFor Snowflake Standard Edition, the retention period can be set to 0 (or unset back to the default of 1 day) at the account and object\nlevel (that is, databases, schemas, and tables).\nFor Snowflake Enterprise Edition (and higher):\nFor transient databases, schemas, and tables, the retention period can be set to 0 (or unset back to the default of 1 day). The same\nis also true for temporary tables.\nFor permanent databases, schemas, and tables, the retention period can be set to any value from 0 up to 90 days.\nNote\nA retention period of 0 days for an object effectively deactivates Time Travel for the object.\nWhen the retention period ends for an object, the historical data is moved into\nSnowflake Fail-safe\n:\nHistorical data is no longer available for querying.\nPast objects can no longer be cloned.\nPast objects that were dropped can no longer be restored.\nTo specify the data retention period for Time Travel:\nThe\nDATA_RETENTION_TIME_IN_DAYS\nobject parameter can be used by users with the ACCOUNTADMIN role to set the default\nretention period for your account.\nThe same parameter can be used to explicitly override the default when creating a database, schema, and individual table.\nThe data retention period for a database, schema, or table can be changed at any time.\nThe\nMIN_DATA_RETENTION_TIME_IN_DAYS\naccount parameter can be set by users with the ACCOUNTADMIN role to set a minimum\nretention period for the account. This parameter does not alter or replace the DATA_RETENTION_TIME_IN_DAYS parameter value. However it\nmay change the effective data retention time. When this parameter is set at the account level, the effective minimum data retention\nperiod for an object is determined by MAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\nLimitations\n\u00b6\n",
    ";\nCopy\nDropping and restoring objects\n\u00b6\nThe following sections explain the Time Travel considerations for the DROP, SHOW, and UNDROP commands.\nDropping objects\n\u00b6\nWhen a table, schema, or database is dropped, it is not immediately overwritten or removed from the system. Instead, it is retained for the\ndata retention period for the object, during which time the object can be restored. Once dropped objects are moved to\nFail-safe\n, you cannot restore them.\nTo drop a notebook, table, schema, or database, use the following commands:\nDROP NOTEBOOK\nDROP TABLE\nDROP SCHEMA\nDROP DATABASE\nNote\nAfter dropping an object, creating an object with the same name does not restore the object. Instead, it creates a new version of the\nobject. The original, dropped version is still available and can be restored.\nRestoring a dropped object restores the object in place (that is, it does not create a new object).\nListing dropped objects\n\u00b6\nDropped objects can be listed using the following commands with the HISTORY keyword specified:\nSHOW TABLES\nSHOW SCHEMAS\nSHOW DATABASES\nSHOW ACCOUNTS\nFor example:\nSHOW\nTABLES\nHISTORY\nLIKE\n'load%'\nIN\nmytestdb\n.\nmyschema\n;\nSHOW\nSCHEMAS\nHISTORY\nIN\nmytestdb\n;\nSHOW DATABASES\nHISTORY\n;\nCopy\nThe output includes all dropped objects and an additional DROPPED_ON column, which displays the date and time when the object was dropped.\nIf an object has been dropped more than once, each version of the object is included as a separate row in the output.\nNote\nAfter the retention period for an object has passed and the object has been purged, it is no longer displayed in the\nSHOW\n<object_type>\nHISTORY output.\nRestoring objects\n\u00b6\nA dropped object that has not been purged from the system (that is, the object is displayed in the SHOW\n<object_type>\nHISTORY output) can be\nrestored using the following commands:\nUNDROP NOTEBOOK\nUNDROP TABLE\nUNDROP SCHEMA\nUNDROP DATABASE\nUNDROP ICEBERG TABLE\nUNDROP DYNAMIC TABLE\nUNDROP EXTERNAL VOLUME\nUNDROP TAG\nUNDROP ACCOUNT\nCalling UNDROP restores the object to its most recent state before the DROP command was issued.\nFor example:\nUNDROP\nTABLE\nmytable\n;\n",
    " object is displayed in the SHOW\n<object_type>\nHISTORY output) can be\nrestored using the following commands:\nUNDROP NOTEBOOK\nUNDROP TABLE\nUNDROP SCHEMA\nUNDROP DATABASE\nUNDROP ICEBERG TABLE\nUNDROP DYNAMIC TABLE\nUNDROP EXTERNAL VOLUME\nUNDROP TAG\nUNDROP ACCOUNT\nCalling UNDROP restores the object to its most recent state before the DROP command was issued.\nFor example:\nUNDROP\nTABLE\nmytable\n;\nUNDROP\nSCHEMA\nmyschema\n;\nUNDROP\nDATABASE\nmydatabase\n;\nUNDROP\nNOTEBOOK\nmynotebook\n;\nCopy\nNote\nIf an object with the same name already exists, UNDROP fails. You must rename the existing object, which then enables you to restore\nthe previous version of the object.\nAccess control requirements and name resolution\n\u00b6\nSimilar to dropping an object, a user must have OWNERSHIP privileges for an object to restore it. In addition, the user must have CREATE\nprivileges on the object type for the database or schema where the dropped object will be restored.\nRestoring tables and schemas is only supported in the current schema or current database, even if a fully-qualified object name is specified.\nExample: Dropping and restoring a table multiple times\n\u00b6\nIn the following example, the\nmytestdb.public\nschema contains two tables:\nloaddata1\nand\nproddata1\n. The\nloaddata1\ntable is\ndropped and recreated twice, creating three versions of the table:\nCurrent version\nSecond (most recent) dropped version\nFirst dropped version\nThe example then illustrates how to restore the two dropped versions of the table:\nFirst, the current table with the same name is renamed to\nloaddata3\n. This enables restoring the most recent version of the dropped\ntable, based on the timestamp.\nThen, the most recent dropped version of the table is restored.\nThe restored table is renamed to\nloaddata2\nto enable restoring the first version of the dropped table.\nLastly, the first version of the dropped table is restored.\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped",
    " table is renamed to\nloaddata2\nto enable restoring the first version of the dropped table.\nLastly, the first version of the dropped table is restored.\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nDROP\nTABLE\nloaddata1\n;\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | Fri, 13 May 2016 19:04:46",
    " MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | Fri, 13 May 2016 19:04:46 -0700 |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nCREATE\nTABLE\nloaddata1\n(\nc1\nnumber\n);\nINSERT\nINTO\nloaddata1\nVALUES\n(\n1111\n),\n(\n2222\n),\n(\n3333\n),\n(\n4444\n);\nDROP\nTABLE\nloaddata1\n;\nCREATE\nTABLE\nloaddata1\n(\nc1\nvarchar\n);\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 0    | 0     | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 4    | 4096  | PUBLIC | 1              | Fri, 13 May 2016 19:05:51 -0700 |\n| Tue, 17 Mar 2016 17:41:55 -070",
    " | PUBLIC | 1              | [NULL]                          |\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 4    | 4096  | PUBLIC | 1              | Fri, 13 May 2016 19:05:51 -0700 |\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | Fri, 13 May 2016 19:04:46 -0700 |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nALTER\nTABLE\nloaddata1\nRENAME\nTO\nloaddata3\n;\nUNDROP\nTABLE\nloaddata1\n;\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 4    | 4096  | PUBLIC | 1              | [NULL]                          |\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA3 | MYTESTDB      | PUBLIC      | TABLE |         |            | 0    | 0     | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1",
    " 0    | 0     | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | Fri, 13 May 2016 19:04:46 -0700 |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nALTER\nTABLE\nloaddata1\nRENAME\nTO\nloaddata2\n;\nUNDROP\nTABLE\nloaddata1\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | [NULL]                          |\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA2 | MYTESTDB      | PUBLIC      | TABLE |         |            | 4    | 4096  | PUBLIC | 1              | [NULL]                          |\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA3 | MYTESTDB      | PUBLIC      | TABLE |         |            | 0    | 0     | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | ",
    "NULL]                          |\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA3 | MYTESTDB      | PUBLIC      | TABLE |         |            | 0    | 0     | PUBLIC | 1              | [NULL]                          |\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nCopy\nOn this page\nIntroduction to Time Travel\nEnabling and deactivating Time Travel\nSpecifying the data retention period for an object\nChecking the data retention period for an object\nChanging the data retention period for an object\nQuerying historical data\nCloning historical objects\nDropping and restoring objects\nRelated content\nOverview of the data lifecycle\nData storage considerations\nRelated info\nFor a tutorial on using Time Travel, see the following page:\nGetting Started with Time Travel\n(Snowflake Quickstarts)",
    "_TIME_IN_DAYS\naccount parameter can be set by users with the ACCOUNTADMIN role to set a minimum\nretention period for the account. This parameter does not alter or replace the DATA_RETENTION_TIME_IN_DAYS parameter value. However it\nmay change the effective data retention time. When this parameter is set at the account level, the effective minimum data retention\nperiod for an object is determined by MAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\nLimitations\n\u00b6\nWhen using Time Travel, the following object types are\nnot\ncloned:\nExternal tables\nInternal (Snowflake) stages\nHybrid tables can be cloned for databases but not for schemas.\nUser tasks in a database or schema are\nnot\ncloned when using CREATE SCHEMA \u2026 TIMESTAMP. In the following example, tasks in the source schema (S1) are not cloned to the schema with a timestamp (S2) but are cloned to the schema without a timestamp (S3).\nCREATE\nSCHEMA\nS1\n;\nUSE\nSCHEMA\nS1\n;\nCREATE\nTASK\nT1\nAS\nSELECT\n1\n;\nCREATE\nSCHEMA\nS2\nCLONE\nS1\nAT\n(\nTIMESTAMP\n=>\n'2025-04-01 12:00:00'\n);\n-- T1 is not cloned into S2\nCREATE\nSCHEMA\nS3\nCLONE\nS1\n;\n-- T1 is cloned into S3\nCopy\nEnabling and deactivating Time Travel\n\u00b6\nNo tasks are required to enable Time Travel. It is automatically enabled with the standard, 1-day retention period.\nHowever, you may want to upgrade to Snowflake Enterprise Edition to enable configuring longer data retention periods of up to 90 days\nfor databases, schemas, and tables. Note that extended data retention requires additional storage which will be reflected in your monthly\nstorage charges. For more information about storage charges, see\nStorage costs for Time Travel and Fail-safe\n.\nTime Travel cannot be deactivated for an account. A user with the ACCOUNTADMIN role can set\nDATA_RETENTION_TIME_IN_DAYS\nto 0 at\nthe account level, which means that all databases (and subsequently all schemas and tables) created in the account have no retention period\nby default; however, this default can be overridden at any time for any database, schema, or table.\nA user with the ACCOUNTADMIN role can also set",
    "\nStorage costs for Time Travel and Fail-safe\n.\nTime Travel cannot be deactivated for an account. A user with the ACCOUNTADMIN role can set\nDATA_RETENTION_TIME_IN_DAYS\nto 0 at\nthe account level, which means that all databases (and subsequently all schemas and tables) created in the account have no retention period\nby default; however, this default can be overridden at any time for any database, schema, or table.\nA user with the ACCOUNTADMIN role can also set the\nMIN_DATA_RETENTION_TIME_IN_DAYS\nat the account level. This parameter\nsetting enforces a minimum data retention period for databases, schemas, and tables. Setting MIN_DATA_RETENTION_TIME_IN_DAYS does not\nalter or replace the DATA_RETENTION_TIME_IN_DAYS parameter value. It may, however, change the effective data retention period for objects.\nWhen MIN_DATA_RETENTION_TIME_IN_DAYS is set at the account level, the data retention period for an object is determined by\nMAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\nTime Travel can be deactivated for individual databases, schemas, and tables by specifying\nDATA_RETENTION_TIME_IN_DAYS\nwith a\nvalue of 0 for the object. However, if DATA_RETENTION_TIME_IN_DAYS is set to a value of 0, and MIN_DATA_RETENTION_TIME_IN_DAYS is set\nat the account level and is greater than 0, the higher value setting takes precedence.\nAttention\nBefore setting\nDATA_RETENTION_TIME_IN_DAYS\nto 0 for any object, consider whether you want to deactivate Time Travel for the object,\nparticularly as it pertains to recovering the object if it is dropped. When an object with no retention period is dropped, you will not\nbe able to restore the object.\nAs a general rule, we recommend maintaining a value of (at least) 1 day for any given object.\nIf the Time Travel retention period is set to 0, any modified or deleted data is moved into Fail-safe (for permanent tables)\nor deleted (for transient tables) by a background process. This may take a short time to complete. During that time, the\nTIME_TRAVEL_BYTES in table storage metrics might contain a non-zero value even when the Time Travel retention period is 0 days.\nSpecifying the data retention period for an object\n\u00b6\nEnterprise Edition Feature\nSpecifying a retention period greater than 1 day requires Enterprise Edition (or higher). To inquire about",
    " data is moved into Fail-safe (for permanent tables)\nor deleted (for transient tables) by a background process. This may take a short time to complete. During that time, the\nTIME_TRAVEL_BYTES in table storage metrics might contain a non-zero value even when the Time Travel retention period is 0 days.\nSpecifying the data retention period for an object\n\u00b6\nEnterprise Edition Feature\nSpecifying a retention period greater than 1 day requires Enterprise Edition (or higher). To inquire about upgrading, please contact\nSnowflake Support\n.\nBy default, the maximum retention period is 1 day (one 24-hour period). With Snowflake Enterprise Edition (and higher), the default\nfor your account can be set to any value up to 90 days:\nWhen creating a table, schema, or database, the account default can be overridden using the\nDATA_RETENTION_TIME_IN_DAYS\nparameter in the command.\nIf a retention period is specified for a database or schema, the period is inherited by default for all objects created in the\ndatabase/schema.\nA minimum retention period can be set on the account using the\nMIN_DATA_RETENTION_TIME_IN_DAYS\nparameter. If this parameter is\nset at the account level, the data retention period for an object is determined by\nMAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\nChecking the data retention period for an object\n\u00b6\nTo check the current retention period for a table, schema, or database, you can check the value of the\nretention_time\ncolumn in the output of the corresponding SHOW command, such as\nSHOW TABLES\n,\nSHOW SCHEMAS\n, or\nSHOW DATABASES\n.\nFor objects that are derived from tables, schemas, or databases, such as materialized views, you can examine the\nretention periods of the parent objects.\nFor streams, you can check the value of the\nstale_after\ncolumn in the output from the\nSHOW STREAMS\ncommand.\nTo include information about objects that have already been dropped, include the HISTORY clause with the\nSHOW command.\nThe following example shows how you might check the retention periods of certain objects by filtering the output of\nthe SHOW commands.\nThe following example checks the retention period for specific named tables:\nSHOW\nTABLES\n->>\nSELECT\n\"name\"\n,\n\"retention_time\"\nFROM\n$\n1\nWHERE\n\"name\"\nIN\n(\n'MY_TABLE1'\n,\n'MY_TABLE2",
    "S\ncommand.\nTo include information about objects that have already been dropped, include the HISTORY clause with the\nSHOW command.\nThe following example shows how you might check the retention periods of certain objects by filtering the output of\nthe SHOW commands.\nThe following example checks the retention period for specific named tables:\nSHOW\nTABLES\n->>\nSELECT\n\"name\"\n,\n\"retention_time\"\nFROM\n$\n1\nWHERE\n\"name\"\nIN\n(\n'MY_TABLE1'\n,\n'MY_TABLE2'\n);\nCopy\nThe following example checks for schemas where Time Travel is turned off:\nSHOW\nSCHEMAS\n->>\nSELECT\n\"name\"\n,\n\"retention_time\"\nFROM\n$\n1\nWHERE\n\"retention_time\"\n=\n0\n;\nCopy\nThe following example checks for databases where retention time is larger than the default.\nThe results include databases that have already been dropped.\nSHOW DATABASES\nHISTORY\n->>\nSELECT\n\"name\"\n,\n\"retention_time\"\n,\n\"dropped_on\"\nFROM\n$\n1\nWHERE\n\"retention_time\"\n>\n1\n;\nCopy\nChanging the data retention period for an object\n\u00b6\nIf you change the data retention period for a table, the new retention period impacts all data that is active, as well as any data currently\nin Time Travel. The impact depends on whether you increase or decrease the period:\nIncreasing Retention\n:\nCauses the data currently in Time Travel to be retained for the longer time period.\nFor example, if you have a table with a 10-day retention period and increase the period to 20 days, data that would have been removed\nafter 10 days is now retained for an additional 10 days before moving into Fail-safe.\nNote that this doesn\u2019t apply to any data that is older than 10 days and has already moved into Fail-safe.\nDecreasing Retention\n:\nReduces the amount of time data is retained in Time Travel:\nFor active data modified after the retention period is reduced, the new shorter period applies.\nFor data that is currently in Time Travel:\nIf the data is still within the new shorter period, it remains in Time Travel.\nIf the data is outside the new period, it moves into Fail-safe.\nFor example, if you have a table with a 10-day retention period and you decrease the period to 1-day, data from days 2 to 10 will be moved\ninto Fail-safe, leaving only the data from day 1 accessible",
    " is reduced, the new shorter period applies.\nFor data that is currently in Time Travel:\nIf the data is still within the new shorter period, it remains in Time Travel.\nIf the data is outside the new period, it moves into Fail-safe.\nFor example, if you have a table with a 10-day retention period and you decrease the period to 1-day, data from days 2 to 10 will be moved\ninto Fail-safe, leaving only the data from day 1 accessible through Time Travel.\nHowever, the process of moving the data from Time Travel into Fail-safe is performed by a background process, so the change is not immediately\nvisible. Snowflake guarantees that the data will be moved, but does not specify when the process will complete; until the background process\ncompletes, the data is still accessible through Time Travel.\nNote\nIf you change the data retention period for a database or schema, the change only affects active objects contained within\nthe database or schema. Any objects that have been dropped (for example, tables) remain unaffected.\nFor example, if you have a schema\ns1\nwith a 90-day retention period and table\nt1\nis in schema\ns1\n,\ntable\nt1\ninherits the 90-day retention period. If you drop table\ns1.t1\n,\nt1\nis retained in Time Travel\nfor 90 days. Later, if you change the schema\u2019s data retention period to 1 day, the retention\nperiod for the dropped table\nt1\nis unchanged. Table\nt1\nwill still be retained in Time Travel for 90 days.\nTo alter the retention period of a dropped object, you must undrop the object, then alter its retention period.\nTo change the retention period for an object, use the appropriate\nALTER <object>\ncommand. For example, to change the\nretention period for a table:\nCREATE\nTABLE\nmytable\n(\ncol1\nNUMBER\n,\ncol2\nDATE\n)\nDATA_RETENTION_TIME_IN_DAYS\n=\n90\n;\nALTER\nTABLE\nmytable\nSET\nDATA_RETENTION_TIME_IN_DAYS\n=\n30\n;\nCopy\nAttention\nChanging the retention period for your account or individual objects changes the value for all lower-level objects that do not have a\nretention period explicitly set. For example:\nIf you change the retention period at the account level, all databases, schemas, and tables that do not",
    "1\nNUMBER\n,\ncol2\nDATE\n)\nDATA_RETENTION_TIME_IN_DAYS\n=\n90\n;\nALTER\nTABLE\nmytable\nSET\nDATA_RETENTION_TIME_IN_DAYS\n=\n30\n;\nCopy\nAttention\nChanging the retention period for your account or individual objects changes the value for all lower-level objects that do not have a\nretention period explicitly set. For example:\nIf you change the retention period at the account level, all databases, schemas, and tables that do not have an explicit retention period\nautomatically inherit the new retention period.\nIf you change the retention period at the schema level, all tables in the schema that do not have an explicit retention period inherit the\nnew retention period.\nKeep this in mind when changing the retention period for your account or any objects in your account because the change might have\nTime Travel consequences that you did not anticipate or intend. In particular, we do\nnot\nrecommend changing the retention period to 0\nat the account level.\nDropped containers and object retention inheritance\n\u00b6\nCurrently, when a database is dropped, the data retention period for child schemas or tables, if explicitly set to be different from the\nretention of the database, is not honored. The child schemas or tables are retained for the same period of time as the database.\nSimilarly, when a schema is dropped, the data retention period for child tables, if explicitly set to be different from the retention of\nthe schema, is not honored. The child tables are retained for the same period of time as the schema.\nTo honor the data retention period for these child objects (schemas or tables), drop them explicitly\nbefore\nyou drop the database\nor schema.\nQuerying historical data\n\u00b6\nWhen any DML operations are performed on a table, Snowflake retains previous versions of the table data for a defined period of time. This\nenables querying earlier versions of the data using the\nAT | BEFORE\nclause.\nThis clause supports querying data either exactly at or immediately preceding a specified point in the table\u2019s history within the\nretention period. The specified point can be time-based (for example, a timestamp or time offset from the present) or it can be the ID for a\ncompleted statement (for example, SELECT or INSERT).\nFor example:\nThe following query selects historical data from a table as of the date and time represented by the specified\ntimestamp\n:\nSELECT\n*\nFROM\nmy_table\nAT\n(\nTIM",
    " querying data either exactly at or immediately preceding a specified point in the table\u2019s history within the\nretention period. The specified point can be time-based (for example, a timestamp or time offset from the present) or it can be the ID for a\ncompleted statement (for example, SELECT or INSERT).\nFor example:\nThe following query selects historical data from a table as of the date and time represented by the specified\ntimestamp\n:\nSELECT\n*\nFROM\nmy_table\nAT\n(\nTIMESTAMP\n=>\n'Wed, 26 Jun 2024 09:20:00 -0700'\n::timestamp_tz\n);\nCopy\nThe following query selects historical data from a table as of 5 minutes ago:\nSELECT\n*\nFROM\nmy_table\nAT\n(\nOFFSET\n=>\n-\n60\n*\n5\n);\nCopy\nThe following query selects historical data from a table up to, but not including any changes made by the specified statement:\nSELECT\n*\nFROM\nmy_table\nBEFORE\n(\nSTATEMENT\n=>\n'8e5d0ca9-005e-44e6-b858-a8f5b37c5726'\n);\nCopy\nNote\nIf the TIMESTAMP, OFFSET, or STATEMENT specified in the\nAT | BEFORE\nclause falls outside the data\nretention period for the table, the query fails and returns an error.\nCloning historical objects\n\u00b6\nIn addition to queries, the\nAT | BEFORE\nclause can be used with the CLONE keyword in the CREATE command\nfor a table, schema, or database to create a logical duplicate of the object at a specified point in the object\u2019s history. If you don\u2019t\nspecify a point in time, the clone defaults to the state of the object as of now\n(the\nCURRENT_TIMESTAMP\nvalue).\nFor example:\nThe following\nCREATE TABLE\nstatement creates a clone of a table as of the date and time represented by the\nspecified timestamp:\nCREATE\nTABLE\nrestored_table\nCLONE\nmy_table\nAT\n(\nTIMESTAMP\n=>\n'Wed, 26 Jun 2024 01:01:00 +0300'\n::timestamp_tz\n);\nCopy\nThe following\nCREATE SCHEMA\nstatement creates a clone of a schema and all its objects as they existed 1 hour\nbefore the current time:\nCREATE\nSCHEMA\nrestored_schema\nCLONE\nmy_schema\nAT\n(\nOFFSET\n=",
    "CREATE\nTABLE\nrestored_table\nCLONE\nmy_table\nAT\n(\nTIMESTAMP\n=>\n'Wed, 26 Jun 2024 01:01:00 +0300'\n::timestamp_tz\n);\nCopy\nThe following\nCREATE SCHEMA\nstatement creates a clone of a schema and all its objects as they existed 1 hour\nbefore the current time:\nCREATE\nSCHEMA\nrestored_schema\nCLONE\nmy_schema\nAT\n(\nOFFSET\n=>\n-\n3600\n);\nCopy\nThe following\nCREATE DATABASE\nstatement creates a clone of a database and all its objects as they existed prior\nto the completion of the specified statement:\nCREATE\nDATABASE\nrestored_db\nCLONE\nmy_db\nBEFORE\n(\nSTATEMENT\n=>\n'8e5d0ca9-005e-44e6-b858-a8f5b37c5726'\n);\nCopy\nNote\nThe cloning operation for a database or schema fails:\nIf the specified Time Travel time is beyond the retention time of\nany current child\n(for example, a table) of the entity.\nAs a workaround for child objects that have been purged from Time Travel, use the\nIGNORE TABLES WITH INSUFFICIENT DATA RETENTION\nparameter of the\nCREATE <object> \u2026 CLONE command. For more information, see\nChild objects and data retention time\n.\nIf the specified Time Travel time is at or before the point in time when the object was created.\nThe following\nCREATE DATABASE\nstatement creates a clone of a database and all its objects as they existed\nfour days ago, skipping any tables that have a data retention period of less than four days:\nCREATE\nDATABASE\nrestored_db\nCLONE\nmy_db\nAT\n(\nTIMESTAMP\n=>\nDATEADD\n(\ndays\n,\n-\n4\n,\ncurrent_timestamp\n)\n::timestamp_tz\n)\nIGNORE TABLES WITH INSUFFICIENT DATA RETENTION\n;\nCopy\nDropping and restoring objects\n\u00b6\nThe following sections explain the Time Travel considerations for the DROP, SHOW, and UNDROP commands.\nDropping objects\n\u00b6\nWhen a table, schema, or database is dropped, it is not immediately overwritten or removed from the system. Instead, it is retained for the\ndata retention period for the object, during which time the object can be restored. Once dropped objects are moved to\nFail-safe\n, you cannot restore them.\nTo drop",
    "Virtual warehouses\n\u00b6\nA virtual warehouse, often referred to simply as a \u201cwarehouse\u201d, is a cluster of compute resources in Snowflake. A virtual warehouse is\navailable in two types:\nStandard\nSnowpark-optimized\nA warehouse provides the required resources, such as CPU, memory, and temporary storage, to\nperform the following operations in a Snowflake session:\nExecuting SQL\nSELECT\nstatements that require compute resources (for example, retrieving rows from tables and views).\nPerforming DML operations, such as:\nUpdating rows in tables (\nDELETE\n,\nINSERT\n,\nUPDATE\n).\nLoading data into tables (\nCOPY INTO <table>\n).\nUnloading data from tables  (\nCOPY INTO <location>\n).\nNote\nTo perform these operations, a warehouse must be running and in use for the session. While a warehouse is running, it consumes Snowflake\ncredits.\nOverview of warehouses\nWarehouses are required for queries, as well as all DML operations, including loading data into tables.\nIn addition to being defined by its type as either Standard or Snowpark-optimized, a warehouse is defined by its size,\nas well as the other properties that can be set to help control and automate warehouse activity.\nSnowpark-optimized warehouses\nSnowpark workloads can be run on both Standard and Snowpark-optimized warehouses. Snowpark-optimized warehouses are recommended for workloads that have large memory requirements such as ML training use cases\nWarehouse considerations\nBest practices and general guidelines for using virtual warehouses in Snowflake to process queries\nMulti-cluster warehouses\nMulti-cluster warehouses enable you to scale compute resources to manage your user and query concurrency needs as they change, such as during peak and off hours.\nWorking with warehouses\nLearn how to create, stop, start and otherwise manage Snowflake warehouses.\nUsing the Query Acceleration Service (QAS)\nThe query acceleration service can accelerate parts of the query workload in a warehouse.\nWhen enabled for a warehouse, query acceleration can improve overall warehouse performance by reducing the impact of outlier queries\n(i.e. queries which use more resources then typical queries).\nMonitoring warehouse load\nWarehouse query load measures the average number of queries that were running or queued within a specific interval.\nOverview of warehouses\nSnowpark-optimized warehouses\nWarehouse considerations\nMulti-cluster warehouses\nWorking with warehouses\nUsing the Query Acceleration Service (QAS)\nMonitoring warehouse load\nRelated content\nUnderstanding compute cost\nWorking with resource monitors",
    " of outlier queries\n(i.e. queries which use more resources then typical queries).\nMonitoring warehouse load\nWarehouse query load measures the average number of queries that were running or queued within a specific interval.\nOverview of warehouses\nSnowpark-optimized warehouses\nWarehouse considerations\nMulti-cluster warehouses\nWorking with warehouses\nUsing the Query Acceleration Service (QAS)\nMonitoring warehouse load\nRelated content\nUnderstanding compute cost\nWorking with resource monitors"
]